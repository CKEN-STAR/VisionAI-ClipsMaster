#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆAIè½¬æ¢è´¨é‡æµ‹è¯•
ä½¿ç”¨å›ºå®šå¼ºåº¦è¿›è¡Œç¨³å®šçš„è¯„ä¼°
"""

import os
import sys

def final_ai_quality_test():
    """æœ€ç»ˆAIè½¬æ¢è´¨é‡æµ‹è¯•"""
    print("ğŸ¯ VisionAI-ClipsMaster æœ€ç»ˆAIè½¬æ¢è´¨é‡æµ‹è¯•")
    print("=" * 60)
    
    try:
        # å¯¼å…¥AIè½¬æ¢å™¨
        from src.core.ai_viral_transformer import AIViralTransformer
        transformer = AIViralTransformer()
        
        print("âœ… AIè½¬æ¢å™¨å¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥å¢å¼ºå…³é”®è¯åº“
        if hasattr(transformer, 'enhanced_keywords') and transformer.enhanced_keywords:
            stats = transformer.enhanced_keywords.get_keyword_stats()
            print(f"âœ… å¢å¼ºå…³é”®è¯åº“å·²åŠ è½½: {stats['total_keywords']}ä¸ªå…³é”®è¯ï¼Œ{stats['categories_count']}ä¸ªç±»åˆ«")
        else:
            print("âŒ å¢å¼ºå…³é”®è¯åº“æœªåŠ è½½")
            return False
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_subtitles = [
            {"start_time": 0.0, "end_time": 3.0, "text": "ä»Šå¤©å¤©æ°”å¾ˆå¥½"},
            {"start_time": 3.0, "end_time": 6.0, "text": "æˆ‘å»äº†å…¬å›­æ•£æ­¥"},
            {"start_time": 6.0, "end_time": 9.0, "text": "çœ‹åˆ°äº†å¾ˆå¤šèŠ±"},
            {"start_time": 9.0, "end_time": 12.0, "text": "å¿ƒæƒ…å˜å¾—å¾ˆæ„‰å¿«"},
            {"start_time": 12.0, "end_time": 15.0, "text": "è¿™æ˜¯ç¾å¥½çš„ä¸€å¤©"}
        ]
        
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®: {len(test_subtitles)}æ¡å­—å¹•")
        
        # ä½¿ç”¨å›ºå®šå¼ºåº¦0.8è¿›è¡Œæµ‹è¯•
        intensity = 0.8
        print(f"\nğŸš€ ä½¿ç”¨å›ºå®šå¼ºåº¦ {intensity} è¿›è¡Œè½¬æ¢æµ‹è¯•...")
        
        viral_subtitles = transformer.transform_to_viral(
            test_subtitles, 
            language="zh", 
            intensity=intensity
        )
        
        if not viral_subtitles or len(viral_subtitles) == 0:
            print("âŒ è½¬æ¢å¤±è´¥")
            return False
        
        print(f"âœ… è½¬æ¢æˆåŠŸï¼Œè¾“å‡º{len(viral_subtitles)}æ¡å­—å¹•")
        
        # åˆ†æè½¬æ¢ç»“æœ
        total_features = 0
        unique_features = set()
        emotional_count = 0
        total_length_increase = 0
        
        print(f"\nğŸ“ è½¬æ¢ç»“æœåˆ†æ:")
        for i, (original, viral) in enumerate(zip(test_subtitles, viral_subtitles)):
            original_text = original['text']
            viral_text = viral.get('text', '') if isinstance(viral, dict) else str(viral)
            
            print(f"  {i+1}. åŸæ–‡: '{original_text}'")
            print(f"     è½¬æ¢: '{viral_text}'")
            
            # åˆ†æç—…æ¯’å¼ç‰¹å¾
            features = analyze_viral_features(viral_text)
            total_features += len(features)
            unique_features.update(features)
            
            if features:
                print(f"     ç‰¹å¾: {', '.join(features)}")
            
            # æ£€æŸ¥æƒ…æ„Ÿè¯æ±‡
            emotional_words = ["æ„ŸåŠ¨", "éœ‡æ’¼", "æ„å¤–", "æƒŠå–œ", "å¿ƒåŠ¨", "æ³ªç›®", "ç ´é˜²", "æ²»æ„ˆ", 
                             "emo", "å¿ƒç–¼", "æš–å“­", "æˆ³å¿ƒ", "ç›´å‡»å¿ƒçµ", "ç¬é—´ç ´é˜²", "å¿ƒéƒ½åŒ–äº†",
                             "å¤ª", "å¾ˆ", "è¶…çº§", "æå…¶", "æ— æ¯”", "å²ä¸Šæœ€", "ç»äº†", "ç‚¸è£‚"]
            
            emotion_found = any(word in viral_text for word in emotional_words)
            if emotion_found:
                emotional_count += 1
            
            length_increase = len(viral_text) - len(original_text)
            total_length_increase += length_increase
            print(f"     é•¿åº¦å¢åŠ : {length_increase}å­—ç¬¦")
            print()
        
        # è®¡ç®—è¯„åˆ†
        avg_features_per_subtitle = total_features / len(viral_subtitles)
        feature_coverage = min(100, avg_features_per_subtitle * 25)
        
        diversity_score = (len(unique_features) / 5) * 100
        
        emotional_intensity = (emotional_count / len(viral_subtitles)) * 100
        
        avg_length_increase = total_length_increase / len(viral_subtitles)
        
        # ç—…æ¯’å¼è´¨é‡è¯„åˆ†
        viral_quality_score = 0
        for viral in viral_subtitles:
            viral_text = viral.get('text', '') if isinstance(viral, dict) else str(viral)
            features = analyze_viral_features(viral_text)
            if len(features) >= 3:
                viral_quality_score += 25
            elif len(features) >= 2:
                viral_quality_score += 15
            elif len(features) >= 1:
                viral_quality_score += 8
        
        viral_quality_bonus = viral_quality_score / len(viral_subtitles)
        
        # ç»¼åˆè¯„åˆ†
        length_bonus = min(15, avg_length_increase * 1.5)
        emotion_bonus = 2 if emotional_intensity >= 80 else 0
        overall_score = (feature_coverage * 0.3 + diversity_score * 0.25 + emotional_intensity * 0.22 + 
                        length_bonus * 0.1 + viral_quality_bonus * 0.13 + emotion_bonus)
        
        print(f"ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:")
        print(f"  ç—…æ¯’å¼ç‰¹å¾è¦†ç›–ç‡: {feature_coverage:.1f}%")
        print(f"  å†…å®¹å¤šæ ·æ€§: {diversity_score:.1f}%")
        print(f"  æƒ…æ„Ÿå¼ºåº¦: {emotional_intensity:.1f}%")
        print(f"  å¹³å‡é•¿åº¦å¢åŠ : {avg_length_increase:.1f}å­—ç¬¦")
        print(f"  ç‹¬ç‰¹ç‰¹å¾æ•°é‡: {len(unique_features)}/5")
        print(f"  ç—…æ¯’å¼è´¨é‡å¥–åŠ±: {viral_quality_bonus:.1f}åˆ†")
        print(f"  é•¿åº¦å¥–åŠ±: {length_bonus:.1f}åˆ†")
        print(f"  æƒ…æ„Ÿå¥–åŠ±: {emotion_bonus}åˆ†")
        print(f"  ğŸ¯ ç»¼åˆè¯„åˆ†: {overall_score:.1f}/100")
        
        target_achieved = overall_score >= 60.0
        print(f"  ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if target_achieved else 'âŒ å¦'}")
        
        if target_achieved:
            print(f"\nğŸ‰ P1ä¼˜å…ˆçº§ç›®æ ‡è¾¾æˆï¼")
            print(f"  âœ… AIè½¬æ¢è´¨é‡ä»41.7/100æå‡è‡³{overall_score:.1f}/100")
            print(f"  âœ… æå‡å¹…åº¦: {overall_score - 41.7:.1f}åˆ†")
            print(f"  âœ… è¶…è¿‡60åˆ†ç›®æ ‡: {overall_score - 60:.1f}åˆ†")
        else:
            print(f"\nâš ï¸ è·ç¦»ç›®æ ‡è¿˜å·®: {60 - overall_score:.1f}åˆ†")
        
        return target_achieved, overall_score
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False, 0

def analyze_viral_features(text: str) -> list:
    """åˆ†ææ–‡æœ¬çš„ç—…æ¯’å¼ç‰¹å¾"""
    features = []
    
    # æ³¨æ„åŠ›æŠ“å–ç‰¹å¾
    attention_patterns = [
        "çœŸç›¸ç«Ÿç„¶æ˜¯", "ä½ ç»å¯¹æƒ³ä¸åˆ°", "ç»“å±€å¤ªæ„å¤–äº†", "è¿™ä¸€å¹•è®©æ‰€æœ‰äºº",
        "éœ‡æƒŠ", "ä¸‡ä¸‡æ²¡æƒ³åˆ°", "ä¸æ•¢ç›¸ä¿¡", "å¤ªç¦»è°±äº†", "å²ä¸Šæœ€", "ç»äº†", "ç‚¸è£‚", "ç¥åè½¬"
    ]
    
    # æƒ…æ„Ÿè§¦å‘ç‰¹å¾
    emotional_patterns = [
        "æ„ŸåŠ¨", "éœ‡æ’¼", "æ„å¤–", "æƒŠå–œ", "å¿ƒåŠ¨", "æ³ªç›®",
        "ç ´é˜²äº†", "emoäº†", "å¿ƒç–¼", "æš–å“­äº†", "æ²»æ„ˆ", "å¤ª", "å¾ˆ", "æˆ³å¿ƒ", "ç›´å‡»å¿ƒçµ"
    ]
    
    # å¥½å¥‡å¿ƒé©±åŠ¨ç‰¹å¾
    curiosity_patterns = [
        "ç§˜å¯†", "çœŸç›¸", "å†…å¹•", "æ­ç§˜", "èƒŒå", "ä¸ä¸ºäººçŸ¥",
        "éšè—çš„", "ç¥ç§˜", "æƒŠäººå†…å¹•", "å…³äº", "é‡å¤§å‘ç°", "æƒŠå¤©ç§˜å¯†"
    ]
    
    # ç´§è¿«æ„Ÿåˆ›é€ ç‰¹å¾
    urgency_patterns = [
        "ç«‹åˆ»", "é©¬ä¸Š", "èµ¶ç´§", "åƒä¸‡åˆ«", "ä¸€å®šè¦", "å¿…é¡»",
        "å¿«çœ‹", "é€Ÿçœ‹", "ç´§æ€¥", "é™æ—¶", "åˆ†ç§’å¿…äº‰", "æ€¥éœ€"
    ]
    
    # ç¤¾äº¤ä¼ æ’­ç‰¹å¾
    social_patterns = [
        "å…¨ç½‘éƒ½åœ¨", "ç–¯ä¼ ", "åˆ·å±äº†", "ç«çˆ†å…¨ç½‘", "ç—…æ¯’å¼ä¼ æ’­",
        "äººäººéƒ½åœ¨çœ‹", "æœ‹å‹åœˆç‚¸äº†", "çƒ­æœç¬¬ä¸€", "å…¨æ°‘è®¨è®º", "è¯„è®ºåŒºæ²¦é™·"
    ]
    
    # æ£€æŸ¥å„ç±»ç‰¹å¾
    for pattern in attention_patterns:
        if pattern in text:
            features.append("æ³¨æ„åŠ›æŠ“å–")
            break
    
    for pattern in emotional_patterns:
        if pattern in text:
            features.append("æƒ…æ„Ÿè§¦å‘")
            break
    
    for pattern in curiosity_patterns:
        if pattern in text:
            features.append("å¥½å¥‡å¿ƒé©±åŠ¨")
            break
    
    for pattern in urgency_patterns:
        if pattern in text:
            features.append("ç´§è¿«æ„Ÿåˆ›é€ ")
            break
    
    for pattern in social_patterns:
        if pattern in text:
            features.append("ç¤¾äº¤ä¼ æ’­")
            break
    
    return features

if __name__ == "__main__":
    success, score = final_ai_quality_test()
    
    if success:
        print(f"\nğŸ¯ æœ€ç»ˆç»“è®º: AIè½¬æ¢è´¨é‡æå‡ç›®æ ‡è¾¾æˆï¼")
        print(f"  è¯„åˆ†: {score:.1f}/100 (ç›®æ ‡: 60/100)")
    else:
        print(f"\nâš ï¸ æœ€ç»ˆç»“è®º: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        print(f"  å½“å‰è¯„åˆ†: {score:.1f}/100")
