#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster çœŸå®è®­ç»ƒè½¬æ¢å™¨
å°†æ¨¡æ‹Ÿè®­ç»ƒè½¬æ¢ä¸ºçœŸå®æœºå™¨å­¦ä¹ è®­ç»ƒ
"""

import os
import re
import time
import torch
import logging
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime

class RealTrainingConverter:
    """çœŸå®è®­ç»ƒè½¬æ¢å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è½¬æ¢å™¨"""
        self.project_root = os.path.dirname(os.path.abspath(__file__))
        
        print("ğŸ”„ VisionAI-ClipsMaster çœŸå®è®­ç»ƒè½¬æ¢å™¨")
        print("=" * 50)
    
    def generate_real_zh_trainer_code(self) -> str:
        """ç”ŸæˆçœŸå®çš„ä¸­æ–‡è®­ç»ƒå™¨ä»£ç """
        return '''
    def train(self, training_data: List[Dict[str, Any]], 
              progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒçœŸå®çš„ä¸­æ–‡æ¨¡å‹è®­ç»ƒ"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ä¾èµ–
            try:
                from transformers import (
                    AutoTokenizer, AutoModelForCausalLM, 
                    Trainer, TrainingArguments, DataCollatorForLanguageModeling
                )
                from peft import LoraConfig, get_peft_model, TaskType
                from datasets import Dataset
                import torch
            except ImportError as e:
                return {"success": False, "error": f"ç¼ºå°‘å¿…éœ€ä¾èµ–: {e}"}
            
            if progress_callback:
                progress_callback(0.05, "åˆå§‹åŒ–ä¸­æ–‡è®­ç»ƒç¯å¢ƒ...")
            
            # éªŒè¯è®­ç»ƒæ•°æ®
            if not training_data or len(training_data) == 0:
                return {"success": False, "error": "è®­ç»ƒæ•°æ®ä¸ºç©º"}
            
            if progress_callback:
                progress_callback(0.1, "åŠ è½½ä¸­æ–‡æ¨¡å‹...")
            
            # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ - ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥é€‚é…4GBå†…å­˜
            model_name = "Qwen/Qwen2-1.5B-Instruct"  # ä½¿ç”¨1.5Bç‰ˆæœ¬ä»¥é€‚é…å†…å­˜é™åˆ¶
            
            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    cache_dir="./models/cache"
                )
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if self.use_gpu and torch.cuda.is_available() else torch.float32,
                    device_map="auto" if self.use_gpu and torch.cuda.is_available() else None,
                    trust_remote_code=True,
                    cache_dir="./models/cache"
                )
                
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    
            except Exception as e:
                return {"success": False, "error": f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.2, "é…ç½®LoRAå¾®è°ƒ...")
            
            # 2. é…ç½®LoRA - ä½¿ç”¨é¡¹ç›®é…ç½®çš„å‚æ•°
            try:
                lora_config = LoraConfig(
                    r=16,  # é¡¹ç›®é…ç½®çš„rank
                    lora_alpha=32,  # é¡¹ç›®é…ç½®çš„alpha
                    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                    lora_dropout=0.1,
                    bias="none",
                    task_type=TaskType.CAUSAL_LM
                )
                model = get_peft_model(model, lora_config)
                model.print_trainable_parameters()
                
            except Exception as e:
                return {"success": False, "error": f"LoRAé…ç½®å¤±è´¥: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.3, "å‡†å¤‡è®­ç»ƒæ•°æ®...")
            
            # 3. å‡†å¤‡æ•°æ®é›† - ä½¿ç”¨é¡¹ç›®çš„æ•°æ®å¤„ç†é€»è¾‘
            try:
                processed_data = self.prepare_chinese_data(training_data)
                texts = []
                
                for item in processed_data["samples"]:
                    # æ„å»ºè®­ç»ƒæ–‡æœ¬ - åŸç‰‡â†’çˆ†æ¬¾çš„å­¦ä¹ æ ¼å¼
                    text = f"åŸå§‹å‰§æœ¬: {item['original']}\\nçˆ†æ¬¾å‰§æœ¬: {item['viral']}{tokenizer.eos_token}"
                    texts.append(text)
                
                if len(texts) == 0:
                    return {"success": False, "error": "æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬"}
                
                def tokenize_function(examples):
                    return tokenizer(
                        examples["text"],
                        truncation=True,
                        padding=True,
                        max_length=512,  # é€‚é…å†…å­˜é™åˆ¶
                        return_tensors="pt"
                    )
                
                dataset = Dataset.from_dict({"text": texts})
                tokenized_dataset = dataset.map(
                    tokenize_function, 
                    batched=True,
                    remove_columns=dataset.column_names
                )
                
            except Exception as e:
                return {"success": False, "error": f"æ•°æ®å‡†å¤‡å¤±è´¥: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.4, "é…ç½®è®­ç»ƒå‚æ•°...")
            
            # 4. é…ç½®è®­ç»ƒå‚æ•° - é€‚é…4GBå†…å­˜
            try:
                training_args = TrainingArguments(
                    output_dir="./results_zh",
                    num_train_epochs=3,
                    per_device_train_batch_size=1,  # 4GBå†…å­˜å…¼å®¹
                    gradient_accumulation_steps=8,  # é¡¹ç›®é…ç½®
                    learning_rate=2e-5,
                    warmup_steps=100,
                    logging_steps=10,
                    save_steps=500,
                    save_total_limit=2,
                    prediction_loss_only=True,
                    remove_unused_columns=False,
                    dataloader_pin_memory=False,
                    fp16=self.use_gpu and torch.cuda.is_available(),
                    report_to=None,  # ç¦ç”¨wandbç­‰æŠ¥å‘Š
                    load_best_model_at_end=False,
                    metric_for_best_model=None
                )
                
                # æ•°æ®æ•´ç†å™¨
                data_collator = DataCollatorForLanguageModeling(
                    tokenizer=tokenizer,
                    mlm=False,  # å› æœè¯­è¨€æ¨¡å‹
                )
                
            except Exception as e:
                return {"success": False, "error": f"è®­ç»ƒå‚æ•°é…ç½®å¤±è´¥: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.5, "å¼€å§‹çœŸå®è®­ç»ƒ...")
            
            # 5. åˆ›å»ºè®­ç»ƒå™¨å¹¶æ‰§è¡ŒçœŸå®è®­ç»ƒ
            try:
                trainer = Trainer(
                    model=model,
                    args=training_args,
                    train_dataset=tokenized_dataset,
                    tokenizer=tokenizer,
                    data_collator=data_collator
                )
                
                # æ‰§è¡ŒçœŸå®è®­ç»ƒ - è¿™é‡Œæ˜¯å…³é”®çš„çœŸå®æœºå™¨å­¦ä¹ è¿‡ç¨‹
                train_result = trainer.train()
                
            except Exception as e:
                return {"success": False, "error": f"è®­ç»ƒæ‰§è¡Œå¤±è´¥: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.9, "ä¿å­˜è®­ç»ƒæ¨¡å‹...")
            
            # 6. ä¿å­˜æ¨¡å‹
            try:
                os.makedirs("./results_zh", exist_ok=True)
                trainer.save_model()
                tokenizer.save_pretrained("./results_zh")
                
            except Exception as e:
                return {"success": False, "error": f"æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}"}
            
            end_time = time.time()
            
            # 7. ç”Ÿæˆè®­ç»ƒç»“æœ
            result = {
                "success": True,
                "training_type": "REAL_ML_TRAINING",
                "model_name": self.model_name,
                "language": self.language,
                "training_duration": end_time - start_time,
                "train_loss": float(train_result.training_loss),
                "global_step": train_result.global_step,
                "samples_processed": len(training_data),
                "device": "cuda" if self.use_gpu and torch.cuda.is_available() else "cpu",
                "lora_config": {
                    "r": 16,
                    "lora_alpha": 32,
                    "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"]
                },
                "created_at": datetime.now().isoformat()
            }
            
            if progress_callback:
                progress_callback(1.0, "ä¸­æ–‡æ¨¡å‹è®­ç»ƒå®Œæˆ!")
            
            self.logger.info(f"ä¸­æ–‡æ¨¡å‹è®­ç»ƒå®Œæˆ: æŸå¤±={train_result.training_loss:.4f}, æ­¥æ•°={train_result.global_step}")
            
            return result
            
        except Exception as e:
            error_msg = f"ä¸­æ–‡æ¨¡å‹è®­ç»ƒå¼‚å¸¸: {str(e)}"
            self.logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "training_type": "REAL_ML_TRAINING_FAILED"
            }
'''
    
    def generate_real_en_trainer_code(self) -> str:
        """ç”ŸæˆçœŸå®çš„è‹±æ–‡è®­ç»ƒå™¨ä»£ç """
        return '''
    def train(self, training_data: List[Dict[str, Any]], 
              progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒçœŸå®çš„è‹±æ–‡æ¨¡å‹è®­ç»ƒ"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ä¾èµ–
            try:
                from transformers import (
                    AutoTokenizer, AutoModelForCausalLM, 
                    Trainer, TrainingArguments, DataCollatorForLanguageModeling
                )
                from peft import LoraConfig, get_peft_model, TaskType
                from datasets import Dataset
                import torch
            except ImportError as e:
                return {"success": False, "error": f"Missing required dependencies: {e}"}
            
            if progress_callback:
                progress_callback(0.05, "Initializing English training environment...")
            
            # éªŒè¯è®­ç»ƒæ•°æ®
            if not training_data or len(training_data) == 0:
                return {"success": False, "error": "Training data is empty"}
            
            if progress_callback:
                progress_callback(0.1, "Loading English model...")
            
            # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ - ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥é€‚é…4GBå†…å­˜
            model_name = "microsoft/DialoGPT-medium"  # ä½¿ç”¨mediumç‰ˆæœ¬ä»¥é€‚é…å†…å­˜é™åˆ¶
            
            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    cache_dir="./models/cache"
                )
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if self.use_gpu and torch.cuda.is_available() else torch.float32,
                    device_map="auto" if self.use_gpu and torch.cuda.is_available() else None,
                    cache_dir="./models/cache"
                )
                
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    
            except Exception as e:
                return {"success": False, "error": f"Model loading failed: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.2, "Configuring LoRA fine-tuning...")
            
            # 2. é…ç½®LoRA - ä½¿ç”¨é¡¹ç›®é…ç½®çš„å‚æ•°
            try:
                lora_config = LoraConfig(
                    r=16,  # é¡¹ç›®é…ç½®çš„rank
                    lora_alpha=32,  # é¡¹ç›®é…ç½®çš„alpha
                    target_modules=["c_attn"],  # DialoGPTçš„æ³¨æ„åŠ›æ¨¡å—
                    lora_dropout=0.1,
                    bias="none",
                    task_type=TaskType.CAUSAL_LM
                )
                model = get_peft_model(model, lora_config)
                model.print_trainable_parameters()
                
            except Exception as e:
                return {"success": False, "error": f"LoRA configuration failed: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.3, "Preparing training data...")
            
            # 3. å‡†å¤‡æ•°æ®é›† - ä½¿ç”¨é¡¹ç›®çš„æ•°æ®å¤„ç†é€»è¾‘
            try:
                processed_data = self.prepare_english_data(training_data)
                texts = []
                
                for item in processed_data["samples"]:
                    # æ„å»ºè®­ç»ƒæ–‡æœ¬ - åŸç‰‡â†’çˆ†æ¬¾çš„å­¦ä¹ æ ¼å¼
                    text = f"Original script: {item['original']}\\nViral script: {item['viral']}{tokenizer.eos_token}"
                    texts.append(text)
                
                if len(texts) == 0:
                    return {"success": False, "error": "No valid training samples"}
                
                def tokenize_function(examples):
                    return tokenizer(
                        examples["text"],
                        truncation=True,
                        padding=True,
                        max_length=512,  # é€‚é…å†…å­˜é™åˆ¶
                        return_tensors="pt"
                    )
                
                dataset = Dataset.from_dict({"text": texts})
                tokenized_dataset = dataset.map(
                    tokenize_function, 
                    batched=True,
                    remove_columns=dataset.column_names
                )
                
            except Exception as e:
                return {"success": False, "error": f"Data preparation failed: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.4, "Configuring training parameters...")
            
            # 4. é…ç½®è®­ç»ƒå‚æ•° - é€‚é…4GBå†…å­˜
            try:
                training_args = TrainingArguments(
                    output_dir="./results_en",
                    num_train_epochs=3,
                    per_device_train_batch_size=1,  # 4GBå†…å­˜å…¼å®¹
                    gradient_accumulation_steps=8,  # é¡¹ç›®é…ç½®
                    learning_rate=2e-5,
                    warmup_steps=100,
                    logging_steps=10,
                    save_steps=500,
                    save_total_limit=2,
                    prediction_loss_only=True,
                    remove_unused_columns=False,
                    dataloader_pin_memory=False,
                    fp16=self.use_gpu and torch.cuda.is_available(),
                    report_to=None,  # ç¦ç”¨wandbç­‰æŠ¥å‘Š
                    load_best_model_at_end=False,
                    metric_for_best_model=None
                )
                
                # æ•°æ®æ•´ç†å™¨
                data_collator = DataCollatorForLanguageModeling(
                    tokenizer=tokenizer,
                    mlm=False,  # å› æœè¯­è¨€æ¨¡å‹
                )
                
            except Exception as e:
                return {"success": False, "error": f"Training arguments configuration failed: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.5, "Starting real training...")
            
            # 5. åˆ›å»ºè®­ç»ƒå™¨å¹¶æ‰§è¡ŒçœŸå®è®­ç»ƒ
            try:
                trainer = Trainer(
                    model=model,
                    args=training_args,
                    train_dataset=tokenized_dataset,
                    tokenizer=tokenizer,
                    data_collator=data_collator
                )
                
                # æ‰§è¡ŒçœŸå®è®­ç»ƒ - è¿™é‡Œæ˜¯å…³é”®çš„çœŸå®æœºå™¨å­¦ä¹ è¿‡ç¨‹
                train_result = trainer.train()
                
            except Exception as e:
                return {"success": False, "error": f"Training execution failed: {str(e)}"}
            
            if progress_callback:
                progress_callback(0.9, "Saving trained model...")
            
            # 6. ä¿å­˜æ¨¡å‹
            try:
                os.makedirs("./results_en", exist_ok=True)
                trainer.save_model()
                tokenizer.save_pretrained("./results_en")
                
            except Exception as e:
                return {"success": False, "error": f"Model saving failed: {str(e)}"}
            
            end_time = time.time()
            
            # 7. ç”Ÿæˆè®­ç»ƒç»“æœ
            result = {
                "success": True,
                "training_type": "REAL_ML_TRAINING",
                "model_name": self.model_name,
                "language": self.language,
                "training_duration": end_time - start_time,
                "train_loss": float(train_result.training_loss),
                "global_step": train_result.global_step,
                "samples_processed": len(training_data),
                "device": "cuda" if self.use_gpu and torch.cuda.is_available() else "cpu",
                "lora_config": {
                    "r": 16,
                    "lora_alpha": 32,
                    "target_modules": ["c_attn"]
                },
                "created_at": datetime.now().isoformat()
            }
            
            if progress_callback:
                progress_callback(1.0, "English model training completed!")
            
            self.logger.info(f"English model training completed: loss={train_result.training_loss:.4f}, steps={train_result.global_step}")
            
            return result
            
        except Exception as e:
            error_msg = f"English model training error: {str(e)}"
            self.logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "training_type": "REAL_ML_TRAINING_FAILED"
            }
'''

    def apply_real_training_to_zh_trainer(self) -> bool:
        """å°†çœŸå®è®­ç»ƒä»£ç åº”ç”¨åˆ°ä¸­æ–‡è®­ç»ƒå™¨"""
        print("ğŸ”„ æ›´æ–°ä¸­æ–‡è®­ç»ƒå™¨...")

        zh_trainer_path = "src/training/zh_trainer.py"

        try:
            # è¯»å–å½“å‰æ–‡ä»¶
            with open(zh_trainer_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æ·»åŠ å¿…è¦çš„å¯¼å…¥
            import_additions = '''import torch
from datetime import datetime
'''

            # åœ¨ç°æœ‰å¯¼å…¥åæ·»åŠ æ–°å¯¼å…¥
            if "import torch" not in content:
                content = content.replace(
                    "import logging",
                    f"import logging\n{import_additions}"
                )

            # æ›¿æ¢trainæ–¹æ³•
            real_train_code = self.generate_real_zh_trainer_code()

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢trainæ–¹æ³•
            pattern = r'def train\(self, training_data.*?(?=\n    def|\nclass|\Z)'
            new_content = re.sub(pattern, real_train_code.strip(), content, flags=re.DOTALL)

            # å†™å›æ–‡ä»¶
            with open(zh_trainer_path, 'w', encoding='utf-8') as f:
                f.write(new_content)

            print("  âœ… ä¸­æ–‡è®­ç»ƒå™¨æ›´æ–°æˆåŠŸ")
            return True

        except Exception as e:
            print(f"  âŒ ä¸­æ–‡è®­ç»ƒå™¨æ›´æ–°å¤±è´¥: {e}")
            return False

    def apply_real_training_to_en_trainer(self) -> bool:
        """å°†çœŸå®è®­ç»ƒä»£ç åº”ç”¨åˆ°è‹±æ–‡è®­ç»ƒå™¨"""
        print("ğŸ”„ æ›´æ–°è‹±æ–‡è®­ç»ƒå™¨...")

        en_trainer_path = "src/training/en_trainer.py"

        try:
            # è¯»å–å½“å‰æ–‡ä»¶
            with open(en_trainer_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æ·»åŠ å¿…è¦çš„å¯¼å…¥
            import_additions = '''import torch
from datetime import datetime
'''

            # åœ¨ç°æœ‰å¯¼å…¥åæ·»åŠ æ–°å¯¼å…¥
            if "import torch" not in content:
                content = content.replace(
                    "import logging",
                    f"import logging\n{import_additions}"
                )

            # æ›¿æ¢trainæ–¹æ³•
            real_train_code = self.generate_real_en_trainer_code()

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢trainæ–¹æ³•
            pattern = r'def train\(self, training_data.*?(?=\n    def|\nclass|\Z)'
            new_content = re.sub(pattern, real_train_code.strip(), content, flags=re.DOTALL)

            # å†™å›æ–‡ä»¶
            with open(en_trainer_path, 'w', encoding='utf-8') as f:
                f.write(new_content)

            print("  âœ… è‹±æ–‡è®­ç»ƒå™¨æ›´æ–°æˆåŠŸ")
            return True

        except Exception as e:
            print(f"  âŒ è‹±æ–‡è®­ç»ƒå™¨æ›´æ–°å¤±è´¥: {e}")
            return False

    def test_real_training_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•çœŸå®è®­ç»ƒé›†æˆ"""
        print("ğŸ§ª æµ‹è¯•çœŸå®è®­ç»ƒé›†æˆ...")

        test_result = {
            "success": False,
            "tests": {},
            "errors": []
        }

        try:
            # æµ‹è¯•å¯¼å…¥
            print("  ğŸ“¦ æµ‹è¯•æ¨¡å—å¯¼å…¥...")
            try:
                from src.training.zh_trainer import ZhTrainer
                from src.training.en_trainer import EnTrainer
                test_result["tests"]["import"] = True
                print("    âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
            except Exception as e:
                test_result["tests"]["import"] = False
                test_result["errors"].append(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
                print(f"    âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

            # æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–
            print("  ğŸ¤– æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–...")
            try:
                zh_trainer = ZhTrainer(use_gpu=False)
                en_trainer = EnTrainer(use_gpu=False)
                test_result["tests"]["initialization"] = True
                print("    âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                test_result["tests"]["initialization"] = False
                test_result["errors"].append(f"è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                print(f"    âŒ è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")

            # æµ‹è¯•æ•°æ®å¤„ç†
            print("  ğŸ“Š æµ‹è¯•æ•°æ®å¤„ç†...")
            try:
                test_data = [
                    {"original": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å‰§æœ¬", "viral": "éœ‡æƒŠï¼è¿™ä¸ªå‰§æœ¬å¤ªç²¾å½©äº†"},
                    {"original": "ä¸»è§’é¢ä¸´é€‰æ‹©", "viral": "ä¸æ•¢ç›¸ä¿¡ï¼ä¸»è§’çš„é€‰æ‹©æ”¹å˜ä¸€åˆ‡"}
                ]

                zh_processed = zh_trainer.prepare_chinese_data(test_data)
                en_processed = en_trainer.prepare_english_data(test_data)

                if zh_processed and en_processed:
                    test_result["tests"]["data_processing"] = True
                    print("    âœ… æ•°æ®å¤„ç†æˆåŠŸ")
                else:
                    test_result["tests"]["data_processing"] = False
                    test_result["errors"].append("æ•°æ®å¤„ç†è¿”å›ç©ºç»“æœ")
                    print("    âŒ æ•°æ®å¤„ç†è¿”å›ç©ºç»“æœ")

            except Exception as e:
                test_result["tests"]["data_processing"] = False
                test_result["errors"].append(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
                print(f"    âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")

            # è®¡ç®—æˆåŠŸç‡
            success_count = sum(test_result["tests"].values())
            total_tests = len(test_result["tests"])
            success_rate = success_count / total_tests if total_tests > 0 else 0

            test_result["success"] = success_rate >= 0.8  # 80%ä»¥ä¸ŠæˆåŠŸç‡
            test_result["success_rate"] = success_rate

            print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{total_tests} æˆåŠŸ ({success_rate*100:.1f}%)")

        except Exception as e:
            test_result["errors"].append(f"æµ‹è¯•è¿‡ç¨‹å¼‚å¸¸: {e}")
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹å¼‚å¸¸: {e}")

        return test_result

    def run_conversion(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è½¬æ¢æµç¨‹"""
        print("ğŸš€ å¼€å§‹çœŸå®è®­ç»ƒè½¬æ¢...")

        conversion_result = {
            "success": False,
            "steps_completed": [],
            "errors": [],
            "test_result": None
        }

        try:
            # æ­¥éª¤1: æ›´æ–°ä¸­æ–‡è®­ç»ƒå™¨
            if self.apply_real_training_to_zh_trainer():
                conversion_result["steps_completed"].append("ä¸­æ–‡è®­ç»ƒå™¨æ›´æ–°")
            else:
                conversion_result["errors"].append("ä¸­æ–‡è®­ç»ƒå™¨æ›´æ–°å¤±è´¥")

            # æ­¥éª¤2: æ›´æ–°è‹±æ–‡è®­ç»ƒå™¨
            if self.apply_real_training_to_en_trainer():
                conversion_result["steps_completed"].append("è‹±æ–‡è®­ç»ƒå™¨æ›´æ–°")
            else:
                conversion_result["errors"].append("è‹±æ–‡è®­ç»ƒå™¨æ›´æ–°å¤±è´¥")

            # æ­¥éª¤3: æµ‹è¯•é›†æˆ
            test_result = self.test_real_training_integration()
            conversion_result["test_result"] = test_result

            if test_result["success"]:
                conversion_result["steps_completed"].append("é›†æˆæµ‹è¯•")
                conversion_result["success"] = True
            else:
                conversion_result["errors"].append("é›†æˆæµ‹è¯•å¤±è´¥")
                conversion_result["errors"].extend(test_result["errors"])

            return conversion_result

        except Exception as e:
            conversion_result["errors"].append(f"è½¬æ¢æµç¨‹å¼‚å¸¸: {str(e)}")
            return conversion_result

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ VisionAI-ClipsMaster çœŸå®è®­ç»ƒè½¬æ¢")
    print("=" * 50)

    converter = RealTrainingConverter()
    result = converter.run_conversion()

    print("\nğŸ“Š è½¬æ¢ç»“æœ:")
    print(f"âœ… æˆåŠŸ: {'æ˜¯' if result['success'] else 'å¦'}")
    print(f"ğŸ“‹ å®Œæˆæ­¥éª¤: {', '.join(result['steps_completed'])}")

    if result["errors"]:
        print("âŒ é”™è¯¯:")
        for error in result["errors"]:
            print(f"  â€¢ {error}")

    if result["test_result"]:
        print(f"ğŸ§ª æµ‹è¯•æˆåŠŸç‡: {result['test_result']['success_rate']*100:.1f}%")

    if result["success"]:
        print("\nğŸ‰ çœŸå®è®­ç»ƒè½¬æ¢æˆåŠŸï¼")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨çœŸå®çš„æœºå™¨å­¦ä¹ è®­ç»ƒäº†ã€‚")
    else:
        print("\nâš ï¸ è½¬æ¢è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")

    return result

if __name__ == "__main__":
    main()
