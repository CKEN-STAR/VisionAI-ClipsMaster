#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster ç»¼åˆæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
æ±‡æ€»æ‰€æœ‰æµ‹è¯•ç»“æœï¼Œæä¾›æ€§èƒ½æŒ‡æ ‡ã€é—®é¢˜åˆ†æå’Œæ”¹è¿›å»ºè®®
"""

import sys
import os
import json
import glob
from datetime import datetime
from pathlib import Path

class ComprehensiveTestReportGenerator:
    """ç»¼åˆæµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.report_data = {
            "report_generation_time": datetime.now().isoformat(),
            "test_summary": {},
            "performance_metrics": {},
            "issues_analysis": {},
            "recommendations": {},
            "verification_standards": {}
        }
        
    def load_test_reports(self):
        """åŠ è½½æ‰€æœ‰æµ‹è¯•æŠ¥å‘Š"""
        report_files = {
            "comprehensive": glob.glob("comprehensive_test_report_*.json"),
            "screenplay_reconstruction": glob.glob("screenplay_reconstruction_test_report_*.json"),
            "video_processing": glob.glob("video_processing_test_report_*.json"),
            "performance_memory": glob.glob("performance_memory_test_report_*.json"),
            "stability_exception": glob.glob("stability_exception_test_report_*.json")
        }
        
        loaded_reports = {}
        
        for category, files in report_files.items():
            if files:
                # å–æœ€æ–°çš„æŠ¥å‘Šæ–‡ä»¶
                latest_file = max(files, key=os.path.getctime)
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        loaded_reports[category] = json.load(f)
                    print(f"å·²åŠ è½½ {category} æµ‹è¯•æŠ¥å‘Š: {latest_file}")
                except Exception as e:
                    print(f"åŠ è½½ {category} æŠ¥å‘Šå¤±è´¥: {e}")
            else:
                print(f"æœªæ‰¾åˆ° {category} æµ‹è¯•æŠ¥å‘Š")
        
        return loaded_reports
    
    def analyze_test_results(self, reports):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        total_tests = 0
        total_passed = 0
        total_failed = 0
        
        test_categories = {}
        
        for category, report in reports.items():
            if "summary" in report:
                summary = report["summary"]
                category_tests = summary.get("total_tests", 0)
                category_passed = summary.get("passed_tests", 0)
                category_failed = summary.get("failed_tests", 0)
                category_pass_rate = summary.get("pass_rate", 0)
                
                test_categories[category] = {
                    "total_tests": category_tests,
                    "passed_tests": category_passed,
                    "failed_tests": category_failed,
                    "pass_rate": category_pass_rate
                }
                
                total_tests += category_tests
                total_passed += category_passed
                total_failed += category_failed
        
        overall_pass_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        self.report_data["test_summary"] = {
            "total_tests": total_tests,
            "total_passed": total_passed,
            "total_failed": total_failed,
            "overall_pass_rate": round(overall_pass_rate, 2),
            "categories": test_categories
        }
        
        return overall_pass_rate
    
    def extract_performance_metrics(self, reports):
        """æå–æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        # ä»æ€§èƒ½æµ‹è¯•æŠ¥å‘Šä¸­æå–æŒ‡æ ‡
        if "performance_memory" in reports:
            perf_report = reports["performance_memory"]
            if "tests" in perf_report:
                tests = perf_report["tests"]
                
                # å†…å­˜ä½¿ç”¨æŒ‡æ ‡
                if "å†…å­˜åŸºçº¿æµ‹è¯•" in tests:
                    memory_test = tests["å†…å­˜åŸºçº¿æµ‹è¯•"]
                    if memory_test.get("status") == "PASS" and "details" in memory_test:
                        details = memory_test["details"]
                        metrics["memory"] = {
                            "total_gb": details.get("system_memory", {}).get("total_gb", 0),
                            "available_gb": details.get("system_memory", {}).get("available_gb", 0),
                            "usage_percent": details.get("system_memory", {}).get("percent", 0),
                            "meets_4gb_requirement": details.get("meets_4gb_requirement", False)
                        }
                
                # UIå“åº”æ—¶é—´
                if "UIå“åº”æ—¶é—´æµ‹è¯•" in tests:
                    ui_test = tests["UIå“åº”æ—¶é—´æµ‹è¯•"]
                    if ui_test.get("status") == "PASS" and "details" in ui_test:
                        details = ui_test["details"]
                        metrics["ui_performance"] = {
                            "creation_time_ms": details.get("component_creation_time", 0),
                            "response_time_ms": details.get("component_response_time", 0),
                            "total_time_ms": details.get("total_ui_time", 0),
                            "meets_1s_requirement": details.get("meets_1s_requirement", False)
                        }
                
                # æ¨¡å‹åˆ‡æ¢æ€§èƒ½
                if "æ¨¡å‹åˆ‡æ¢æ€§èƒ½æµ‹è¯•" in tests:
                    switch_test = tests["æ¨¡å‹åˆ‡æ¢æ€§èƒ½æµ‹è¯•"]
                    if switch_test.get("status") == "PASS" and "details" in switch_test:
                        details = switch_test["details"]
                        metrics["model_switching"] = {
                            "chinese_detection_ms": details.get("chinese_detection_time", 0),
                            "english_detection_ms": details.get("english_detection_time", 0),
                            "average_switch_ms": details.get("average_switch_time", 0),
                            "meets_1_5s_requirement": details.get("meets_1_5s_requirement", False)
                        }
        
        # ä»ç¨³å®šæ€§æµ‹è¯•æŠ¥å‘Šä¸­æå–æŒ‡æ ‡
        if "stability_exception" in reports:
            stability_report = reports["stability_exception"]
            if "tests" in stability_report:
                tests = stability_report["tests"]
                
                # ç¨³å®šæ€§æŒ‡æ ‡
                if "çŸ­æœŸç¨³å®šæ€§æµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼‰" in tests:
                    stability_test = tests["çŸ­æœŸç¨³å®šæ€§æµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼‰"]
                    if stability_test.get("status") == "PASS" and "details" in stability_test:
                        details = stability_test["details"]
                        metrics["stability"] = {
                            "duration_seconds": details.get("actual_duration_seconds", 0),
                            "total_operations": details.get("total_operations", 0),
                            "operations_per_second": details.get("operations_per_second", 0),
                            "error_rate_percent": details.get("error_rate_percent", 0),
                            "stability_achieved": details.get("stability_achieved", False)
                        }
        
        self.report_data["performance_metrics"] = metrics
    
    def analyze_issues(self, reports):
        """åˆ†æå‘ç°çš„é—®é¢˜"""
        issues = []
        
        for category, report in reports.items():
            if "tests" in report:
                for test_name, test_result in report["tests"].items():
                    if test_result.get("status") == "FAIL":
                        issue = {
                            "category": category,
                            "test_name": test_name,
                            "error": test_result.get("error", "æœªçŸ¥é”™è¯¯"),
                            "details": test_result.get("details", {}),
                            "severity": self._assess_severity(test_name, test_result)
                        }
                        issues.append(issue)
        
        self.report_data["issues_analysis"] = {
            "total_issues": len(issues),
            "issues": issues,
            "critical_issues": [i for i in issues if i["severity"] == "critical"],
            "major_issues": [i for i in issues if i["severity"] == "major"],
            "minor_issues": [i for i in issues if i["severity"] == "minor"]
        }
    
    def _assess_severity(self, test_name, test_result):
        """è¯„ä¼°é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
        error = str(test_result.get("error", "")).lower()
        
        # å…³é”®é—®é¢˜
        if any(keyword in test_name.lower() for keyword in ["å†…å­˜", "memory", "ç¨³å®šæ€§", "stability"]):
            return "critical"
        
        # ä¸»è¦é—®é¢˜
        if any(keyword in test_name.lower() for keyword in ["æ¨¡å‹", "model", "åˆ‡æ¢", "switch"]):
            return "major"
        
        # æ¬¡è¦é—®é¢˜
        return "minor"
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºé—®é¢˜åˆ†æç”Ÿæˆå»ºè®®
        issues = self.report_data["issues_analysis"]
        
        if issues["critical_issues"]:
            recommendations.append({
                "priority": "é«˜",
                "category": "å†…å­˜ä¼˜åŒ–",
                "description": "å‘ç°å…³é”®å†…å­˜é—®é¢˜ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ä½¿ç”¨ç­–ç•¥",
                "actions": [
                    "å®ç°æ›´æ¿€è¿›çš„é‡åŒ–ç­–ç•¥ï¼ˆQ2_Kï¼‰",
                    "ä¼˜åŒ–æ¨¡å‹åˆ†ç‰‡åŠ è½½æœºåˆ¶",
                    "å¢å¼ºå†…å­˜ç›‘æ§å’Œè‡ªåŠ¨æ¸…ç†åŠŸèƒ½"
                ]
            })
        
        if issues["major_issues"]:
            recommendations.append({
                "priority": "ä¸­",
                "category": "æ¨¡å‹æ€§èƒ½",
                "description": "æ¨¡å‹åˆ‡æ¢æˆ–åŠ è½½å­˜åœ¨æ€§èƒ½é—®é¢˜",
                "actions": [
                    "ä¼˜åŒ–æ¨¡å‹é¢„åŠ è½½ç­–ç•¥",
                    "å®ç°æ¨¡å‹ç¼“å­˜æœºåˆ¶",
                    "æ”¹è¿›è¯­è¨€æ£€æµ‹ç®—æ³•"
                ]
            })
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡ç”Ÿæˆå»ºè®®
        metrics = self.report_data["performance_metrics"]
        
        if "memory" in metrics and not metrics["memory"].get("meets_4gb_requirement", True):
            recommendations.append({
                "priority": "é«˜",
                "category": "å†…å­˜å…¼å®¹æ€§",
                "description": "ç³»ç»Ÿå†…å­˜ä½¿ç”¨è¶…å‡º4GBé™åˆ¶",
                "actions": [
                    "å¯ç”¨æ›´æ¿€è¿›çš„é‡åŒ–æ¨¡å¼",
                    "å®ç°åŠ¨æ€å†…å­˜ç®¡ç†",
                    "ä¼˜åŒ–æ•°æ®ç»“æ„å’Œç®—æ³•"
                ]
            })
        
        if "ui_performance" in metrics and not metrics["ui_performance"].get("meets_1s_requirement", True):
            recommendations.append({
                "priority": "ä¸­",
                "category": "UIæ€§èƒ½",
                "description": "UIå“åº”æ—¶é—´è¶…å‡º1ç§’è¦æ±‚",
                "actions": [
                    "ä¼˜åŒ–UIç»„ä»¶åŠ è½½",
                    "å®ç°å¼‚æ­¥UIæ›´æ–°",
                    "å‡å°‘UIåˆå§‹åŒ–å¼€é”€"
                ]
            })
        
        self.report_data["recommendations"] = recommendations
    
    def check_verification_standards(self):
        """æ£€æŸ¥éªŒè¯æ ‡å‡†"""
        standards = {
            "core_functionality_pass_rate": {
                "target": 98.0,
                "actual": self.report_data["test_summary"].get("overall_pass_rate", 0),
                "status": "PASS" if self.report_data["test_summary"].get("overall_pass_rate", 0) >= 98.0 else "FAIL"
            },
            "memory_peak_limit": {
                "target": 3.8,
                "actual": self.report_data["performance_metrics"].get("memory", {}).get("usage_percent", 0) / 100 * 
                         self.report_data["performance_metrics"].get("memory", {}).get("total_gb", 16),
                "status": "UNKNOWN"  # éœ€è¦æ›´è¯¦ç»†çš„å†…å­˜å³°å€¼æ•°æ®
            },
            "ui_response_time": {
                "target": 1000,  # æ¯«ç§’
                "actual": self.report_data["performance_metrics"].get("ui_performance", {}).get("total_time_ms", 0),
                "status": "PASS" if self.report_data["performance_metrics"].get("ui_performance", {}).get("meets_1s_requirement", False) else "FAIL"
            },
            "model_switching_delay": {
                "target": 1500,  # æ¯«ç§’
                "actual": max(
                    self.report_data["performance_metrics"].get("model_switching", {}).get("chinese_detection_ms", 0),
                    self.report_data["performance_metrics"].get("model_switching", {}).get("english_detection_ms", 0)
                ),
                "status": "PASS" if self.report_data["performance_metrics"].get("model_switching", {}).get("meets_1_5s_requirement", False) else "FAIL"
            }
        }
        
        self.report_data["verification_standards"] = standards
    
    def generate_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        print("=" * 80)
        print("VisionAI-ClipsMaster ç»¼åˆåŠŸèƒ½æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        # åŠ è½½æ‰€æœ‰æµ‹è¯•æŠ¥å‘Š
        reports = self.load_test_reports()
        
        if not reports:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶")
            return
        
        # åˆ†ææµ‹è¯•ç»“æœ
        overall_pass_rate = self.analyze_test_results(reports)
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        self.extract_performance_metrics(reports)
        
        # åˆ†æé—®é¢˜
        self.analyze_issues(reports)
        
        # ç”Ÿæˆå»ºè®®
        self.generate_recommendations()
        
        # æ£€æŸ¥éªŒè¯æ ‡å‡†
        self.check_verification_standards()
        
        # è¾“å‡ºæŠ¥å‘Š
        self.print_report()
        
        # ä¿å­˜æŠ¥å‘Š
        self.save_report()
        
        return self.report_data
    
    def print_report(self):
        """æ‰“å°æŠ¥å‘Šå†…å®¹"""
        print("\nğŸ“Š æµ‹è¯•æ€»ç»“")
        print("-" * 40)
        summary = self.report_data["test_summary"]
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"é€šè¿‡æµ‹è¯•: {summary['total_passed']}")
        print(f"å¤±è´¥æµ‹è¯•: {summary['total_failed']}")
        print(f"æ€»ä½“é€šè¿‡ç‡: {summary['overall_pass_rate']:.2f}%")
        
        print("\nğŸ“ˆ å„æ¨¡å—æµ‹è¯•ç»“æœ")
        print("-" * 40)
        for category, stats in summary["categories"].items():
            status = "âœ…" if stats["pass_rate"] == 100 else "âš ï¸" if stats["pass_rate"] >= 80 else "âŒ"
            print(f"{status} {category}: {stats['passed_tests']}/{stats['total_tests']} ({stats['pass_rate']:.1f}%)")
        
        print("\nâš¡ æ€§èƒ½æŒ‡æ ‡")
        print("-" * 40)
        metrics = self.report_data["performance_metrics"]
        
        if "memory" in metrics:
            mem = metrics["memory"]
            print(f"å†…å­˜ä½¿ç”¨: {mem.get('usage_percent', 0):.1f}% ({mem.get('available_gb', 0):.1f}GBå¯ç”¨)")
        
        if "ui_performance" in metrics:
            ui = metrics["ui_performance"]
            print(f"UIå“åº”æ—¶é—´: {ui.get('total_time_ms', 0):.1f}ms")
        
        if "stability" in metrics:
            stab = metrics["stability"]
            print(f"ç¨³å®šæ€§æµ‹è¯•: {stab.get('total_operations', 0)}æ¬¡æ“ä½œï¼Œé”™è¯¯ç‡{stab.get('error_rate_percent', 0):.2f}%")
        
        print("\nğŸ” éªŒè¯æ ‡å‡†æ£€æŸ¥")
        print("-" * 40)
        standards = self.report_data["verification_standards"]
        for standard, data in standards.items():
            status = "âœ…" if data["status"] == "PASS" else "âŒ" if data["status"] == "FAIL" else "â“"
            print(f"{status} {standard}: {data['actual']} (ç›®æ ‡: {data['target']})")
        
        issues = self.report_data["issues_analysis"]
        if issues["total_issues"] > 0:
            print(f"\nâš ï¸ å‘ç°é—®é¢˜")
            print("-" * 40)
            print(f"å…³é”®é—®é¢˜: {len(issues['critical_issues'])}ä¸ª")
            print(f"ä¸»è¦é—®é¢˜: {len(issues['major_issues'])}ä¸ª")
            print(f"æ¬¡è¦é—®é¢˜: {len(issues['minor_issues'])}ä¸ª")
        
        recommendations = self.report_data["recommendations"]
        if recommendations:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®")
            print("-" * 40)
            for rec in recommendations:
                print(f"[{rec['priority']}] {rec['category']}: {rec['description']}")
    
    def save_report(self):
        """ä¿å­˜ç»¼åˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"VisionAI_ClipsMaster_Comprehensive_Test_Report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ ç»¼åˆæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

if __name__ == "__main__":
    generator = ComprehensiveTestReportGenerator()
    generator.generate_report()
