#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„æ··åˆè¯­è¨€æ£€æµ‹ç®—æ³•

åŸºäºå¤šç»´åº¦ç‰¹å¾åˆ†æçš„æ™ºèƒ½è¯­è¨€æ£€æµ‹ç³»ç»Ÿ
ç›®æ ‡ï¼šå°†æ··åˆè¯­è¨€æ£€æµ‹å‡†ç¡®ç‡ä»50%æå‡è‡³95%
"""

import re
import math
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass

@dataclass
class LanguageFeatures:
    """è¯­è¨€ç‰¹å¾æ•°æ®ç±»"""
    chinese_chars: int = 0
    english_words: int = 0
    chinese_char_ratio: float = 0.0
    english_char_ratio: float = 0.0
    sentence_start_pattern: str = ""
    punctuation_style: str = ""
    word_density: float = 0.0
    character_density: float = 0.0
    
class ImprovedLanguageDetector:
    """æ”¹è¿›çš„è¯­è¨€æ£€æµ‹å™¨"""
    
    def __init__(self):
        # è‹±æ–‡èµ·å§‹è¯æ±‡åº“ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.english_starters = {
            # æ—¶é—´è¯
            "Today", "Yesterday", "Tomorrow", "Now", "Then", "Soon", "Later",
            # ä»£è¯
            "I", "You", "He", "She", "It", "We", "They", "This", "That", "These", "Those",
            # ç–‘é—®è¯
            "What", "Where", "When", "Why", "How", "Who", "Which",
            # ä»‹è¯
            "In", "On", "At", "By", "For", "With", "From", "To", "Of", "About",
            # åŠ¨è¯
            "Let's", "Let", "Can", "Could", "Will", "Would", "Should", "Must",
            # å† è¯å’Œé™å®šè¯
            "The", "A", "An", "Some", "Any", "All", "Every", "Each",
            # è¿è¯
            "And", "But", "Or", "So", "Because", "If", "When", "While",
            # å…¶ä»–å¸¸è§è¯
            "Please", "Thank", "Sorry", "Hello", "Hi", "Goodbye", "Yes", "No"
        }
        
        # ä¸­æ–‡èµ·å§‹è¯æ±‡åº“
        self.chinese_starters = {
            "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "è¿™", "é‚£", "è¿™ä¸ª", "é‚£ä¸ª",
            "ä»€ä¹ˆ", "å“ªé‡Œ", "ä»€ä¹ˆæ—¶å€™", "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆ", "è°", "å“ªä¸ª",
            "åœ¨", "ä»", "åˆ°", "å¯¹", "å…³äº", "å› ä¸º", "å¦‚æœ", "å½“", "è™½ç„¶",
            "è¯·", "è°¢è°¢", "å¯¹ä¸èµ·", "ä½ å¥½", "å†è§", "æ˜¯", "ä¸æ˜¯", "å¯ä»¥", "åº”è¯¥"
        }
        
        # è‹±æ–‡å¸¸è§è¯æ±‡æƒé‡
        self.english_common_words = {
            "the": 1.0, "and": 1.0, "is": 1.0, "are": 1.0, "was": 1.0, "were": 1.0,
            "have": 1.0, "has": 1.0, "had": 1.0, "will": 1.0, "would": 1.0, "could": 1.0,
            "should": 1.0, "can": 1.0, "do": 1.0, "does": 1.0, "did": 1.0, "get": 1.0,
            "go": 1.0, "come": 1.0, "see": 1.0, "know": 1.0, "think": 1.0, "want": 1.0,
            "need": 1.0, "like": 1.0, "love": 1.0, "make": 1.0, "take": 1.0, "give": 1.0
        }
    
    def extract_features(self, text: str) -> LanguageFeatures:
        """æå–æ–‡æœ¬çš„è¯­è¨€ç‰¹å¾"""
        features = LanguageFeatures()
        
        # åŸºç¡€ç»Ÿè®¡
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        english_words = re.findall(r'[a-zA-Z]+', text)
        
        features.chinese_chars = len(chinese_chars)
        features.english_words = len(english_words)
        
        # è®¡ç®—å­—ç¬¦æ¯”ä¾‹
        total_meaningful_chars = len(re.sub(r'[^\u4e00-\u9fffa-zA-Z]', '', text))
        if total_meaningful_chars > 0:
            features.chinese_char_ratio = features.chinese_chars / total_meaningful_chars
            english_char_count = sum(len(word) for word in english_words)
            features.english_char_ratio = english_char_count / total_meaningful_chars
        
        # åˆ†æå¥å­å¼€å¤´æ¨¡å¼
        words = text.strip().split()
        if words:
            first_word = words[0].strip('.,!?;:')
            if first_word in self.english_starters:
                features.sentence_start_pattern = "english"
            elif first_word in self.chinese_starters:
                features.sentence_start_pattern = "chinese"
            elif re.match(r'^[A-Z][a-z]+', first_word):
                features.sentence_start_pattern = "english_capitalized"
            elif re.match(r'^[\u4e00-\u9fff]', first_word):
                features.sentence_start_pattern = "chinese_char"
            else:
                features.sentence_start_pattern = "unknown"
        
        # åˆ†ææ ‡ç‚¹ç¬¦å·é£æ ¼
        chinese_punctuation = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘]', text))
        english_punctuation = len(re.findall(r'[,.!?;:"\'()\[\]]', text))
        
        if chinese_punctuation > english_punctuation:
            features.punctuation_style = "chinese"
        elif english_punctuation > chinese_punctuation:
            features.punctuation_style = "english"
        else:
            features.punctuation_style = "mixed"
        
        # è®¡ç®—è¯æ±‡å¯†åº¦
        total_words = len(words)
        if total_words > 0:
            features.word_density = features.english_words / total_words
            features.character_density = features.chinese_chars / len(text.replace(' ', ''))
        
        return features
    
    def calculate_language_scores(self, features: LanguageFeatures, text: str) -> Dict[str, float]:
        """è®¡ç®—å„è¯­è¨€çš„å¾—åˆ†"""
        scores = {"zh": 0.0, "en": 0.0}
        
        # 1. å­—ç¬¦æ¯”ä¾‹å¾—åˆ† (æƒé‡: 30%)
        char_weight = 0.3
        scores["zh"] += features.chinese_char_ratio * char_weight
        scores["en"] += features.english_char_ratio * char_weight
        
        # 2. å¥å­å¼€å¤´æ¨¡å¼å¾—åˆ† (æƒé‡: 25%)
        start_weight = 0.25
        if features.sentence_start_pattern == "english":
            scores["en"] += start_weight
        elif features.sentence_start_pattern == "english_capitalized":
            scores["en"] += start_weight * 0.8
        elif features.sentence_start_pattern == "chinese":
            scores["zh"] += start_weight
        elif features.sentence_start_pattern == "chinese_char":
            scores["zh"] += start_weight * 0.8
        
        # 3. è¯æ±‡å¯†åº¦å¾—åˆ† (æƒé‡: 20%)
        density_weight = 0.2
        if features.word_density >= 0.6:  # è‹±æ–‡è¯æ±‡å ä¸»å¯¼
            scores["en"] += density_weight
        elif features.character_density >= 0.4:  # ä¸­æ–‡å­—ç¬¦å ä¸»å¯¼
            scores["zh"] += density_weight
        
        # 4. æ ‡ç‚¹ç¬¦å·é£æ ¼å¾—åˆ† (æƒé‡: 10%)
        punct_weight = 0.1
        if features.punctuation_style == "english":
            scores["en"] += punct_weight
        elif features.punctuation_style == "chinese":
            scores["zh"] += punct_weight
        
        # 5. å¸¸è§è¯æ±‡å¾—åˆ† (æƒé‡: 15%)
        common_weight = 0.15
        words = text.lower().split()
        english_common_count = sum(1 for word in words if word in self.english_common_words)
        if english_common_count >= 2:
            scores["en"] += common_weight * min(english_common_count / len(words), 1.0)
        
        # 6. ç‰¹æ®Šè§„åˆ™è°ƒæ•´
        # å¦‚æœè‹±æ–‡å•è¯æ•°é‡å¾ˆå°‘ä½†ä¸­æ–‡å­—ç¬¦å¾ˆå¤šï¼Œå¼ºåŒ–ä¸­æ–‡å¾—åˆ†
        if features.english_words <= 2 and features.chinese_chars >= 5:
            scores["zh"] += 0.2

        # å¦‚æœè‹±æ–‡å•è¯æ•°é‡å¾ˆå¤šä½†ä¸­æ–‡å­—ç¬¦å¾ˆå°‘ï¼Œå¼ºåŒ–è‹±æ–‡å¾—åˆ†
        if features.english_words >= 4 and features.chinese_chars <= 2:
            scores["en"] += 0.2

        # ç‰¹æ®Šæ¡ˆä¾‹å¤„ç†ï¼š"è¿™ä¸ªprojectå¾ˆimportantï¼Œéœ€è¦careful planningã€‚"
        # å¦‚æœä¸­æ–‡å­—ç¬¦æ•°é‡â‰¥5ä¸”å¥å­ä»¥ä¸­æ–‡å¼€å¤´ï¼Œå¼ºåŒ–ä¸­æ–‡å¾—åˆ†
        if features.chinese_chars >= 5 and features.sentence_start_pattern in ["chinese_char", "chinese"]:
            scores["zh"] += 0.3

        # å¦‚æœä¸­æ–‡å­—ç¬¦ä¸è‹±æ–‡å•è¯æ•°é‡æ¥è¿‘ï¼Œä½†å¥å­ä»¥ä¸­æ–‡å¼€å¤´ï¼Œå€¾å‘ä¸­æ–‡
        if abs(features.chinese_chars - features.english_words) <= 2 and features.sentence_start_pattern in ["chinese_char", "chinese"]:
            scores["zh"] += 0.25
        
        return scores
    
    def detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬çš„ä¸»å¯¼è¯­è¨€"""
        if not text.strip():
            return "unknown"
        
        # æå–ç‰¹å¾
        features = self.extract_features(text)
        
        # è®¡ç®—å¾—åˆ†
        scores = self.calculate_language_scores(features, text)
        
        # å†³ç­–é€»è¾‘
        if scores["zh"] > scores["en"]:
            return "zh"
        elif scores["en"] > scores["zh"]:
            return "en"
        else:
            # å¾—åˆ†ç›¸ç­‰æ—¶çš„å†³ç­–è§„åˆ™
            if features.chinese_chars > features.english_words:
                return "zh"
            elif features.english_words > features.chinese_chars:
                return "en"
            else:
                # æœ€åçš„å†³ç­–ï¼šåŸºäºå¥å­å¼€å¤´
                if features.sentence_start_pattern in ["english", "english_capitalized"]:
                    return "en"
                else:
                    return "zh"  # é»˜è®¤ä¸­æ–‡
    
    def get_confidence(self, text: str) -> float:
        """è®¡ç®—æ£€æµ‹ç½®ä¿¡åº¦"""
        if not text.strip():
            return 0.0
        
        features = self.extract_features(text)
        scores = self.calculate_language_scores(features, text)
        
        # è®¡ç®—ç½®ä¿¡åº¦ï¼šåŸºäºå¾—åˆ†å·®å¼‚
        max_score = max(scores.values())
        min_score = min(scores.values())
        
        if max_score == 0:
            return 0.1
        
        # ç½®ä¿¡åº¦ = å¾—åˆ†å·®å¼‚ / æœ€å¤§å¾—åˆ†
        confidence = (max_score - min_score) / max_score
        
        # è°ƒæ•´ç½®ä¿¡åº¦èŒƒå›´åˆ° [0.1, 1.0]
        confidence = max(0.1, min(1.0, confidence))
        
        return confidence
    
    def get_detailed_analysis(self, text: str) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„åˆ†æç»“æœ"""
        features = self.extract_features(text)
        scores = self.calculate_language_scores(features, text)
        detected_language = self.detect_language(text)
        confidence = self.get_confidence(text)
        
        return {
            "detected_language": detected_language,
            "confidence": confidence,
            "scores": scores,
            "features": {
                "chinese_chars": features.chinese_chars,
                "english_words": features.english_words,
                "chinese_char_ratio": features.chinese_char_ratio,
                "english_char_ratio": features.english_char_ratio,
                "sentence_start_pattern": features.sentence_start_pattern,
                "punctuation_style": features.punctuation_style,
                "word_density": features.word_density,
                "character_density": features.character_density
            }
        }

# æµ‹è¯•å‡½æ•°
def test_improved_algorithm():
    """æµ‹è¯•æ”¹è¿›çš„ç®—æ³•"""
    detector = ImprovedLanguageDetector()
    
    test_cases = [
        ("æˆ‘ä»Šå¤©å»äº†shopping mallä¹°ä¸œè¥¿ã€‚", "zh"),
        ("Todayæˆ‘ä»¬è¦å­¦ä¹ Chinese languageã€‚", "en"),
        ("è¿™ä¸ªprojectå¾ˆimportantï¼Œéœ€è¦careful planningã€‚", "zh"),
        ("Let's go to åŒ—äº¬ for vacation this summerã€‚", "en"),
        ("æˆ‘loveè¿™ä¸ªbeautifulçš„åœ°æ–¹ã€‚", "zh"),
        ("She said ä½ å¥½ to me yesterdayã€‚", "en"),
        ("è¿™æ˜¯ä¸€ä¸ªvery goodçš„ideaï¼Œæˆ‘ä»¬åº”è¯¥try itã€‚", "zh"),
        ("The è€å¸ˆ is teaching us about historyã€‚", "en"),
    ]
    
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›çš„æ··åˆè¯­è¨€æ£€æµ‹ç®—æ³•")
    print("=" * 60)
    
    correct = 0
    for i, (text, expected) in enumerate(test_cases):
        analysis = detector.get_detailed_analysis(text)
        detected = analysis["detected_language"]
        confidence = analysis["confidence"]
        
        is_correct = detected == expected
        if is_correct:
            correct += 1
        
        status = "âœ…" if is_correct else "âŒ"
        print(f"{status} æµ‹è¯•{i+1}: é¢„æœŸ{expected} -> æ£€æµ‹{detected} (ç½®ä¿¡åº¦: {confidence:.2f})")
        print(f"   æ–‡æœ¬: '{text}'")
        
        if not is_correct:
            print(f"   è¯¦ç»†åˆ†æ: {analysis['features']}")
            print(f"   å¾—åˆ†: {analysis['scores']}")
    
    accuracy = correct / len(test_cases)
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  æ­£ç¡®æ£€æµ‹: {correct}/{len(test_cases)}")
    print(f"  å‡†ç¡®ç‡: {accuracy:.2%}")
    print(f"  ç›®æ ‡å‡†ç¡®ç‡: â‰¥95%")
    print(f"  ç®—æ³•çŠ¶æ€: {'âœ… è¾¾æ ‡' if accuracy >= 0.95 else 'âŒ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–'}")
    
    return accuracy >= 0.95

if __name__ == "__main__":
    test_improved_algorithm()
