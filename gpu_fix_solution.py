#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster GPUé—®é¢˜ä¿®å¤æ–¹æ¡ˆ
åŸºäºè¯Šæ–­ç»“æœæä¾›é’ˆå¯¹æ€§çš„ä¿®å¤å’Œä¼˜åŒ–

ä¿®å¤å†…å®¹:
1. ä¼˜åŒ–GPUæ£€æµ‹é€»è¾‘ï¼Œæ­£ç¡®å¤„ç†é›†æˆæ˜¾å¡
2. ä¿®å¤CPUæ¨¡å¼ä¸‹çš„å†…å­˜ç®¡ç†
3. ä¼˜åŒ–æ— ç‹¬æ˜¾ç¯å¢ƒçš„æ€§èƒ½
4. æä¾›æ¸…æ™°çš„GPUçŠ¶æ€æç¤º
"""

import os
import sys
import platform
import psutil
import logging
from typing import Dict, Any, Optional, List
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

class ImprovedGPUDetector:
    """æ”¹è¿›çš„GPUæ£€æµ‹å™¨ - æ­£ç¡®å¤„ç†é›†æˆæ˜¾å¡å’Œæ— ç‹¬æ˜¾æƒ…å†µ"""
    
    def __init__(self):
        self.detection_results = {
            "gpu_available": False,
            "gpu_type": "none",  # none, integrated, nvidia, amd
            "gpu_name": "æœªæ£€æµ‹åˆ°GPU",
            "gpu_memory_gb": 0,
            "cuda_available": False,
            "recommended_mode": "cpu",  # cpu, gpu, hybrid
            "optimization_suggestions": []
        }
        
    def detect_gpu_comprehensive(self) -> Dict[str, Any]:
        """å…¨é¢GPUæ£€æµ‹ - åŒ…æ‹¬é›†æˆæ˜¾å¡"""
        print("ğŸ” æ‰§è¡Œæ”¹è¿›çš„GPUæ£€æµ‹...")
        
        # 1. æ£€æµ‹PyTorch CUDAæ”¯æŒ
        self._detect_pytorch_cuda()
        
        # 2. æ£€æµ‹ç³»ç»Ÿæ˜¾å¡ï¼ˆåŒ…æ‹¬é›†æˆæ˜¾å¡ï¼‰
        self._detect_system_graphics()
        
        # 3. ç¡®å®šæ¨èè¿è¡Œæ¨¡å¼
        self._determine_recommended_mode()
        
        # 4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        self._generate_optimization_suggestions()
        
        return self.detection_results
    
    def _detect_pytorch_cuda(self):
        """æ£€æµ‹PyTorch CUDAæ”¯æŒ"""
        try:
            import torch
            
            # æ£€æŸ¥CUDAç¼–è¯‘æ”¯æŒ
            cuda_compiled = torch.version.cuda is not None
            cuda_available = torch.cuda.is_available() if cuda_compiled else False
            
            self.detection_results["cuda_available"] = cuda_available
            
            if cuda_available:
                device_count = torch.cuda.device_count()
                if device_count > 0:
                    gpu_name = torch.cuda.get_device_name(0)
                    device_props = torch.cuda.get_device_properties(0)
                    
                    self.detection_results.update({
                        "gpu_available": True,
                        "gpu_type": "nvidia",
                        "gpu_name": gpu_name,
                        "gpu_memory_gb": device_props.total_memory / 1024**3,
                        "recommended_mode": "gpu"
                    })
                    print(f"  âœ… æ£€æµ‹åˆ°NVIDIA GPU: {gpu_name}")
                    return
            
            print(f"  âš ï¸ PyTorch CUDAä¸å¯ç”¨ (ç‰ˆæœ¬: {torch.__version__})")
            
        except ImportError:
            print(f"  âŒ PyTorchæœªå®‰è£…")
        except Exception as e:
            print(f"  âŒ PyTorchæ£€æµ‹å¼‚å¸¸: {e}")
    
    def _detect_system_graphics(self):
        """æ£€æµ‹ç³»ç»Ÿæ˜¾å¡ï¼ˆåŒ…æ‹¬é›†æˆæ˜¾å¡ï¼‰"""
        if platform.system() == "Windows":
            self._detect_windows_graphics()
        else:
            self._detect_linux_graphics()
    
    def _detect_windows_graphics(self):
        """Windowsæ˜¾å¡æ£€æµ‹"""
        try:
            import subprocess
            
            # ä½¿ç”¨wmicæ£€æµ‹æ‰€æœ‰æ˜¾å¡
            result = subprocess.run(
                ["wmic", "path", "win32_VideoController", "get", "name,adapterram", "/format:csv"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                
                for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                    if line.strip() and ',' in line:
                        parts = line.split(',')
                        if len(parts) >= 3:
                            name = parts[2].strip()
                            memory_str = parts[1].strip()
                            
                            if name and name != "Name":
                                # è§£ææ˜¾å­˜å¤§å°
                                try:
                                    memory_bytes = int(memory_str) if memory_str.isdigit() else 0
                                    memory_gb = memory_bytes / 1024**3 if memory_bytes > 0 else 0
                                except:
                                    memory_gb = 0
                                
                                # åˆ¤æ–­æ˜¾å¡ç±»å‹
                                name_upper = name.upper()
                                if any(keyword in name_upper for keyword in ["INTEL", "IRIS", "UHD", "HD GRAPHICS"]):
                                    gpu_type = "integrated"
                                    recommended_mode = "cpu"  # é›†æˆæ˜¾å¡æ¨èCPUæ¨¡å¼
                                elif any(keyword in name_upper for keyword in ["NVIDIA", "GEFORCE", "RTX", "GTX"]):
                                    gpu_type = "nvidia"
                                    recommended_mode = "gpu"
                                elif any(keyword in name_upper for keyword in ["AMD", "RADEON"]):
                                    gpu_type = "amd"
                                    recommended_mode = "gpu"
                                else:
                                    gpu_type = "unknown"
                                    recommended_mode = "cpu"
                                
                                # æ›´æ–°æ£€æµ‹ç»“æœ
                                if not self.detection_results["gpu_available"] or gpu_type in ["nvidia", "amd"]:
                                    self.detection_results.update({
                                        "gpu_available": True,
                                        "gpu_type": gpu_type,
                                        "gpu_name": name,
                                        "gpu_memory_gb": memory_gb,
                                        "recommended_mode": recommended_mode
                                    })
                                
                                print(f"  âœ… æ£€æµ‹åˆ°æ˜¾å¡: {name} ({gpu_type}, {memory_gb:.1f}GB)")
                
        except Exception as e:
            print(f"  âŒ Windowsæ˜¾å¡æ£€æµ‹å¤±è´¥: {e}")
    
    def _detect_linux_graphics(self):
        """Linuxæ˜¾å¡æ£€æµ‹"""
        try:
            import subprocess
            
            # ä½¿ç”¨lspciæ£€æµ‹æ˜¾å¡
            result = subprocess.run(
                ["lspci", "-nn"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                
                for line in lines:
                    if "VGA" in line or "3D" in line:
                        line_upper = line.upper()
                        
                        if "NVIDIA" in line_upper:
                            self.detection_results.update({
                                "gpu_available": True,
                                "gpu_type": "nvidia",
                                "gpu_name": line.split(': ')[1] if ': ' in line else line,
                                "recommended_mode": "gpu"
                            })
                        elif "AMD" in line_upper or "RADEON" in line_upper:
                            self.detection_results.update({
                                "gpu_available": True,
                                "gpu_type": "amd", 
                                "gpu_name": line.split(': ')[1] if ': ' in line else line,
                                "recommended_mode": "gpu"
                            })
                        elif "INTEL" in line_upper:
                            self.detection_results.update({
                                "gpu_available": True,
                                "gpu_type": "integrated",
                                "gpu_name": line.split(': ')[1] if ': ' in line else line,
                                "recommended_mode": "cpu"
                            })
                        
                        print(f"  âœ… æ£€æµ‹åˆ°æ˜¾å¡: {self.detection_results['gpu_name']}")
                
        except Exception as e:
            print(f"  âŒ Linuxæ˜¾å¡æ£€æµ‹å¤±è´¥: {e}")
    
    def _determine_recommended_mode(self):
        """ç¡®å®šæ¨èè¿è¡Œæ¨¡å¼"""
        if self.detection_results["cuda_available"]:
            self.detection_results["recommended_mode"] = "gpu"
        elif self.detection_results["gpu_type"] == "integrated":
            self.detection_results["recommended_mode"] = "cpu"  # é›†æˆæ˜¾å¡æ¨èCPUæ¨¡å¼
        else:
            self.detection_results["recommended_mode"] = "cpu"
    
    def _generate_optimization_suggestions(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        gpu_type = self.detection_results["gpu_type"]
        cuda_available = self.detection_results["cuda_available"]
        
        if gpu_type == "integrated":
            suggestions.extend([
                "æ£€æµ‹åˆ°Intelé›†æˆæ˜¾å¡ï¼Œæ¨èä½¿ç”¨CPUæ¨¡å¼ä»¥è·å¾—æœ€ä½³æ€§èƒ½",
                "å¯ç”¨Intel OpenVINOä¼˜åŒ–ä»¥åŠ é€ŸCPUæ¨ç†",
                "ä½¿ç”¨é‡åŒ–æ¨¡å‹å‡å°‘å†…å­˜å ç”¨"
            ])
        elif gpu_type == "nvidia" and not cuda_available:
            suggestions.extend([
                "æ£€æµ‹åˆ°NVIDIAæ˜¾å¡ä½†CUDAä¸å¯ç”¨ï¼Œå»ºè®®å®‰è£…CUDAç‰ˆæœ¬PyTorch",
                "å®‰è£…æœ€æ–°NVIDIAé©±åŠ¨ç¨‹åº",
                "é…ç½®CUDAç¯å¢ƒå˜é‡"
            ])
        elif gpu_type == "amd":
            suggestions.extend([
                "æ£€æµ‹åˆ°AMDæ˜¾å¡ï¼Œå»ºè®®ä½¿ç”¨ROCmæˆ–OpenCLåŠ é€Ÿ",
                "è€ƒè™‘ä½¿ç”¨CPUæ¨¡å¼ä»¥ç¡®ä¿ç¨³å®šæ€§"
            ])
        elif gpu_type == "none":
            suggestions.extend([
                "æœªæ£€æµ‹åˆ°ç‹¬ç«‹æ˜¾å¡ï¼Œä½¿ç”¨CPUæ¨¡å¼",
                "å¯ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†",
                "ä½¿ç”¨ONNX Runtimeä¼˜åŒ–CPUæ¨ç†"
            ])
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        memory = psutil.virtual_memory()
        if memory.percent > 75:
            suggestions.extend([
                "å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå¯ç”¨æ¿€è¿›å†…å­˜æ¸…ç†ç­–ç•¥",
                "ä½¿ç”¨æ¨¡å‹åˆ†ç‰‡åŠ è½½å‡å°‘å†…å­˜å ç”¨",
                "è€ƒè™‘ä½¿ç”¨swapæ–‡ä»¶æ‰©å±•è™šæ‹Ÿå†…å­˜"
            ])
        
        self.detection_results["optimization_suggestions"] = suggestions

class CPUModeOptimizer:
    """CPUæ¨¡å¼ä¼˜åŒ–å™¨ - é’ˆå¯¹æ— ç‹¬æ˜¾ç¯å¢ƒä¼˜åŒ–"""
    
    def __init__(self):
        self.optimization_config = {
            "memory_limit_gb": 3.8,
            "max_threads": min(psutil.cpu_count(), 8),  # é™åˆ¶çº¿ç¨‹æ•°
            "batch_size": 1,  # CPUæ¨¡å¼ä½¿ç”¨å°æ‰¹æ¬¡
            "enable_quantization": True,
            "enable_memory_mapping": True
        }
    
    def optimize_for_cpu_mode(self) -> Dict[str, Any]:
        """ä¸ºCPUæ¨¡å¼ä¼˜åŒ–ç³»ç»Ÿé…ç½®"""
        print("âš™ï¸ ä¼˜åŒ–CPUæ¨¡å¼é…ç½®...")
        
        optimizations = []
        
        # 1. å†…å­˜ä¼˜åŒ–
        memory_opts = self._optimize_memory_usage()
        optimizations.extend(memory_opts)
        
        # 2. CPUä¼˜åŒ–
        cpu_opts = self._optimize_cpu_usage()
        optimizations.extend(cpu_opts)
        
        # 3. æ¨¡å‹ä¼˜åŒ–
        model_opts = self._optimize_model_loading()
        optimizations.extend(model_opts)
        
        return {
            "config": self.optimization_config,
            "optimizations_applied": optimizations
        }
    
    def _optimize_memory_usage(self) -> List[str]:
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        optimizations = []
        
        # è®¾ç½®å†…å­˜é™åˆ¶
        memory = psutil.virtual_memory()
        if memory.total / 1024**3 <= 6:  # 6GBä»¥ä¸‹è®¾å¤‡
            self.optimization_config["memory_limit_gb"] = 3.0
            optimizations.append("è°ƒæ•´å†…å­˜é™åˆ¶ä¸º3.0GBï¼ˆä½é…è®¾å¤‡ï¼‰")
        
        # å¯ç”¨å†…å­˜æ˜ å°„
        self.optimization_config["enable_memory_mapping"] = True
        optimizations.append("å¯ç”¨å†…å­˜æ˜ å°„å‡å°‘RAMå ç”¨")
        
        # å¯ç”¨é‡åŒ–
        self.optimization_config["enable_quantization"] = True
        optimizations.append("å¯ç”¨æ¨¡å‹é‡åŒ–ï¼ˆQ4_K_Mï¼‰")
        
        return optimizations
    
    def _optimize_cpu_usage(self) -> List[str]:
        """ä¼˜åŒ–CPUä½¿ç”¨"""
        optimizations = []
        
        # è®¾ç½®çº¿ç¨‹æ•°
        cpu_count = psutil.cpu_count()
        if cpu_count >= 8:
            self.optimization_config["max_threads"] = 6
        elif cpu_count >= 4:
            self.optimization_config["max_threads"] = cpu_count - 1
        else:
            self.optimization_config["max_threads"] = cpu_count
        
        optimizations.append(f"è®¾ç½®æœ€å¤§çº¿ç¨‹æ•°ä¸º{self.optimization_config['max_threads']}")
        
        # è®¾ç½®æ‰¹æ¬¡å¤§å°
        self.optimization_config["batch_size"] = 1
        optimizations.append("è®¾ç½®æ‰¹æ¬¡å¤§å°ä¸º1ï¼ˆCPUæ¨¡å¼ä¼˜åŒ–ï¼‰")
        
        return optimizations
    
    def _optimize_model_loading(self) -> List[str]:
        """ä¼˜åŒ–æ¨¡å‹åŠ è½½"""
        optimizations = []
        
        # å¯ç”¨åˆ†ç‰‡åŠ è½½
        self.optimization_config["enable_sharding"] = True
        optimizations.append("å¯ç”¨æ¨¡å‹åˆ†ç‰‡åŠ è½½")
        
        # å¯ç”¨ç¼“å­˜
        self.optimization_config["enable_caching"] = True
        optimizations.append("å¯ç”¨æ¨¡å‹ç¼“å­˜æœºåˆ¶")
        
        return optimizations

def apply_gpu_fixes():
    """åº”ç”¨GPUé—®é¢˜ä¿®å¤"""
    print("ğŸ”§ VisionAI-ClipsMaster GPUé—®é¢˜ä¿®å¤")
    print("=" * 50)
    
    # 1. æ‰§è¡Œæ”¹è¿›çš„GPUæ£€æµ‹
    detector = ImprovedGPUDetector()
    gpu_results = detector.detect_gpu_comprehensive()
    
    print(f"\nğŸ“Š GPUæ£€æµ‹ç»“æœ:")
    print(f"  GPUå¯ç”¨: {'æ˜¯' if gpu_results['gpu_available'] else 'å¦'}")
    print(f"  GPUç±»å‹: {gpu_results['gpu_type']}")
    print(f"  GPUåç§°: {gpu_results['gpu_name']}")
    if gpu_results['gpu_memory_gb'] > 0:
        print(f"  GPUæ˜¾å­˜: {gpu_results['gpu_memory_gb']:.1f}GB")
    print(f"  CUDAå¯ç”¨: {'æ˜¯' if gpu_results['cuda_available'] else 'å¦'}")
    print(f"  æ¨èæ¨¡å¼: {gpu_results['recommended_mode'].upper()}")
    
    # 2. åº”ç”¨CPUæ¨¡å¼ä¼˜åŒ–
    if gpu_results['recommended_mode'] == 'cpu':
        print(f"\nâš™ï¸ åº”ç”¨CPUæ¨¡å¼ä¼˜åŒ–...")
        optimizer = CPUModeOptimizer()
        cpu_results = optimizer.optimize_for_cpu_mode()
        
        print(f"  ä¼˜åŒ–é…ç½®:")
        for key, value in cpu_results['config'].items():
            print(f"    {key}: {value}")
    
    # 3. æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
    suggestions = gpu_results.get('optimization_suggestions', [])
    if suggestions:
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, suggestion in enumerate(suggestions, 1):
            print(f"  {i}. {suggestion}")
    
    # 4. ç”Ÿæˆä¿®å¤æŠ¥å‘Š
    fix_report = {
        "timestamp": psutil.time.time(),
        "gpu_detection": gpu_results,
        "cpu_optimization": cpu_results if gpu_results['recommended_mode'] == 'cpu' else None,
        "system_status": {
            "memory_usage_percent": psutil.virtual_memory().percent,
            "cpu_count": psutil.cpu_count(),
            "platform": platform.system()
        }
    }
    
    return fix_report

if __name__ == "__main__":
    # æ‰§è¡ŒGPUä¿®å¤
    report = apply_gpu_fixes()
    
    # ä¿å­˜ä¿®å¤æŠ¥å‘Š
    import json
    with open("gpu_fix_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ GPUä¿®å¤æŠ¥å‘Šå·²ä¿å­˜åˆ°: gpu_fix_report.json")
    
    # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
    gpu_detection = report["gpu_detection"]
    print(f"\nâœ… ä¿®å¤å®Œæˆ!")
    print(f"ğŸ¯ æ¨èè¿è¡Œæ¨¡å¼: {gpu_detection['recommended_mode'].upper()}")
    print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {report['system_status']['memory_usage_percent']:.1f}%")
    
    if gpu_detection['recommended_mode'] == 'cpu':
        print(f"ğŸ”§ ç³»ç»Ÿå·²ä¼˜åŒ–ä¸ºCPUæ¨¡å¼ï¼Œé€‚åˆ4GBè®¾å¤‡è¿è¡Œ")
    else:
        print(f"ğŸš€ ç³»ç»Ÿå·²é…ç½®GPUåŠ é€Ÿæ¨¡å¼")
