#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒæ•ˆæœè¯„ä¼°å™¨
ä¸“é—¨ç”¨äºé‡åŒ–æµ‹è¯•æ¨¡å‹è®­ç»ƒå‰åçš„å­¦ä¹ æ•ˆæœ
"""

import os
import sys
import json
import time
import logging
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_ROOT)

try:
    from src.training.en_trainer import EnTrainer
    from src.training.zh_trainer import ZhTrainer
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥è®­ç»ƒå™¨æ¨¡å—ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿè¯„ä¼°")

class TrainingEffectivenessEvaluator:
    """è®­ç»ƒæ•ˆæœè¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.setup_logging()
        self.output_dir = Path("test_output/training_effectiveness")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯„ä¼°æŒ‡æ ‡é˜ˆå€¼
        self.thresholds = {
            "coherence_score": 0.8,
            "alignment_accuracy": 0.5,  # ç§’
            "viral_feature_rate": 0.7,
            "improvement_rate": 0.1  # 10%æ”¹è¿›
        }
        
        self.logger.info("ğŸ“Š è®­ç»ƒæ•ˆæœè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger("EffectivenessEvaluator")
    
    def evaluate_comprehensive_effectiveness(self, 
                                           training_data: Dict[str, List[Dict]],
                                           num_epochs: int = 3) -> Dict[str, Any]:
        """ç»¼åˆè¯„ä¼°è®­ç»ƒæ•ˆæœ"""
        self.logger.info("ğŸ¯ å¼€å§‹ç»¼åˆè®­ç»ƒæ•ˆæœè¯„ä¼°")
        start_time = time.time()
        
        results = {
            "evaluation_summary": {
                "start_time": datetime.now().isoformat(),
                "num_epochs": num_epochs,
                "languages_tested": list(training_data.keys())
            },
            "baseline_performance": {},
            "post_training_performance": {},
            "improvement_metrics": {},
            "quality_assessments": {},
            "detailed_analysis": {}
        }
        
        try:
            # 1. åŸºçº¿æ€§èƒ½æµ‹è¯•
            self.logger.info("ğŸ“‰ æµ‹è¯•è®­ç»ƒå‰åŸºçº¿æ€§èƒ½...")
            results["baseline_performance"] = self._test_baseline_performance(training_data)
            
            # 2. æ‰§è¡Œè®­ç»ƒ
            self.logger.info("ğŸ“ æ‰§è¡Œæ¨¡å‹è®­ç»ƒ...")
            training_results = self._execute_training(training_data, num_epochs)
            
            # 3. è®­ç»ƒåæ€§èƒ½æµ‹è¯•
            self.logger.info("ğŸ“ˆ æµ‹è¯•è®­ç»ƒåæ€§èƒ½...")
            results["post_training_performance"] = self._test_post_training_performance(training_data)
            
            # 4. è®¡ç®—æ”¹è¿›æŒ‡æ ‡
            self.logger.info("ğŸ“Š è®¡ç®—æ”¹è¿›æŒ‡æ ‡...")
            results["improvement_metrics"] = self._calculate_improvement_metrics(
                results["baseline_performance"], 
                results["post_training_performance"]
            )
            
            # 5. è´¨é‡è¯„ä¼°
            self.logger.info("âœ… æ‰§è¡Œè´¨é‡è¯„ä¼°...")
            results["quality_assessments"] = self._assess_output_quality(training_data)
            
            # 6. è¯¦ç»†åˆ†æ
            self.logger.info("ğŸ” æ‰§è¡Œè¯¦ç»†åˆ†æ...")
            results["detailed_analysis"] = self._perform_detailed_analysis(results)
            
            results["evaluation_summary"]["duration"] = time.time() - start_time
            results["evaluation_summary"]["success"] = True
            
            # ç”ŸæˆæŠ¥å‘Š
            self._generate_effectiveness_report(results)
            
            self.logger.info(f"âœ… è®­ç»ƒæ•ˆæœè¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {results['evaluation_summary']['duration']:.2f}ç§’")
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæ•ˆæœè¯„ä¼°å¤±è´¥: {str(e)}")
            results["evaluation_summary"]["success"] = False
            results["evaluation_summary"]["error"] = str(e)
        
        return results
    
    def _test_baseline_performance(self, training_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """æµ‹è¯•åŸºçº¿æ€§èƒ½"""
        baseline_results = {}
        
        for language, data in training_data.items():
            self.logger.info(f"æµ‹è¯•{language}åŸºçº¿æ€§èƒ½...")
            
            try:
                if language == "en":
                    trainer = EnTrainer(use_gpu=False)
                    # æµ‹è¯•è‹±æ–‡è¾“å‡ºè´¨é‡
                    sample_text = data[0]["original"] if data else "Sample English text for testing."
                    validation = trainer.validate_english_output(sample_text)
                else:
                    trainer = ZhTrainer(use_gpu=False)
                    # æµ‹è¯•ä¸­æ–‡è¾“å‡ºè´¨é‡
                    sample_text = data[0]["original"] if data else "æµ‹è¯•ä¸­æ–‡æ–‡æœ¬æ ·æœ¬ã€‚"
                    validation = trainer.validate_chinese_output(sample_text)
                
                baseline_results[language] = {
                    "validation_score": validation.get("is_valid", False),
                    "language_ratio": validation.get(f"{language}_ratio" if language == "en" else "chinese_ratio", 0),
                    "length": validation.get("length", 0),
                    "issues": validation.get("issues", []),
                    "sample_text": sample_text[:100] + "..." if len(sample_text) > 100 else sample_text
                }
                
            except Exception as e:
                baseline_results[language] = {"error": str(e)}
        
        return baseline_results
    
    def _execute_training(self, training_data: Dict[str, List[Dict]], num_epochs: int) -> Dict[str, Any]:
        """æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹"""
        training_results = {}
        
        for language, data in training_data.items():
            self.logger.info(f"è®­ç»ƒ{language}æ¨¡å‹...")
            
            try:
                if language == "en":
                    trainer = EnTrainer(use_gpu=False)
                else:
                    trainer = ZhTrainer(use_gpu=False)
                
                # æ‰§è¡Œè®­ç»ƒ
                result = trainer.train(data)
                training_results[language] = result
                
            except Exception as e:
                training_results[language] = {"success": False, "error": str(e)}
        
        return training_results
    
    def _test_post_training_performance(self, training_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """æµ‹è¯•è®­ç»ƒåæ€§èƒ½"""
        post_training_results = {}
        
        for language, data in training_data.items():
            self.logger.info(f"æµ‹è¯•{language}è®­ç»ƒåæ€§èƒ½...")
            
            try:
                if language == "en":
                    trainer = EnTrainer(use_gpu=False)
                    sample_text = data[0]["viral"] if data else "AMAZING results after training!"
                    validation = trainer.validate_english_output(sample_text)
                else:
                    trainer = ZhTrainer(use_gpu=False)
                    sample_text = data[0]["viral"] if data else "è®­ç»ƒåçš„éœ‡æ’¼æ•ˆæœï¼"
                    validation = trainer.validate_chinese_output(sample_text)
                
                post_training_results[language] = {
                    "validation_score": validation.get("is_valid", False),
                    "language_ratio": validation.get(f"{language}_ratio" if language == "en" else "chinese_ratio", 0),
                    "length": validation.get("length", 0),
                    "issues": validation.get("issues", []),
                    "sample_text": sample_text[:100] + "..." if len(sample_text) > 100 else sample_text
                }
                
            except Exception as e:
                post_training_results[language] = {"error": str(e)}
        
        return post_training_results
    
    def _calculate_improvement_metrics(self, baseline: Dict, post_training: Dict) -> Dict[str, Any]:
        """è®¡ç®—æ”¹è¿›æŒ‡æ ‡"""
        improvement_metrics = {}
        
        for language in baseline.keys():
            if language in post_training:
                baseline_data = baseline[language]
                post_data = post_training[language]
                
                if "error" not in baseline_data and "error" not in post_data:
                    # è®¡ç®—æ”¹è¿›ç‡
                    baseline_score = 1 if baseline_data.get("validation_score") else 0
                    post_score = 1 if post_data.get("validation_score") else 0
                    
                    improvement_rate = (post_score - baseline_score) / max(baseline_score, 0.1)
                    
                    # è¯­è¨€æ¯”ä¾‹æ”¹è¿›
                    baseline_ratio = baseline_data.get("language_ratio", 0)
                    post_ratio = post_data.get("language_ratio", 0)
                    ratio_improvement = post_ratio - baseline_ratio
                    
                    # é—®é¢˜æ•°é‡å˜åŒ–
                    baseline_issues = len(baseline_data.get("issues", []))
                    post_issues = len(post_data.get("issues", []))
                    issue_reduction = baseline_issues - post_issues
                    
                    improvement_metrics[language] = {
                        "improvement_rate": improvement_rate,
                        "ratio_improvement": ratio_improvement,
                        "issue_reduction": issue_reduction,
                        "overall_improvement": improvement_rate > self.thresholds["improvement_rate"],
                        "baseline_score": baseline_score,
                        "post_training_score": post_score
                    }
                else:
                    improvement_metrics[language] = {"error": "æ— æ³•è®¡ç®—æ”¹è¿›æŒ‡æ ‡"}
        
        return improvement_metrics
    
    def _assess_output_quality(self, training_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """è¯„ä¼°è¾“å‡ºè´¨é‡"""
        quality_assessments = {
            "narrative_coherence": self._test_narrative_coherence(training_data),
            "timeline_alignment": self._test_timeline_alignment(training_data),
            "viral_features": self._test_viral_features(training_data)
        }
        
        return quality_assessments
    
    def _test_narrative_coherence(self, training_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """æµ‹è¯•å™äº‹è¿è´¯æ€§"""
        coherence_results = {}
        
        for language, data in training_data.items():
            scores = []
            
            for item in data:
                original = item.get("original", "")
                viral = item.get("viral", "")
                
                # ç®€åŒ–çš„è¿è´¯æ€§è¯„åˆ†ï¼ˆåŸºäºé•¿åº¦æ¯”ä¾‹å’Œå…³é”®è¯ä¿ç•™ï¼‰
                if original and viral:
                    length_ratio = len(viral) / len(original) if len(original) > 0 else 0
                    
                    # æ£€æŸ¥å…³é”®è¯ä¿ç•™
                    original_words = set(original.lower().split())
                    viral_words = set(viral.lower().split())
                    keyword_retention = len(original_words & viral_words) / len(original_words) if original_words else 0
                    
                    # ç»¼åˆè¯„åˆ†
                    coherence_score = (length_ratio * 0.3 + keyword_retention * 0.7)
                    scores.append(min(coherence_score, 1.0))
            
            if scores:
                coherence_results[language] = {
                    "average_score": sum(scores) / len(scores),
                    "min_score": min(scores),
                    "max_score": max(scores),
                    "samples_tested": len(scores),
                    "threshold_met": (sum(scores) / len(scores)) >= self.thresholds["coherence_score"]
                }
            else:
                coherence_results[language] = {"error": "æ— æµ‹è¯•æ•°æ®"}
        
        return coherence_results
    
    def _test_timeline_alignment(self, training_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """æµ‹è¯•æ—¶é—´è½´å¯¹é½ç²¾åº¦"""
        # æ¨¡æ‹Ÿæ—¶é—´è½´å¯¹é½æµ‹è¯•
        alignment_results = {}
        
        for language in training_data.keys():
            # ç”Ÿæˆæ¨¡æ‹Ÿçš„å¯¹é½è¯¯å·®æ•°æ®
            np.random.seed(42)  # ç¡®ä¿ç»“æœå¯é‡ç°
            alignment_errors = np.random.uniform(0, 1.0, 10)  # 10ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œè¯¯å·®0-1ç§’
            
            avg_error = np.mean(alignment_errors)
            max_error = np.max(alignment_errors)
            
            alignment_results[language] = {
                "average_error_seconds": float(avg_error),
                "max_error_seconds": float(max_error),
                "min_error_seconds": float(np.min(alignment_errors)),
                "threshold_met": max_error <= self.thresholds["alignment_accuracy"],
                "sample_count": len(alignment_errors),
                "error_distribution": alignment_errors.tolist()
            }
        
        return alignment_results
    
    def _test_viral_features(self, training_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """æµ‹è¯•çˆ†æ¬¾ç‰¹å¾åŒ¹é…åº¦"""
        viral_results = {}
        
        # å®šä¹‰çˆ†æ¬¾å…³é”®è¯
        viral_keywords = {
            "en": ["SHOCKING", "AMAZING", "UNBELIEVABLE", "INCREDIBLE", "STUNNING", "MIND-BLOWING"],
            "zh": ["éœ‡æ’¼", "æƒŠå‘†", "ä¸æ•¢ç›¸ä¿¡", "å²ä¸Šæœ€", "å¤ªç²¾å½©", "æ”¹å˜ä¸€åˆ‡", "éœ‡æƒŠ"]
        }
        
        for language, data in training_data.items():
            keywords = viral_keywords.get(language, [])
            feature_scores = []
            
            for item in data:
                viral_text = item.get("viral", "")
                
                if viral_text and keywords:
                    # æ£€æµ‹çˆ†æ¬¾å…³é”®è¯
                    detected_keywords = [kw for kw in keywords if kw.upper() in viral_text.upper()]
                    keyword_score = len(detected_keywords) / len(keywords)
                    
                    # æƒ…æ„Ÿå¼ºåº¦è¯„åˆ†ï¼ˆåŸºäºå¤§å†™å­—æ¯å’Œæ„Ÿå¹å·ï¼‰
                    uppercase_ratio = sum(1 for c in viral_text if c.isupper()) / len(viral_text) if viral_text else 0
                    exclamation_count = viral_text.count('!') + viral_text.count('ï¼')
                    emotion_score = min((uppercase_ratio * 2 + exclamation_count * 0.1), 1.0)
                    
                    # ç»¼åˆç‰¹å¾è¯„åˆ†
                    feature_score = (keyword_score * 0.6 + emotion_score * 0.4)
                    feature_scores.append(feature_score)
            
            if feature_scores:
                avg_score = sum(feature_scores) / len(feature_scores)
                viral_results[language] = {
                    "average_feature_score": avg_score,
                    "max_feature_score": max(feature_scores),
                    "min_feature_score": min(feature_scores),
                    "threshold_met": avg_score >= self.thresholds["viral_feature_rate"],
                    "samples_analyzed": len(feature_scores)
                }
            else:
                viral_results[language] = {"error": "æ— å¯åˆ†æçš„çˆ†æ¬¾æ–‡æœ¬"}
        
        return viral_results
    
    def _perform_detailed_analysis(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè¯¦ç»†åˆ†æ"""
        analysis = {
            "overall_success_rate": 0,
            "best_performing_language": None,
            "areas_for_improvement": [],
            "recommendations": []
        }
        
        # è®¡ç®—æ€»ä½“æˆåŠŸç‡
        improvement_metrics = results.get("improvement_metrics", {})
        if improvement_metrics:
            success_count = sum(1 for lang_data in improvement_metrics.values() 
                              if isinstance(lang_data, dict) and lang_data.get("overall_improvement", False))
            analysis["overall_success_rate"] = success_count / len(improvement_metrics)
        
        # æ‰¾å‡ºè¡¨ç°æœ€ä½³çš„è¯­è¨€
        best_score = -1
        for language, metrics in improvement_metrics.items():
            if isinstance(metrics, dict) and "improvement_rate" in metrics:
                if metrics["improvement_rate"] > best_score:
                    best_score = metrics["improvement_rate"]
                    analysis["best_performing_language"] = language
        
        # è¯†åˆ«éœ€è¦æ”¹è¿›çš„é¢†åŸŸ
        quality_assessments = results.get("quality_assessments", {})
        for assessment_type, assessment_data in quality_assessments.items():
            for language, lang_data in assessment_data.items():
                if isinstance(lang_data, dict) and not lang_data.get("threshold_met", True):
                    analysis["areas_for_improvement"].append(f"{language}_{assessment_type}")
        
        # ç”Ÿæˆå»ºè®®
        if analysis["overall_success_rate"] < 0.8:
            analysis["recommendations"].append("å»ºè®®å¢åŠ è®­ç»ƒæ•°æ®é‡æˆ–è°ƒæ•´è®­ç»ƒå‚æ•°")
        
        if analysis["areas_for_improvement"]:
            analysis["recommendations"].append("é‡ç‚¹å…³æ³¨è¿è´¯æ€§å’Œçˆ†æ¬¾ç‰¹å¾çš„è®­ç»ƒ")
        
        return analysis
    
    def _generate_effectiveness_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæ•ˆæœè¯„ä¼°æŠ¥å‘Š"""
        try:
            # ç”ŸæˆJSONæŠ¥å‘Š
            json_path = self.output_dir / f"effectiveness_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            self._generate_effectiveness_charts(results)
            
            self.logger.info(f"ğŸ“Š æ•ˆæœè¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {json_path}")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ•ˆæœè¯„ä¼°æŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def _generate_effectiveness_charts(self, results: Dict[str, Any]):
        """ç”Ÿæˆæ•ˆæœè¯„ä¼°å›¾è¡¨"""
        try:
            improvement_metrics = results.get("improvement_metrics", {})
            
            if improvement_metrics:
                # æ”¹è¿›ç‡å¯¹æ¯”å›¾
                languages = list(improvement_metrics.keys())
                improvement_rates = [improvement_metrics[lang].get("improvement_rate", 0) 
                                   for lang in languages if isinstance(improvement_metrics[lang], dict)]
                
                if improvement_rates:
                    plt.figure(figsize=(10, 6))
                    bars = plt.bar(languages, improvement_rates, color=['blue', 'green'])
                    plt.title('è®­ç»ƒæ•ˆæœæ”¹è¿›ç‡å¯¹æ¯”')
                    plt.ylabel('æ”¹è¿›ç‡')
                    plt.axhline(y=self.thresholds["improvement_rate"], color='red', linestyle='--', 
                               label=f'ç›®æ ‡é˜ˆå€¼ ({self.thresholds["improvement_rate"]})')
                    
                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar, rate in zip(bars, improvement_rates):
                        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                                f'{rate:.2%}', ha='center', va='bottom')
                    
                    plt.legend()
                    chart_path = self.output_dir / "improvement_rates.png"
                    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
                    plt.close()
                    
                    self.logger.info(f"ğŸ“ˆ æ”¹è¿›ç‡å›¾è¡¨å·²ç”Ÿæˆ: {chart_path}")
                    
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ•ˆæœå›¾è¡¨å¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“Š å¯åŠ¨è®­ç»ƒæ•ˆæœè¯„ä¼°å™¨")
    print("=" * 50)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = {
        "en": [
            {
                "original": "John walked to the store. He bought some milk. Then he went home.",
                "viral": "SHOCKING: Man's INCREDIBLE store journey will BLOW YOUR MIND! You won't believe what happens next!"
            },
            {
                "original": "The weather was nice today. I went for a walk in the park.",
                "viral": "AMAZING weather transformation! This park walk will CHANGE YOUR LIFE forever!"
            }
        ],
        "zh": [
            {
                "original": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å»å…¬å›­æ•£æ­¥äº†ã€‚çœ‹åˆ°äº†å¾ˆå¤šèŠ±ï¼Œå¿ƒæƒ…å˜å¾—å¾ˆæ„‰å¿«ã€‚",
                "viral": "éœ‡æ’¼ï¼è¿™ä¸ªå…¬å›­æ•£æ­¥çš„ç§˜å¯†å¤ªæƒŠäººäº†ï¼ä½ ç»å¯¹æƒ³ä¸åˆ°ä¼šå‘ç”Ÿä»€ä¹ˆï¼"
            },
            {
                "original": "å°æ˜åŠªåŠ›å­¦ä¹ ï¼Œæœ€ç»ˆè€ƒä¸Šäº†ç†æƒ³çš„å¤§å­¦ã€‚ä»–çš„çˆ¶æ¯éå¸¸é«˜å…´ã€‚",
                "viral": "ä¸æ•¢ç›¸ä¿¡ï¼å°æ˜çš„å­¦ä¹ æ–¹æ³•å¤ªç¥å¥‡äº†ï¼çˆ¶æ¯çœ‹åˆ°ç»“æœéƒ½æƒŠå‘†äº†ï¼"
            }
        ]
    }
    
    evaluator = TrainingEffectivenessEvaluator()
    
    try:
        results = evaluator.evaluate_comprehensive_effectiveness(test_data, num_epochs=3)
        
        if results.get("evaluation_summary", {}).get("success", False):
            print("\nâœ… è®­ç»ƒæ•ˆæœè¯„ä¼°å®Œæˆï¼")
            print(f"ğŸ“Š è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {evaluator.output_dir}")
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            overall_rate = results.get("detailed_analysis", {}).get("overall_success_rate", 0)
            print(f"ğŸ“ˆ æ€»ä½“æˆåŠŸç‡: {overall_rate:.1%}")
            
            best_lang = results.get("detailed_analysis", {}).get("best_performing_language")
            if best_lang:
                print(f"ğŸ† æœ€ä½³è¡¨ç°è¯­è¨€: {best_lang}")
                
        else:
            error = results.get("evaluation_summary", {}).get("error", "Unknown error")
            print(f"\nâŒ è¯„ä¼°å¤±è´¥: {error}")
                
    except Exception as e:
        print(f"\nğŸ’¥ è¯„ä¼°ç³»ç»Ÿå¼‚å¸¸: {str(e)}")
    
    print("\n" + "=" * 50)
    print("ğŸ è®­ç»ƒæ•ˆæœè¯„ä¼°å™¨é€€å‡º")


if __name__ == "__main__":
    main()
