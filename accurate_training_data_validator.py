#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‡†ç¡®çš„è®­ç»ƒæ•°æ®éªŒè¯å™¨
åŸºäºå®é™…çš„è®­ç»ƒæ•°æ®æ ¼å¼è¿›è¡ŒéªŒè¯
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class AccurateTrainingDataValidator:
    """å‡†ç¡®çš„è®­ç»ƒæ•°æ®éªŒè¯å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "validation_summary": {},
            "detailed_analysis": {},
            "quality_assessment": {},
            "recommendations": []
        }
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
    def validate_training_pair_format(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯è®­ç»ƒæ•°æ®å¯¹æ ¼å¼"""
        validation = {
            "is_valid": True,
            "format_score": 100,
            "issues": [],
            "strengths": []
        }
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ["original", "viral"]
        for field in required_fields:
            if field not in data:
                validation["issues"].append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                validation["is_valid"] = False
                validation["format_score"] -= 30
            elif not isinstance(data[field], str) or not data[field].strip():
                validation["issues"].append(f"å­—æ®µ {field} ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
                validation["is_valid"] = False
                validation["format_score"] -= 20
                
        # æ£€æŸ¥å…ƒæ•°æ®
        if "metadata" in data:
            metadata = data["metadata"]
            validation["strengths"].append("åŒ…å«å…ƒæ•°æ®ä¿¡æ¯")
            
            # æ£€æŸ¥å…ƒæ•°æ®å­—æ®µ
            metadata_fields = ["language", "genre", "quality_score"]
            for field in metadata_fields:
                if field in metadata:
                    validation["strengths"].append(f"åŒ…å«{field}ä¿¡æ¯")
                else:
                    validation["issues"].append(f"å…ƒæ•°æ®ç¼ºå°‘{field}å­—æ®µ")
                    validation["format_score"] -= 5
        else:
            validation["issues"].append("ç¼ºå°‘å…ƒæ•°æ®")
            validation["format_score"] -= 10
            
        return validation
        
    def analyze_content_quality(self, original: str, viral: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹è´¨é‡"""
        analysis = {
            "content_metrics": {},
            "transformation_analysis": {},
            "quality_indicators": {},
            "issues": []
        }
        
        # åŸºç¡€å†…å®¹æŒ‡æ ‡
        analysis["content_metrics"] = {
            "original_length": len(original),
            "viral_length": len(viral),
            "length_ratio": len(viral) / len(original) if len(original) > 0 else 0,
            "original_word_count": len(original.split()),
            "viral_word_count": len(viral.split())
        }
        
        # è½¬æ¢åˆ†æ
        length_ratio = analysis["content_metrics"]["length_ratio"]
        if length_ratio > 1.5:
            analysis["transformation_analysis"]["type"] = "æ‰©å±•å‹"
            analysis["transformation_analysis"]["description"] = "çˆ†æ¬¾ç‰ˆæœ¬æ¯”åŸç‰ˆæ›´è¯¦ç»†"
        elif length_ratio < 0.5:
            analysis["transformation_analysis"]["type"] = "å‹ç¼©å‹"
            analysis["transformation_analysis"]["description"] = "çˆ†æ¬¾ç‰ˆæœ¬é«˜åº¦æµ“ç¼©"
        else:
            analysis["transformation_analysis"]["type"] = "é‡æ„å‹"
            analysis["transformation_analysis"]["description"] = "çˆ†æ¬¾ç‰ˆæœ¬é•¿åº¦é€‚ä¸­ï¼Œé‡æ–°ç»„ç»‡"
            
        # è´¨é‡æŒ‡æ ‡
        viral_keywords = ["éœ‡æ’¼", "æƒŠå‘†", "éœ¸é“", "çªç„¶", "ååº”", "æ‰€æœ‰äºº", "ï¼"]
        keyword_count = sum(1 for keyword in viral_keywords if keyword in viral)
        
        analysis["quality_indicators"] = {
            "has_emotional_words": keyword_count > 0,
            "emotional_intensity": keyword_count,
            "has_exclamation": "ï¼" in viral or "!" in viral,
            "has_suspense_elements": any(word in viral for word in ["éœ‡æ’¼", "æƒŠå‘†", "çªç„¶"]),
            "engagement_score": min(keyword_count * 20, 100)
        }
        
        # æ£€æŸ¥æ½œåœ¨é—®é¢˜
        if len(original.strip()) < 10:
            analysis["issues"].append("åŸå§‹å†…å®¹è¿‡çŸ­")
        if len(viral.strip()) < 10:
            analysis["issues"].append("çˆ†æ¬¾å†…å®¹è¿‡çŸ­")
        if original == viral:
            analysis["issues"].append("åŸå§‹å†…å®¹ä¸çˆ†æ¬¾å†…å®¹ç›¸åŒ")
        if keyword_count == 0:
            analysis["issues"].append("çˆ†æ¬¾å†…å®¹ç¼ºä¹å¸å¼•åŠ›å…³é”®è¯")
            
        return analysis
        
    def scan_training_directory(self, dir_path: Path, language: str) -> Dict[str, Any]:
        """æ‰«æè®­ç»ƒæ•°æ®ç›®å½•"""
        results = {
            "language": language,
            "total_files": 0,
            "valid_files": 0,
            "invalid_files": 0,
            "total_pairs": 0,
            "valid_pairs": 0,
            "file_details": [],
            "quality_distribution": {"high": 0, "medium": 0, "low": 0}
        }
        
        if not dir_path.exists():
            self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {dir_path}")
            return results
            
        # æ‰«ææ‰€æœ‰JSONå’ŒTXTæ–‡ä»¶
        data_files = list(dir_path.glob("*.json")) + list(dir_path.glob("*.txt"))
        results["total_files"] = len(data_files)
        
        for file_path in data_files:
            file_result = self._analyze_training_file(file_path, language)
            results["file_details"].append(file_result)
            
            if file_result["is_valid"]:
                results["valid_files"] += 1
                results["valid_pairs"] += file_result["pair_count"]
                
                # è´¨é‡åˆ†å¸ƒç»Ÿè®¡
                avg_quality = file_result.get("average_quality_score", 0)
                if avg_quality >= 80:
                    results["quality_distribution"]["high"] += file_result["pair_count"]
                elif avg_quality >= 60:
                    results["quality_distribution"]["medium"] += file_result["pair_count"]
                else:
                    results["quality_distribution"]["low"] += file_result["pair_count"]
            else:
                results["invalid_files"] += 1
                
            results["total_pairs"] += file_result["pair_count"]
            
        return results
        
    def _analyze_training_file(self, file_path: Path, language: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªè®­ç»ƒæ–‡ä»¶"""
        result = {
            "file_name": file_path.name,
            "language": language,
            "is_valid": False,
            "pair_count": 0,
            "format_issues": [],
            "content_issues": [],
            "quality_scores": [],
            "average_quality_score": 0
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                
            if file_path.suffix == '.json':
                try:
                    data = json.loads(content)
                    
                    if isinstance(data, dict):
                        # å•ä¸ªè®­ç»ƒå¯¹
                        pair_validation = self.validate_training_pair_format(data)
                        result["is_valid"] = pair_validation["is_valid"]
                        result["pair_count"] = 1
                        result["format_issues"] = pair_validation["issues"]
                        
                        if pair_validation["is_valid"]:
                            content_analysis = self.analyze_content_quality(
                                data["original"], data["viral"]
                            )
                            result["content_issues"] = content_analysis["issues"]
                            quality_score = content_analysis["quality_indicators"]["engagement_score"]
                            result["quality_scores"] = [quality_score]
                            result["average_quality_score"] = quality_score
                            
                    elif isinstance(data, list):
                        # å¤šä¸ªè®­ç»ƒå¯¹
                        valid_pairs = 0
                        all_quality_scores = []
                        
                        for item in data:
                            if isinstance(item, dict):
                                pair_validation = self.validate_training_pair_format(item)
                                if pair_validation["is_valid"]:
                                    valid_pairs += 1
                                    content_analysis = self.analyze_content_quality(
                                        item["original"], item["viral"]
                                    )
                                    quality_score = content_analysis["quality_indicators"]["engagement_score"]
                                    all_quality_scores.append(quality_score)
                                else:
                                    result["format_issues"].extend(pair_validation["issues"])
                                    
                        result["pair_count"] = len(data)
                        result["is_valid"] = valid_pairs > 0
                        result["quality_scores"] = all_quality_scores
                        result["average_quality_score"] = sum(all_quality_scores) / len(all_quality_scores) if all_quality_scores else 0
                        
                except json.JSONDecodeError as e:
                    result["format_issues"].append(f"JSONæ ¼å¼é”™è¯¯: {str(e)}")
                    
            else:
                # TXTæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç®€å•æ–‡æœ¬
                if len(content) > 0:
                    result["pair_count"] = 1
                    result["is_valid"] = True
                    # ç®€å•æ–‡æœ¬æ–‡ä»¶çš„è´¨é‡è¯„ä¼°
                    result["quality_scores"] = [50]  # é»˜è®¤ä¸­ç­‰è´¨é‡
                    result["average_quality_score"] = 50
                else:
                    result["format_issues"].append("æ–‡ä»¶ä¸ºç©º")
                    
        except UnicodeDecodeError:
            result["format_issues"].append("æ–‡ä»¶ç¼–ç é”™è¯¯")
        except Exception as e:
            result["format_issues"].append(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}")
            
        return result
        
    def generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        summary = self.results["validation_summary"]
        
        lines = [
            "=" * 80,
            "VisionAI-ClipsMaster è®­ç»ƒæ•°æ®éªŒè¯æŠ¥å‘Š",
            "=" * 80,
            f"éªŒè¯æ—¶é—´: {self.results['timestamp']}",
            "",
            "ğŸ“Š æ•°æ®æ¦‚è§ˆ:",
            f"  è‹±æ–‡è®­ç»ƒæ–‡ä»¶: {summary.get('english', {}).get('total_files', 0)}",
            f"  ä¸­æ–‡è®­ç»ƒæ–‡ä»¶: {summary.get('chinese', {}).get('total_files', 0)}",
            f"  æ€»è®­ç»ƒå¯¹æ•°: {summary.get('total_pairs', 0)}",
            f"  æœ‰æ•ˆè®­ç»ƒå¯¹: {summary.get('valid_pairs', 0)}",
            f"  æ•°æ®æœ‰æ•ˆç‡: {summary.get('validity_rate', 0):.1f}%",
            "",
            "ğŸ¯ è´¨é‡åˆ†æ:",
            f"  é«˜è´¨é‡æ•°æ®å¯¹: {summary.get('quality_distribution', {}).get('high', 0)}",
            f"  ä¸­ç­‰è´¨é‡æ•°æ®å¯¹: {summary.get('quality_distribution', {}).get('medium', 0)}",
            f"  ä½è´¨é‡æ•°æ®å¯¹: {summary.get('quality_distribution', {}).get('low', 0)}",
            f"  å¹³å‡è´¨é‡åˆ†æ•°: {summary.get('average_quality', 0):.1f}/100",
            ""
        ]
        
        # æ·»åŠ å»ºè®®
        if self.results["recommendations"]:
            lines.extend([
                "ğŸ’¡ æ”¹è¿›å»ºè®®:",
                ""
            ])
            for i, rec in enumerate(self.results["recommendations"], 1):
                lines.append(f"  {i}. {rec}")
            lines.append("")
            
        lines.extend([
            "=" * 80,
            "æŠ¥å‘Šç»“æŸ",
            "=" * 80
        ])
        
        return "\n".join(lines)
        
    def run_validation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        self.logger.info("å¼€å§‹å‡†ç¡®çš„è®­ç»ƒæ•°æ®éªŒè¯")
        
        # æ‰«æè‹±æ–‡æ•°æ®
        en_dir = self.project_root / "data/training/en"
        en_results = self.scan_training_directory(en_dir, "en")
        
        # æ‰«æä¸­æ–‡æ•°æ®
        zh_dir = self.project_root / "data/training/zh"
        zh_results = self.scan_training_directory(zh_dir, "zh")
        
        # æ±‡æ€»ç»“æœ
        total_pairs = en_results["total_pairs"] + zh_results["total_pairs"]
        valid_pairs = en_results["valid_pairs"] + zh_results["valid_pairs"]
        
        self.results["validation_summary"] = {
            "english": en_results,
            "chinese": zh_results,
            "total_pairs": total_pairs,
            "valid_pairs": valid_pairs,
            "validity_rate": valid_pairs / total_pairs * 100 if total_pairs > 0 else 0,
            "quality_distribution": {
                "high": en_results["quality_distribution"]["high"] + zh_results["quality_distribution"]["high"],
                "medium": en_results["quality_distribution"]["medium"] + zh_results["quality_distribution"]["medium"],
                "low": en_results["quality_distribution"]["low"] + zh_results["quality_distribution"]["low"]
            }
        }
        
        # è®¡ç®—å¹³å‡è´¨é‡
        all_quality_scores = []
        for lang_results in [en_results, zh_results]:
            for file_detail in lang_results["file_details"]:
                all_quality_scores.extend(file_detail["quality_scores"])
                
        avg_quality = sum(all_quality_scores) / len(all_quality_scores) if all_quality_scores else 0
        self.results["validation_summary"]["average_quality"] = avg_quality
        
        # ç”Ÿæˆå»ºè®®
        self._generate_recommendations()
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"accurate_training_validation_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
            
        self.logger.info(f"éªŒè¯å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_file}")
        return self.results
        
    def _generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        summary = self.results["validation_summary"]
        recommendations = []
        
        # æ•°æ®é‡å»ºè®®
        if summary["total_pairs"] < 10:
            recommendations.append("è®­ç»ƒæ•°æ®æ€»é‡ä¸è¶³ï¼Œå»ºè®®æ”¶é›†æ›´å¤šåŸç‰‡â†’çˆ†æ¬¾çš„è®­ç»ƒå¯¹")
        elif summary["total_pairs"] < 50:
            recommendations.append("è®­ç»ƒæ•°æ®é‡åå°‘ï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ ·æœ¬ä»¥æé«˜æ¨¡å‹æ•ˆæœ")
            
        # è´¨é‡å»ºè®®
        if summary["average_quality"] < 60:
            recommendations.append("å¹³å‡è´¨é‡åä½ï¼Œå»ºè®®ä¼˜åŒ–çˆ†æ¬¾å†…å®¹çš„å¸å¼•åŠ›å’Œè¡¨è¾¾æ–¹å¼")
        elif summary["average_quality"] < 80:
            recommendations.append("è´¨é‡ä¸­ç­‰ï¼Œå»ºè®®å¢åŠ æ›´å¤šé«˜è´¨é‡çš„çˆ†æ¬¾æ ·æœ¬")
            
        # è¯­è¨€å¹³è¡¡å»ºè®®
        en_pairs = summary["english"]["valid_pairs"]
        zh_pairs = summary["chinese"]["valid_pairs"]
        
        if en_pairs == 0:
            recommendations.append("ç¼ºå°‘è‹±æ–‡è®­ç»ƒæ•°æ®ï¼Œå»ºè®®æ·»åŠ è‹±æ–‡åŸç‰‡â†’çˆ†æ¬¾å­—å¹•å¯¹")
        if zh_pairs == 0:
            recommendations.append("ç¼ºå°‘ä¸­æ–‡è®­ç»ƒæ•°æ®ï¼Œå»ºè®®æ·»åŠ ä¸­æ–‡åŸç‰‡â†’çˆ†æ¬¾å­—å¹•å¯¹")
            
        if en_pairs > 0 and zh_pairs > 0:
            ratio = min(en_pairs, zh_pairs) / max(en_pairs, zh_pairs)
            if ratio < 0.3:
                recommendations.append("ä¸­è‹±æ–‡æ•°æ®ä¸å¹³è¡¡ï¼Œå»ºè®®å¢åŠ æ•°é‡è¾ƒå°‘çš„è¯­è¨€çš„è®­ç»ƒæ ·æœ¬")
                
        # è´¨é‡åˆ†å¸ƒå»ºè®®
        quality_dist = summary["quality_distribution"]
        high_ratio = quality_dist["high"] / summary["valid_pairs"] if summary["valid_pairs"] > 0 else 0
        
        if high_ratio < 0.3:
            recommendations.append("é«˜è´¨é‡æ ·æœ¬æ¯”ä¾‹åä½ï¼Œå»ºè®®å¢åŠ æ›´å¤šä¼˜ç§€çš„çˆ†æ¬¾æ¡ˆä¾‹")
            
        self.results["recommendations"] = recommendations


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¯åŠ¨å‡†ç¡®çš„è®­ç»ƒæ•°æ®éªŒè¯")
    print("=" * 50)
    
    validator = AccurateTrainingDataValidator()
    results = validator.run_validation()
    
    # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
    report = validator.generate_comprehensive_report()
    print(report)
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
