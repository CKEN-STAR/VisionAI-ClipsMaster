#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster GPUè¯Šæ–­å·¥å…·
å…¨é¢è¯Šæ–­GPUæ£€æµ‹é—®é¢˜å¹¶æä¾›ä¿®å¤æ–¹æ¡ˆ

è¯Šæ–­å†…å®¹:
1. PyTorch CUDAæ”¯æŒæ£€æµ‹
2. NVIDIA-SMIå¯ç”¨æ€§æ£€æµ‹
3. Windows WMI GPUæ£€æµ‹
4. å†…å­˜ä½¿ç”¨ä¸GPUå…³ç³»åˆ†æ
5. æä¾›é’ˆå¯¹æ€§ä¿®å¤æ–¹æ¡ˆ
"""

import os
import sys
import platform
import subprocess
import psutil
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

class GPUDiagnosticTool:
    """GPUè¯Šæ–­å·¥å…·"""
    
    def __init__(self):
        self.diagnosis_results = {
            "system_info": {},
            "pytorch_cuda": {},
            "nvidia_smi": {},
            "windows_wmi": {},
            "memory_analysis": {},
            "recommendations": []
        }
        
    def collect_system_info(self):
        """æ”¶é›†ç³»ç»ŸåŸºç¡€ä¿¡æ¯"""
        print("ğŸ” æ”¶é›†ç³»ç»ŸåŸºç¡€ä¿¡æ¯...")
        
        try:
            memory = psutil.virtual_memory()
            self.diagnosis_results["system_info"] = {
                "platform": platform.system(),
                "platform_version": platform.version(),
                "architecture": platform.architecture()[0],
                "python_version": platform.python_version(),
                "total_memory_gb": memory.total / 1024**3,
                "available_memory_gb": memory.available / 1024**3,
                "memory_percent": memory.percent,
                "cpu_count": psutil.cpu_count(),
                "cpu_count_logical": psutil.cpu_count(logical=True)
            }
            
            print(f"  æ“ä½œç³»ç»Ÿ: {self.diagnosis_results['system_info']['platform']}")
            print(f"  Pythonç‰ˆæœ¬: {self.diagnosis_results['system_info']['python_version']}")
            print(f"  æ€»å†…å­˜: {self.diagnosis_results['system_info']['total_memory_gb']:.2f}GB")
            print(f"  å¯ç”¨å†…å­˜: {self.diagnosis_results['system_info']['available_memory_gb']:.2f}GB")
            print(f"  CPUæ ¸å¿ƒ: {self.diagnosis_results['system_info']['cpu_count']}")
            
        except Exception as e:
            print(f"  âŒ ç³»ç»Ÿä¿¡æ¯æ”¶é›†å¤±è´¥: {e}")
            
    def diagnose_pytorch_cuda(self):
        """è¯Šæ–­PyTorch CUDAæ”¯æŒ"""
        print("\nğŸ”¥ è¯Šæ–­PyTorch CUDAæ”¯æŒ...")
        
        try:
            import torch
            self.diagnosis_results["pytorch_cuda"]["installed"] = True
            self.diagnosis_results["pytorch_cuda"]["version"] = torch.__version__
            
            print(f"  âœ… PyTorchå·²å®‰è£…: {torch.__version__}")
            
            # æ£€æŸ¥CUDAç¼–è¯‘æ”¯æŒ
            cuda_compiled = torch.version.cuda is not None
            self.diagnosis_results["pytorch_cuda"]["cuda_compiled"] = cuda_compiled
            self.diagnosis_results["pytorch_cuda"]["cuda_version"] = torch.version.cuda
            
            if cuda_compiled:
                print(f"  âœ… CUDAç¼–è¯‘æ”¯æŒ: {torch.version.cuda}")
            else:
                print(f"  âŒ CUDAç¼–è¯‘æ”¯æŒ: æ— ")
                self.diagnosis_results["recommendations"].append(
                    "å®‰è£…æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
                )
            
            # æ£€æŸ¥CUDAè¿è¡Œæ—¶å¯ç”¨æ€§
            cuda_available = torch.cuda.is_available()
            self.diagnosis_results["pytorch_cuda"]["cuda_available"] = cuda_available
            
            if cuda_available:
                device_count = torch.cuda.device_count()
                self.diagnosis_results["pytorch_cuda"]["device_count"] = device_count
                print(f"  âœ… CUDAè¿è¡Œæ—¶å¯ç”¨: {device_count}ä¸ªè®¾å¤‡")
                
                # è·å–GPUè®¾å¤‡ä¿¡æ¯
                devices = []
                for i in range(device_count):
                    device_name = torch.cuda.get_device_name(i)
                    device_props = torch.cuda.get_device_properties(i)
                    devices.append({
                        "id": i,
                        "name": device_name,
                        "memory_total_gb": device_props.total_memory / 1024**3,
                        "multiprocessor_count": device_props.multi_processor_count
                    })
                    print(f"    GPU {i}: {device_name} ({device_props.total_memory / 1024**3:.1f}GB)")
                
                self.diagnosis_results["pytorch_cuda"]["devices"] = devices
            else:
                print(f"  âŒ CUDAè¿è¡Œæ—¶ä¸å¯ç”¨")
                if cuda_compiled:
                    self.diagnosis_results["recommendations"].append(
                        "CUDAè¿è¡Œæ—¶ä¸å¯ç”¨ï¼Œå¯èƒ½åŸå› ï¼š1) æ— NVIDIA GPU 2) é©±åŠ¨ç‰ˆæœ¬ä¸å…¼å®¹ 3) CUDA Toolkitæœªå®‰è£…"
                    )
                
        except ImportError:
            print(f"  âŒ PyTorchæœªå®‰è£…")
            self.diagnosis_results["pytorch_cuda"]["installed"] = False
            self.diagnosis_results["recommendations"].append(
                "å®‰è£…PyTorch: pip install torch torchvision torchaudio"
            )
        except Exception as e:
            print(f"  âŒ PyTorch CUDAæ£€æµ‹å¼‚å¸¸: {e}")
            self.diagnosis_results["pytorch_cuda"]["error"] = str(e)
    
    def diagnose_nvidia_smi(self):
        """è¯Šæ–­NVIDIA-SMIå¯ç”¨æ€§"""
        print("\nğŸ–¥ï¸ è¯Šæ–­NVIDIA-SMIå¯ç”¨æ€§...")
        
        try:
            # æµ‹è¯•nvidia-smiå‘½ä»¤
            result = subprocess.run(
                ["nvidia-smi", "--version"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                print(f"  âœ… nvidia-smiå¯ç”¨")
                self.diagnosis_results["nvidia_smi"]["available"] = True
                self.diagnosis_results["nvidia_smi"]["version_output"] = result.stdout.strip()
                
                # è·å–GPUåˆ—è¡¨
                gpu_result = subprocess.run(
                    ["nvidia-smi", "-L"],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                
                if gpu_result.returncode == 0 and gpu_result.stdout.strip():
                    gpu_lines = gpu_result.stdout.strip().split("\n")
                    self.diagnosis_results["nvidia_smi"]["gpu_list"] = gpu_lines
                    print(f"  âœ… æ£€æµ‹åˆ°{len(gpu_lines)}ä¸ªNVIDIA GPU:")
                    for line in gpu_lines:
                        print(f"    {line}")
                else:
                    print(f"  âš ï¸ nvidia-smiå¯ç”¨ä½†æœªæ£€æµ‹åˆ°GPU")
                    self.diagnosis_results["nvidia_smi"]["gpu_list"] = []
                    
            else:
                print(f"  âŒ nvidia-smiæ‰§è¡Œå¤±è´¥: è¿”å›ç {result.returncode}")
                self.diagnosis_results["nvidia_smi"]["available"] = False
                self.diagnosis_results["nvidia_smi"]["error"] = f"è¿”å›ç {result.returncode}"
                
        except FileNotFoundError:
            print(f"  âŒ nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°")
            self.diagnosis_results["nvidia_smi"]["available"] = False
            self.diagnosis_results["nvidia_smi"]["error"] = "å‘½ä»¤æœªæ‰¾åˆ°"
            self.diagnosis_results["recommendations"].append(
                "nvidia-smiä¸å¯ç”¨ï¼Œå¯èƒ½åŸå› ï¼š1) æ— NVIDIA GPU 2) NVIDIAé©±åŠ¨æœªå®‰è£… 3) é©±åŠ¨ç‰ˆæœ¬è¿‡æ—§"
            )
            
        except subprocess.TimeoutExpired:
            print(f"  âŒ nvidia-smiæ‰§è¡Œè¶…æ—¶")
            self.diagnosis_results["nvidia_smi"]["available"] = False
            self.diagnosis_results["nvidia_smi"]["error"] = "æ‰§è¡Œè¶…æ—¶"
            
        except Exception as e:
            print(f"  âŒ nvidia-smiæ£€æµ‹å¼‚å¸¸: {e}")
            self.diagnosis_results["nvidia_smi"]["available"] = False
            self.diagnosis_results["nvidia_smi"]["error"] = str(e)
    
    def diagnose_windows_wmi(self):
        """è¯Šæ–­Windows WMI GPUæ£€æµ‹"""
        if platform.system() != "Windows":
            print("\nâ­ï¸ è·³è¿‡Windows WMIæ£€æµ‹ï¼ˆéWindowsç³»ç»Ÿï¼‰")
            return
            
        print("\nğŸªŸ è¯Šæ–­Windows WMI GPUæ£€æµ‹...")
        
        # æ–¹æ³•1: ä½¿ç”¨wmicå‘½ä»¤
        try:
            result = subprocess.run(
                ["wmic", "path", "win32_VideoController", "get", "name,adapterram,driverversion", "/format:csv"],
                capture_output=True,
                text=True,
                timeout=15
            )
            
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                gpus = []
                
                for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                    if line.strip() and ',' in line:
                        parts = line.split(',')
                        if len(parts) >= 4:
                            name = parts[3].strip()
                            if name and name != "Name":
                                gpu_info = {
                                    "name": name,
                                    "memory": parts[1].strip() if len(parts) > 1 else "N/A",
                                    "driver": parts[2].strip() if len(parts) > 2 else "N/A"
                                }
                                gpus.append(gpu_info)
                
                self.diagnosis_results["windows_wmi"]["wmic_available"] = True
                self.diagnosis_results["windows_wmi"]["detected_gpus"] = gpus
                
                print(f"  âœ… WMICæ£€æµ‹åˆ°{len(gpus)}ä¸ªæ˜¾å¡:")
                for gpu in gpus:
                    print(f"    {gpu['name']}")
                    
            else:
                print(f"  âŒ WMICæ‰§è¡Œå¤±è´¥")
                self.diagnosis_results["windows_wmi"]["wmic_available"] = False
                
        except Exception as e:
            print(f"  âŒ WMICæ£€æµ‹å¼‚å¸¸: {e}")
            self.diagnosis_results["windows_wmi"]["wmic_error"] = str(e)
        
        # æ–¹æ³•2: å°è¯•WMIæ¨¡å—
        try:
            import wmi
            c = wmi.WMI()
            wmi_gpus = []
            
            for gpu in c.Win32_VideoController():
                if gpu.Name:
                    wmi_gpus.append({
                        "name": gpu.Name,
                        "driver_version": gpu.DriverVersion,
                        "memory": gpu.AdapterRAM
                    })
            
            self.diagnosis_results["windows_wmi"]["wmi_module_available"] = True
            self.diagnosis_results["windows_wmi"]["wmi_detected_gpus"] = wmi_gpus
            
            print(f"  âœ… WMIæ¨¡å—æ£€æµ‹åˆ°{len(wmi_gpus)}ä¸ªæ˜¾å¡:")
            for gpu in wmi_gpus:
                print(f"    {gpu['name']}")
                
        except ImportError:
            print(f"  âš ï¸ WMIæ¨¡å—ä¸å¯ç”¨")
            self.diagnosis_results["windows_wmi"]["wmi_module_available"] = False
            self.diagnosis_results["recommendations"].append(
                "å®‰è£…WMIæ¨¡å—ä»¥å¢å¼ºGPUæ£€æµ‹: pip install WMI"
            )
        except Exception as e:
            print(f"  âŒ WMIæ¨¡å—æ£€æµ‹å¼‚å¸¸: {e}")
            self.diagnosis_results["windows_wmi"]["wmi_error"] = str(e)
    
    def analyze_memory_usage(self):
        """åˆ†æå†…å­˜ä½¿ç”¨ä¸GPUçš„å…³ç³»"""
        print("\nğŸ’¾ åˆ†æå†…å­˜ä½¿ç”¨ä¸GPUå…³ç³»...")
        
        memory = psutil.virtual_memory()
        
        self.diagnosis_results["memory_analysis"] = {
            "total_gb": memory.total / 1024**3,
            "used_gb": memory.used / 1024**3,
            "available_gb": memory.available / 1024**3,
            "percent": memory.percent,
            "high_usage": memory.percent > 80,
            "target_limit_gb": 3.8
        }
        
        print(f"  æ€»å†…å­˜: {memory.total / 1024**3:.2f}GB")
        print(f"  å·²ç”¨å†…å­˜: {memory.used / 1024**3:.2f}GB ({memory.percent:.1f}%)")
        print(f"  å¯ç”¨å†…å­˜: {memory.available / 1024**3:.2f}GB")
        print(f"  ç›®æ ‡é™åˆ¶: 3.8GB")
        
        # åˆ†æå†…å­˜ä½¿ç”¨è¿‡é«˜çš„åŸå› 
        if memory.percent > 80:
            print(f"  âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ ({memory.percent:.1f}%)")
            
            # æ£€æŸ¥æ˜¯å¦å› ä¸ºæ— GPUè€Œå¯¼è‡´CPUå†…å­˜å ç”¨è¿‡å¤§
            pytorch_cuda = self.diagnosis_results.get("pytorch_cuda", {})
            if not pytorch_cuda.get("cuda_available", False):
                print(f"  ğŸ’¡ å¯èƒ½åŸå› : æ— GPUåŠ é€Ÿï¼ŒCPUå¤„ç†å ç”¨å¤§é‡å†…å­˜")
                self.diagnosis_results["recommendations"].append(
                    "å†…å­˜ä½¿ç”¨è¿‡é«˜å¯èƒ½ä¸æ— GPUåŠ é€Ÿæœ‰å…³ï¼Œå»ºè®®ï¼š1) å®‰è£…GPUé©±åŠ¨ 2) ä¼˜åŒ–CPUæ¨¡å¼å†…å­˜ä½¿ç”¨"
                )
            
            if memory.used / 1024**3 > 3.8:
                print(f"  ğŸš¨ è¶…å‡º4GBè®¾å¤‡ç›®æ ‡é™åˆ¶")
                self.diagnosis_results["recommendations"].append(
                    "å½“å‰å†…å­˜ä½¿ç”¨è¶…å‡º4GBè®¾å¤‡é™åˆ¶ï¼Œéœ€è¦ä¼˜åŒ–å†…å­˜ç®¡ç†ç­–ç•¥"
                )
        else:
            print(f"  âœ… å†…å­˜ä½¿ç”¨æ­£å¸¸")
    
    def generate_recommendations(self):
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        print("\nğŸ’¡ ç”Ÿæˆä¿®å¤å»ºè®®...")
        
        pytorch_cuda = self.diagnosis_results.get("pytorch_cuda", {})
        nvidia_smi = self.diagnosis_results.get("nvidia_smi", {})
        memory_analysis = self.diagnosis_results.get("memory_analysis", {})
        
        # GPUç›¸å…³å»ºè®®
        if not pytorch_cuda.get("cuda_available", False):
            if not pytorch_cuda.get("cuda_compiled", False):
                self.diagnosis_results["recommendations"].append(
                    "ğŸ”§ å®‰è£…æ”¯æŒCUDAçš„PyTorch: pip uninstall torch && pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
                )
            
            if not nvidia_smi.get("available", False):
                self.diagnosis_results["recommendations"].append(
                    "ğŸ”§ å®‰è£…NVIDIAé©±åŠ¨: ä»NVIDIAå®˜ç½‘ä¸‹è½½æœ€æ–°é©±åŠ¨ç¨‹åº"
                )
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        if memory_analysis.get("high_usage", False):
            self.diagnosis_results["recommendations"].append(
                "ğŸ”§ ä¼˜åŒ–å†…å­˜ä½¿ç”¨: å®æ–½æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†ç­–ç•¥"
            )
            
        # 4GBè®¾å¤‡ä¼˜åŒ–å»ºè®®
        if memory_analysis.get("total_gb", 0) <= 6:
            self.diagnosis_results["recommendations"].append(
                "ğŸ”§ 4GBè®¾å¤‡ä¼˜åŒ–: å¯ç”¨é‡åŒ–æ¨¡å‹å’Œåˆ†ç‰‡åŠ è½½"
            )
        
        # æ— GPUç¯å¢ƒä¼˜åŒ–
        if not pytorch_cuda.get("cuda_available", False):
            self.diagnosis_results["recommendations"].append(
                "ğŸ”§ CPUæ¨¡å¼ä¼˜åŒ–: ä½¿ç”¨OpenVINOæˆ–ONNX RuntimeåŠ é€ŸCPUæ¨ç†"
            )
    
    def run_full_diagnosis(self):
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print("ğŸ” VisionAI-ClipsMaster GPUè¯Šæ–­å·¥å…·")
        print("=" * 50)
        
        # æ‰§è¡Œå„é¡¹è¯Šæ–­
        self.collect_system_info()
        self.diagnose_pytorch_cuda()
        self.diagnose_nvidia_smi()
        self.diagnose_windows_wmi()
        self.analyze_memory_usage()
        self.generate_recommendations()
        
        # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        print("\n" + "=" * 50)
        print("ğŸ“Š è¯Šæ–­ç»“æœæ€»ç»“:")
        
        # GPUå¯ç”¨æ€§æ€»ç»“
        pytorch_cuda = self.diagnosis_results.get("pytorch_cuda", {})
        nvidia_smi = self.diagnosis_results.get("nvidia_smi", {})
        
        gpu_available = pytorch_cuda.get("cuda_available", False)
        print(f"  GPUåŠ é€Ÿå¯ç”¨: {'âœ… æ˜¯' if gpu_available else 'âŒ å¦'}")
        
        if gpu_available:
            device_count = pytorch_cuda.get("device_count", 0)
            print(f"  GPUè®¾å¤‡æ•°é‡: {device_count}")
        else:
            print(f"  è¿è¡Œæ¨¡å¼: CPUæ¨¡å¼")
        
        # å†…å­˜çŠ¶æ€æ€»ç»“
        memory_analysis = self.diagnosis_results.get("memory_analysis", {})
        memory_status = "æ­£å¸¸" if not memory_analysis.get("high_usage", False) else "è¿‡é«˜"
        print(f"  å†…å­˜çŠ¶æ€: {memory_status} ({memory_analysis.get('percent', 0):.1f}%)")
        
        # ä¿®å¤å»ºè®®
        recommendations = self.diagnosis_results.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ”§ ä¿®å¤å»ºè®® ({len(recommendations)}é¡¹):")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        else:
            print(f"\nâœ… ç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€ä¿®å¤")
        
        return self.diagnosis_results

if __name__ == "__main__":
    # è¿è¡ŒGPUè¯Šæ–­
    diagnostic = GPUDiagnosticTool()
    results = diagnostic.run_full_diagnosis()
    
    # ä¿å­˜è¯Šæ–­æŠ¥å‘Š
    import json
    with open("gpu_diagnostic_report.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ°: gpu_diagnostic_report.json")
