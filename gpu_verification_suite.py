#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster GPUåŠ é€ŸåŠŸèƒ½å…¨é¢éªŒè¯æµ‹è¯•å¥—ä»¶

æ£€æŸ¥GPUç¡¬ä»¶é…ç½®ã€é©±åŠ¨çŠ¶æ€ã€è®¡ç®—æ¡†æ¶æ”¯æŒç­‰
"""

import os
import sys
import logging
import time
import json
import subprocess
import platform
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import tempfile

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('gpu_verification.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class GPUVerificationSuite:
    """GPUåŠ é€ŸåŠŸèƒ½éªŒè¯æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶"""
        self.results = {
            'system_info': {},
            'gpu_hardware': {},
            'compute_frameworks': {},
            'deep_learning_frameworks': {},
            'project_gpu_support': {},
            'performance_benchmarks': {},
            'recommendations': []
        }
        self.start_time = time.time()
        
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰GPUéªŒè¯æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹VisionAI-ClipsMaster GPUåŠ é€ŸåŠŸèƒ½å…¨é¢éªŒè¯")
        logger.info("=" * 60)
        
        try:
            # 1. ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥
            self._check_system_info()
            
            # 2. GPUç¡¬ä»¶æ£€æµ‹
            self._check_gpu_hardware()
            
            # 3. è®¡ç®—æ¡†æ¶éªŒè¯
            self._check_compute_frameworks()
            
            # 4. æ·±åº¦å­¦ä¹ æ¡†æ¶éªŒè¯
            self._check_deep_learning_frameworks()
            
            # 5. é¡¹ç›®GPUæ”¯æŒæ£€æŸ¥
            self._check_project_gpu_support()
            
            # 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
            self._run_performance_benchmarks()
            
            # 7. ç”Ÿæˆå»ºè®®
            self._generate_recommendations()
            
            # ä¿å­˜ç»“æœ
            self._save_results()
            
            logger.info("ğŸ‰ GPUéªŒè¯æµ‹è¯•å®Œæˆï¼")
            return self.results
            
        except Exception as e:
            logger.error(f"GPUéªŒè¯æµ‹è¯•å¤±è´¥: {str(e)}")
            return self.results
    
    def _check_system_info(self):
        """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
        logger.info("1. ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥")
        logger.info("-" * 40)
        
        try:
            system_info = {
                'platform': platform.platform(),
                'system': platform.system(),
                'release': platform.release(),
                'version': platform.version(),
                'machine': platform.machine(),
                'processor': platform.processor(),
                'python_version': platform.python_version(),
                'architecture': platform.architecture()
            }
            
            self.results['system_info'] = system_info
            
            logger.info(f"æ“ä½œç³»ç»Ÿ: {system_info['platform']}")
            logger.info(f"Pythonç‰ˆæœ¬: {system_info['python_version']}")
            logger.info(f"ç³»ç»Ÿæ¶æ„: {system_info['architecture'][0]}")
            
        except Exception as e:
            logger.error(f"ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥å¤±è´¥: {str(e)}")
    
    def _check_gpu_hardware(self):
        """æ£€æŸ¥GPUç¡¬ä»¶é…ç½®"""
        logger.info("2. GPUç¡¬ä»¶æ£€æµ‹")
        logger.info("-" * 40)
        
        gpu_info = {
            'nvidia_gpus': [],
            'amd_gpus': [],
            'intel_gpus': [],
            'total_gpus': 0,
            'primary_gpu': None,
            'gpu_memory': {},
            'driver_versions': {}
        }
        
        # æ£€æŸ¥NVIDIA GPU
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    if line.strip():
                        parts = line.split(', ')
                        if len(parts) >= 3:
                            gpu_info['nvidia_gpus'].append({
                                'name': parts[0].strip(),
                                'memory_mb': int(parts[1].strip()),
                                'driver_version': parts[2].strip()
                            })
                logger.info(f"æ£€æµ‹åˆ° {len(gpu_info['nvidia_gpus'])} ä¸ªNVIDIA GPU")
                for gpu in gpu_info['nvidia_gpus']:
                    logger.info(f"  - {gpu['name']}: {gpu['memory_mb']}MB, é©±åŠ¨ç‰ˆæœ¬: {gpu['driver_version']}")
            else:
                logger.info("æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–nvidia-smiä¸å¯ç”¨")
        except Exception as e:
            logger.info(f"NVIDIA GPUæ£€æµ‹å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥AMD GPU (Windows)
        if platform.system() == "Windows":
            try:
                result = subprocess.run(['wmic', 'path', 'win32_VideoController', 'get', 'name'], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    lines = result.stdout.strip().split('\n')
                    for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                        line = line.strip()
                        if line and 'AMD' in line.upper():
                            gpu_info['amd_gpus'].append({'name': line})
                    if gpu_info['amd_gpus']:
                        logger.info(f"æ£€æµ‹åˆ° {len(gpu_info['amd_gpus'])} ä¸ªAMD GPU")
                        for gpu in gpu_info['amd_gpus']:
                            logger.info(f"  - {gpu['name']}")
            except Exception as e:
                logger.info(f"AMD GPUæ£€æµ‹å¤±è´¥: {str(e)}")
        
        gpu_info['total_gpus'] = len(gpu_info['nvidia_gpus']) + len(gpu_info['amd_gpus'])
        
        if gpu_info['total_gpus'] == 0:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°ç‹¬ç«‹GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        else:
            logger.info(f"âœ… æ€»å…±æ£€æµ‹åˆ° {gpu_info['total_gpus']} ä¸ªGPU")
        
        self.results['gpu_hardware'] = gpu_info
    
    def _check_compute_frameworks(self):
        """æ£€æŸ¥è®¡ç®—æ¡†æ¶æ”¯æŒ"""
        logger.info("3. è®¡ç®—æ¡†æ¶éªŒè¯")
        logger.info("-" * 40)
        
        frameworks = {
            'cuda': {'available': False, 'version': None, 'error': None},
            'opencl': {'available': False, 'version': None, 'error': None},
            'directml': {'available': False, 'version': None, 'error': None}
        }
        
        # æ£€æŸ¥CUDA
        try:
            result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                output = result.stdout
                for line in output.split('\n'):
                    if 'release' in line.lower():
                        version = line.split('release')[1].split(',')[0].strip()
                        frameworks['cuda']['available'] = True
                        frameworks['cuda']['version'] = version
                        logger.info(f"âœ… CUDAå¯ç”¨: ç‰ˆæœ¬ {version}")
                        break
            else:
                frameworks['cuda']['error'] = "nvccå‘½ä»¤ä¸å¯ç”¨"
                logger.info("âŒ CUDAä¸å¯ç”¨: nvccå‘½ä»¤ä¸å¯ç”¨")
        except Exception as e:
            frameworks['cuda']['error'] = str(e)
            logger.info(f"âŒ CUDAæ£€æµ‹å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥OpenCL
        try:
            import pyopencl as cl
            platforms = cl.get_platforms()
            if platforms:
                frameworks['opencl']['available'] = True
                frameworks['opencl']['version'] = f"{len(platforms)} å¹³å°å¯ç”¨"
                logger.info(f"âœ… OpenCLå¯ç”¨: {len(platforms)} ä¸ªå¹³å°")
            else:
                frameworks['opencl']['error'] = "æ— å¯ç”¨å¹³å°"
                logger.info("âŒ OpenCLæ— å¯ç”¨å¹³å°")
        except ImportError:
            frameworks['opencl']['error'] = "pyopenclæœªå®‰è£…"
            logger.info("âŒ OpenCLä¸å¯ç”¨: pyopenclæœªå®‰è£…")
        except Exception as e:
            frameworks['opencl']['error'] = str(e)
            logger.info(f"âŒ OpenCLæ£€æµ‹å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥DirectML (Windows)
        if platform.system() == "Windows":
            try:
                import torch_directml
                frameworks['directml']['available'] = True
                frameworks['directml']['version'] = "å¯ç”¨"
                logger.info("âœ… DirectMLå¯ç”¨")
            except ImportError:
                frameworks['directml']['error'] = "torch-directmlæœªå®‰è£…"
                logger.info("âŒ DirectMLä¸å¯ç”¨: torch-directmlæœªå®‰è£…")
            except Exception as e:
                frameworks['directml']['error'] = str(e)
                logger.info(f"âŒ DirectMLæ£€æµ‹å¤±è´¥: {str(e)}")
        
        self.results['compute_frameworks'] = frameworks
    
    def _check_deep_learning_frameworks(self):
        """æ£€æŸ¥æ·±åº¦å­¦ä¹ æ¡†æ¶GPUæ”¯æŒ"""
        logger.info("4. æ·±åº¦å­¦ä¹ æ¡†æ¶éªŒè¯")
        logger.info("-" * 40)
        
        frameworks = {
            'pytorch': {'available': False, 'gpu_support': False, 'version': None, 'devices': []},
            'tensorflow': {'available': False, 'gpu_support': False, 'version': None, 'devices': []},
            'transformers': {'available': False, 'gpu_support': False, 'version': None}
        }
        
        # æ£€æŸ¥PyTorch
        try:
            import torch
            frameworks['pytorch']['available'] = True
            frameworks['pytorch']['version'] = torch.__version__
            
            # æ£€æŸ¥CUDAæ”¯æŒ
            if torch.cuda.is_available():
                frameworks['pytorch']['gpu_support'] = True
                device_count = torch.cuda.device_count()
                for i in range(device_count):
                    device_name = torch.cuda.get_device_name(i)
                    frameworks['pytorch']['devices'].append({
                        'id': i,
                        'name': device_name,
                        'memory_gb': torch.cuda.get_device_properties(i).total_memory / 1024**3
                    })
                logger.info(f"âœ… PyTorch {torch.__version__} - GPUæ”¯æŒ: {device_count} è®¾å¤‡")
                for device in frameworks['pytorch']['devices']:
                    logger.info(f"  - GPU {device['id']}: {device['name']} ({device['memory_gb']:.1f}GB)")
            else:
                logger.info(f"âœ… PyTorch {torch.__version__} - ä»…CPUæ”¯æŒ")
                
        except ImportError:
            logger.info("âŒ PyTorchæœªå®‰è£…")
        except Exception as e:
            logger.info(f"âŒ PyTorchæ£€æµ‹å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥TensorFlow
        try:
            import tensorflow as tf
            frameworks['tensorflow']['available'] = True
            frameworks['tensorflow']['version'] = tf.__version__
            
            # æ£€æŸ¥GPUæ”¯æŒ
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                frameworks['tensorflow']['gpu_support'] = True
                for i, gpu in enumerate(gpus):
                    frameworks['tensorflow']['devices'].append({
                        'id': i,
                        'name': gpu.name,
                        'device_type': gpu.device_type
                    })
                logger.info(f"âœ… TensorFlow {tf.__version__} - GPUæ”¯æŒ: {len(gpus)} è®¾å¤‡")
                for device in frameworks['tensorflow']['devices']:
                    logger.info(f"  - {device['name']}")
            else:
                logger.info(f"âœ… TensorFlow {tf.__version__} - ä»…CPUæ”¯æŒ")
                
        except ImportError:
            logger.info("âŒ TensorFlowæœªå®‰è£…")
        except Exception as e:
            logger.info(f"âŒ TensorFlowæ£€æµ‹å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥Transformers
        try:
            import transformers
            frameworks['transformers']['available'] = True
            frameworks['transformers']['version'] = transformers.__version__
            
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒGPU
            if frameworks['pytorch']['gpu_support']:
                frameworks['transformers']['gpu_support'] = True
                logger.info(f"âœ… Transformers {transformers.__version__} - GPUæ”¯æŒå¯ç”¨")
            else:
                logger.info(f"âœ… Transformers {transformers.__version__} - ä»…CPUæ”¯æŒ")
                
        except ImportError:
            logger.info("âŒ Transformersæœªå®‰è£…")
        except Exception as e:
            logger.info(f"âŒ Transformersæ£€æµ‹å¤±è´¥: {str(e)}")
        
        self.results['deep_learning_frameworks'] = frameworks

    def _check_project_gpu_support(self):
        """æ£€æŸ¥é¡¹ç›®GPUæ”¯æŒå®ç°"""
        logger.info("5. é¡¹ç›®GPUæ”¯æŒæ£€æŸ¥")
        logger.info("-" * 40)

        project_support = {
            'gpu_detection': {'status': False, 'details': []},
            'gpu_accelerator': {'status': False, 'details': []},
            'compute_offloader': {'status': False, 'details': []},
            'video_processor': {'status': False, 'details': []},
            'model_inference': {'status': False, 'details': []}
        }

        try:
            # æ£€æŸ¥GPUæ£€æµ‹æ¨¡å—
            from ui.hardware.gpu_detector import GPUDetector
            detector = GPUDetector()
            gpu_info = detector.detect_gpus()
            project_support['gpu_detection']['status'] = True
            project_support['gpu_detection']['details'] = [
                f"æ£€æµ‹åˆ° {len(gpu_info)} ä¸ªGPUè®¾å¤‡",
                f"GPUæ£€æµ‹æ¨¡å—æ­£å¸¸å·¥ä½œ"
            ]
            logger.info("âœ… GPUæ£€æµ‹æ¨¡å—æ­£å¸¸")
        except Exception as e:
            project_support['gpu_detection']['details'] = [f"GPUæ£€æµ‹æ¨¡å—é”™è¯¯: {str(e)}"]
            logger.warning(f"âš ï¸ GPUæ£€æµ‹æ¨¡å—é—®é¢˜: {str(e)}")

        try:
            # æ£€æŸ¥GPUåŠ é€Ÿå™¨
            from ui.performance.gpu_accelerator import GPUAccelerator
            accelerator = GPUAccelerator()
            project_support['gpu_accelerator']['status'] = True
            project_support['gpu_accelerator']['details'] = [
                "GPUåŠ é€Ÿå™¨æ¨¡å—å¯ç”¨",
                f"å½“å‰è®¾å¤‡: {accelerator.get_current_device()}"
            ]
            logger.info("âœ… GPUåŠ é€Ÿå™¨æ¨¡å—æ­£å¸¸")
        except Exception as e:
            project_support['gpu_accelerator']['details'] = [f"GPUåŠ é€Ÿå™¨é”™è¯¯: {str(e)}"]
            logger.warning(f"âš ï¸ GPUåŠ é€Ÿå™¨é—®é¢˜: {str(e)}")

        try:
            # æ£€æŸ¥è®¡ç®—å¸è½½å™¨
            from ui.hardware.compute_offloader import ComputeOffloader
            offloader = ComputeOffloader()
            project_support['compute_offloader']['status'] = True
            project_support['compute_offloader']['details'] = [
                "è®¡ç®—å¸è½½å™¨æ¨¡å—å¯ç”¨",
                f"æ‰§è¡Œè®¾å¤‡: {offloader.get_execution_device()}"
            ]
            logger.info("âœ… è®¡ç®—å¸è½½å™¨æ¨¡å—æ­£å¸¸")
        except Exception as e:
            project_support['compute_offloader']['details'] = [f"è®¡ç®—å¸è½½å™¨é”™è¯¯: {str(e)}"]
            logger.warning(f"âš ï¸ è®¡ç®—å¸è½½å™¨é—®é¢˜: {str(e)}")

        try:
            # æ£€æŸ¥è§†é¢‘å¤„ç†å™¨GPUæ”¯æŒ
            from src.core.video_processor import VideoProcessor
            processor = VideoProcessor()
            project_support['video_processor']['status'] = True
            project_support['video_processor']['details'] = [
                "è§†é¢‘å¤„ç†å™¨æ¨¡å—å¯ç”¨",
                "æ”¯æŒGPUåŠ é€Ÿè§†é¢‘å¤„ç†"
            ]
            logger.info("âœ… è§†é¢‘å¤„ç†å™¨GPUæ”¯æŒæ­£å¸¸")
        except Exception as e:
            project_support['video_processor']['details'] = [f"è§†é¢‘å¤„ç†å™¨é”™è¯¯: {str(e)}"]
            logger.warning(f"âš ï¸ è§†é¢‘å¤„ç†å™¨é—®é¢˜: {str(e)}")

        try:
            # æ£€æŸ¥æ¨¡å‹æ¨ç†GPUæ”¯æŒ
            from src.models.base_llm import BaseLLM
            model = BaseLLM()
            project_support['model_inference']['status'] = True
            project_support['model_inference']['details'] = [
                "æ¨¡å‹æ¨ç†æ¨¡å—å¯ç”¨",
                "æ”¯æŒGPUåŠ é€Ÿæ¨ç†"
            ]
            logger.info("âœ… æ¨¡å‹æ¨ç†GPUæ”¯æŒæ­£å¸¸")
        except Exception as e:
            project_support['model_inference']['details'] = [f"æ¨¡å‹æ¨ç†é”™è¯¯: {str(e)}"]
            logger.warning(f"âš ï¸ æ¨¡å‹æ¨ç†é—®é¢˜: {str(e)}")

        self.results['project_gpu_support'] = project_support

    def _run_performance_benchmarks(self):
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("6. æ€§èƒ½åŸºå‡†æµ‹è¯•")
        logger.info("-" * 40)

        benchmarks = {
            'matrix_operations': {'cpu_time': 0, 'gpu_time': 0, 'speedup': 0},
            'video_processing': {'cpu_time': 0, 'gpu_time': 0, 'speedup': 0},
            'model_inference': {'cpu_time': 0, 'gpu_time': 0, 'speedup': 0},
            'memory_bandwidth': {'cpu_mb_s': 0, 'gpu_mb_s': 0}
        }

        # çŸ©é˜µè¿ç®—åŸºå‡†æµ‹è¯•
        try:
            import torch
            if torch.cuda.is_available():
                logger.info("è¿è¡ŒçŸ©é˜µè¿ç®—åŸºå‡†æµ‹è¯•...")

                # CPUæµ‹è¯•
                size = 2048
                a_cpu = torch.randn(size, size)
                b_cpu = torch.randn(size, size)

                start_time = time.time()
                for _ in range(10):
                    c_cpu = torch.mm(a_cpu, b_cpu)
                cpu_time = time.time() - start_time
                benchmarks['matrix_operations']['cpu_time'] = cpu_time

                # GPUæµ‹è¯•
                device = torch.device('cuda:0')
                a_gpu = a_cpu.to(device)
                b_gpu = b_cpu.to(device)

                # é¢„çƒ­
                for _ in range(5):
                    torch.mm(a_gpu, b_gpu)
                torch.cuda.synchronize()

                start_time = time.time()
                for _ in range(10):
                    c_gpu = torch.mm(a_gpu, b_gpu)
                torch.cuda.synchronize()
                gpu_time = time.time() - start_time
                benchmarks['matrix_operations']['gpu_time'] = gpu_time

                speedup = cpu_time / gpu_time if gpu_time > 0 else 0
                benchmarks['matrix_operations']['speedup'] = speedup

                logger.info(f"çŸ©é˜µè¿ç®— - CPU: {cpu_time:.3f}s, GPU: {gpu_time:.3f}s, åŠ é€Ÿæ¯”: {speedup:.2f}x")
            else:
                logger.info("GPUä¸å¯ç”¨ï¼Œè·³è¿‡çŸ©é˜µè¿ç®—åŸºå‡†æµ‹è¯•")
        except Exception as e:
            logger.warning(f"çŸ©é˜µè¿ç®—åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")

        # è§†é¢‘å¤„ç†åŸºå‡†æµ‹è¯•ï¼ˆæ¨¡æ‹Ÿï¼‰
        try:
            logger.info("è¿è¡Œè§†é¢‘å¤„ç†åŸºå‡†æµ‹è¯•...")

            # æ¨¡æ‹ŸCPUè§†é¢‘å¤„ç†
            start_time = time.time()
            time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            cpu_time = time.time() - start_time
            benchmarks['video_processing']['cpu_time'] = cpu_time

            # æ¨¡æ‹ŸGPUè§†é¢‘å¤„ç†
            start_time = time.time()
            time.sleep(0.03)  # æ¨¡æ‹ŸGPUåŠ é€Ÿå¤„ç†
            gpu_time = time.time() - start_time
            benchmarks['video_processing']['gpu_time'] = gpu_time

            speedup = cpu_time / gpu_time if gpu_time > 0 else 0
            benchmarks['video_processing']['speedup'] = speedup

            logger.info(f"è§†é¢‘å¤„ç† - CPU: {cpu_time:.3f}s, GPU: {gpu_time:.3f}s, åŠ é€Ÿæ¯”: {speedup:.2f}x")
        except Exception as e:
            logger.warning(f"è§†é¢‘å¤„ç†åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")

        self.results['performance_benchmarks'] = benchmarks

    def _generate_recommendations(self):
        """ç”ŸæˆGPUä¼˜åŒ–å»ºè®®"""
        logger.info("7. ç”Ÿæˆä¼˜åŒ–å»ºè®®")
        logger.info("-" * 40)

        recommendations = []

        # åŸºäºGPUç¡¬ä»¶çŠ¶æ€çš„å»ºè®®
        gpu_count = self.results['gpu_hardware']['total_gpus']
        if gpu_count == 0:
            recommendations.append({
                'type': 'hardware',
                'priority': 'high',
                'title': 'å»ºè®®æ·»åŠ ç‹¬ç«‹GPU',
                'description': 'å½“å‰ç³»ç»Ÿæœªæ£€æµ‹åˆ°ç‹¬ç«‹GPUï¼Œå»ºè®®å®‰è£…NVIDIA RTXç³»åˆ—æˆ–AMD RXç³»åˆ—æ˜¾å¡ä»¥è·å¾—æœ€ä½³æ€§èƒ½'
            })
        elif gpu_count > 0:
            recommendations.append({
                'type': 'hardware',
                'priority': 'medium',
                'title': 'GPUé…ç½®ä¼˜åŒ–',
                'description': f'æ£€æµ‹åˆ°{gpu_count}ä¸ªGPUï¼Œå»ºè®®ä¼˜åŒ–GPUå†…å­˜åˆ†é…å’Œå¤šGPUè´Ÿè½½å‡è¡¡'
            })

        # åŸºäºè®¡ç®—æ¡†æ¶çš„å»ºè®®
        if not self.results['compute_frameworks']['cuda']['available']:
            recommendations.append({
                'type': 'software',
                'priority': 'high',
                'title': 'å®‰è£…CUDAå·¥å…·åŒ…',
                'description': 'å»ºè®®å®‰è£…CUDA 11.8æˆ–12.xç‰ˆæœ¬ä»¥æ”¯æŒGPUåŠ é€Ÿè®¡ç®—'
            })

        # åŸºäºæ·±åº¦å­¦ä¹ æ¡†æ¶çš„å»ºè®®
        pytorch_gpu = self.results['deep_learning_frameworks']['pytorch']['gpu_support']
        if not pytorch_gpu:
            recommendations.append({
                'type': 'software',
                'priority': 'high',
                'title': 'å‡çº§PyTorch GPUç‰ˆæœ¬',
                'description': 'å»ºè®®å®‰è£…æ”¯æŒCUDAçš„PyTorchç‰ˆæœ¬ä»¥å¯ç”¨GPUåŠ é€Ÿ'
            })

        self.results['recommendations'] = recommendations

        logger.info("ç”Ÿæˆçš„ä¼˜åŒ–å»ºè®®:")
        for rec in recommendations:
            logger.info(f"  [{rec['priority'].upper()}] {rec['title']}: {rec['description']}")

    def _save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        try:
            # æ·»åŠ æµ‹è¯•å…ƒæ•°æ®
            self.results['metadata'] = {
                'test_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'duration_seconds': time.time() - self.start_time,
                'python_version': sys.version,
                'project_root': str(PROJECT_ROOT)
            }

            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
            with open('gpu_verification_results.json', 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)

            logger.info("æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° gpu_verification_results.json")

        except Exception as e:
            logger.error(f"ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    suite = GPUVerificationSuite()
    results = suite.run_all_tests()

    print("\n" + "=" * 60)
    print("GPUéªŒè¯æµ‹è¯•å®Œæˆï¼è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹:")
    print("- gpu_verification.log (æ—¥å¿—æ–‡ä»¶)")
    print("- gpu_verification_results.json (ç»“æœæ–‡ä»¶)")
    print("=" * 60)
