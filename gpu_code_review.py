#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster GPUåŠ é€ŸåŠŸèƒ½ä»£ç å®¡æŸ¥å·¥å…·

æ£€æŸ¥é¡¹ç›®ä¸­GPUåŠ é€Ÿå®ç°çš„æ­£ç¡®æ€§å’Œå®Œæ•´æ€§
"""

import os
import sys
import logging
import ast
import re
from typing import Dict, List, Any, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('gpu_code_review.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class GPUCodeReviewer:
    """GPUåŠ é€ŸåŠŸèƒ½ä»£ç å®¡æŸ¥å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä»£ç å®¡æŸ¥å™¨"""
        self.results = {
            'gpu_modules': {},
            'video_processing': {},
            'model_inference': {},
            'memory_management': {},
            'device_switching': {},
            'fallback_mechanisms': {},
            'performance_optimizations': {},
            'issues_found': [],
            'recommendations': []
        }
        
    def run_code_review(self) -> Dict[str, Any]:
        """è¿è¡ŒGPUä»£ç å®¡æŸ¥"""
        logger.info("ğŸ” å¼€å§‹VisionAI-ClipsMaster GPUåŠ é€ŸåŠŸèƒ½ä»£ç å®¡æŸ¥")
        logger.info("=" * 60)
        
        try:
            # 1. æ£€æŸ¥GPUç›¸å…³æ¨¡å—
            self._review_gpu_modules()
            
            # 2. æ£€æŸ¥è§†é¢‘å¤„ç†GPUåŠ é€Ÿ
            self._review_video_processing()
            
            # 3. æ£€æŸ¥æ¨¡å‹æ¨ç†GPUæ”¯æŒ
            self._review_model_inference()
            
            # 4. æ£€æŸ¥GPUå†…å­˜ç®¡ç†
            self._review_memory_management()
            
            # 5. æ£€æŸ¥è®¾å¤‡åˆ‡æ¢é€»è¾‘
            self._review_device_switching()
            
            # 6. æ£€æŸ¥é™çº§æœºåˆ¶
            self._review_fallback_mechanisms()
            
            # 7. æ£€æŸ¥æ€§èƒ½ä¼˜åŒ–
            self._review_performance_optimizations()
            
            # 8. ç”Ÿæˆå»ºè®®
            self._generate_recommendations()
            
            logger.info("ğŸ‰ GPUä»£ç å®¡æŸ¥å®Œæˆï¼")
            return self.results
            
        except Exception as e:
            logger.error(f"GPUä»£ç å®¡æŸ¥å¤±è´¥: {str(e)}")
            return self.results
    
    def _review_gpu_modules(self):
        """æ£€æŸ¥GPUç›¸å…³æ¨¡å—"""
        logger.info("1. GPUç›¸å…³æ¨¡å—æ£€æŸ¥")
        logger.info("-" * 40)
        
        gpu_modules = {
            'gpu_detector': {'path': 'ui/hardware/gpu_detector.py', 'exists': False, 'functions': []},
            'gpu_accelerator': {'path': 'ui/performance/gpu_accelerator.py', 'exists': False, 'functions': []},
            'compute_offloader': {'path': 'ui/hardware/compute_offloader.py', 'exists': False, 'functions': []},
            'performance_tier': {'path': 'ui/hardware/performance_tier.py', 'exists': False, 'functions': []}
        }
        
        for module_name, module_info in gpu_modules.items():
            module_path = PROJECT_ROOT / module_info['path']
            if module_path.exists():
                module_info['exists'] = True
                logger.info(f"âœ… {module_name}: æ–‡ä»¶å­˜åœ¨")
                
                # åˆ†ææ¨¡å—å†…å®¹
                try:
                    with open(module_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # æŸ¥æ‰¾GPUç›¸å…³å‡½æ•°
                    gpu_functions = self._find_gpu_functions(content)
                    module_info['functions'] = gpu_functions
                    
                    if gpu_functions:
                        logger.info(f"  - å‘ç°GPUç›¸å…³å‡½æ•°: {', '.join(gpu_functions)}")
                    else:
                        logger.warning(f"  - æœªå‘ç°GPUç›¸å…³å‡½æ•°")
                        
                except Exception as e:
                    logger.error(f"  - è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
            else:
                logger.warning(f"âš ï¸ {module_name}: æ–‡ä»¶ä¸å­˜åœ¨ ({module_info['path']})")
        
        self.results['gpu_modules'] = gpu_modules
    
    def _find_gpu_functions(self, content: str) -> List[str]:
        """æŸ¥æ‰¾GPUç›¸å…³å‡½æ•°"""
        gpu_functions = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾å‡½æ•°å®šä¹‰
        function_pattern = r'def\\\1+(\\\1*(?:gpu|cuda|device|accelerat|offload)\\\1*)\\\1*\\\1'
        matches = re.findall(function_pattern, content, re.IGNORECASE)
        gpu_functions.extend(matches)
        
        # æŸ¥æ‰¾åŒ…å«GPUå…³é”®è¯çš„å‡½æ•°
        lines = content.split('\n')
        for i, line in enumerate(lines):
            if line.strip().startswith('def '):
                func_name = line.split('def ')[1].split('(')[0].strip()
                # æ£€æŸ¥å‡½æ•°ä½“æ˜¯å¦åŒ…å«GPUç›¸å…³ä»£ç 
                func_body = self._get_function_body(lines, i)
                if any(keyword in func_body.lower() for keyword in ['gpu', 'cuda', 'device', 'torch.cuda']):
                    if func_name not in gpu_functions:
                        gpu_functions.append(func_name)
        
        return gpu_functions
    
    def _get_function_body(self, lines: List[str], start_line: int) -> str:
        """è·å–å‡½æ•°ä½“å†…å®¹"""
        body_lines = []
        indent_level = None
        
        for i in range(start_line + 1, min(start_line + 20, len(lines))):
            line = lines[i]
            if not line.strip():
                continue
                
            current_indent = len(line) - len(line.lstrip())
            
            if indent_level is None:
                indent_level = current_indent
            elif current_indent <= indent_level and line.strip():
                break
                
            body_lines.append(line)
        
        return '\n'.join(body_lines)
    
    def _review_video_processing(self):
        """æ£€æŸ¥è§†é¢‘å¤„ç†GPUåŠ é€Ÿ"""
        logger.info("2. è§†é¢‘å¤„ç†GPUåŠ é€Ÿæ£€æŸ¥")
        logger.info("-" * 40)
        
        video_processing = {
            'video_processor': {'gpu_support': False, 'gpu_functions': []},
            'clip_generator': {'gpu_support': False, 'gpu_functions': []},
            'quality_controller': {'gpu_support': False, 'gpu_functions': []}
        }
        
        # æ£€æŸ¥è§†é¢‘å¤„ç†å™¨
        video_processor_path = PROJECT_ROOT / 'src/core/video_processor.py'
        if video_processor_path.exists():
            try:
                with open(video_processor_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                gpu_functions = self._find_gpu_functions(content)
                video_processing['video_processor']['gpu_functions'] = gpu_functions
                
                # æ£€æŸ¥æ˜¯å¦æœ‰GPUåŠ é€Ÿä»£ç 
                if any(keyword in content.lower() for keyword in ['gpu', 'cuda', 'device']):
                    video_processing['video_processor']['gpu_support'] = True
                    logger.info("âœ… è§†é¢‘å¤„ç†å™¨: åŒ…å«GPUåŠ é€Ÿä»£ç ")
                else:
                    logger.warning("âš ï¸ è§†é¢‘å¤„ç†å™¨: æœªå‘ç°GPUåŠ é€Ÿä»£ç ")
                    
            except Exception as e:
                logger.error(f"è§†é¢‘å¤„ç†å™¨æ£€æŸ¥å¤±è´¥: {str(e)}")
        else:
            logger.warning("âš ï¸ è§†é¢‘å¤„ç†å™¨æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ£€æŸ¥å‰ªè¾‘ç”Ÿæˆå™¨
        clip_generator_path = PROJECT_ROOT / 'src/core/clip_generator.py'
        if clip_generator_path.exists():
            try:
                with open(clip_generator_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                gpu_functions = self._find_gpu_functions(content)
                video_processing['clip_generator']['gpu_functions'] = gpu_functions
                
                if any(keyword in content.lower() for keyword in ['gpu', 'cuda', 'device']):
                    video_processing['clip_generator']['gpu_support'] = True
                    logger.info("âœ… å‰ªè¾‘ç”Ÿæˆå™¨: åŒ…å«GPUåŠ é€Ÿä»£ç ")
                else:
                    logger.warning("âš ï¸ å‰ªè¾‘ç”Ÿæˆå™¨: æœªå‘ç°GPUåŠ é€Ÿä»£ç ")
                    
            except Exception as e:
                logger.error(f"å‰ªè¾‘ç”Ÿæˆå™¨æ£€æŸ¥å¤±è´¥: {str(e)}")
        else:
            logger.warning("âš ï¸ å‰ªè¾‘ç”Ÿæˆå™¨æ–‡ä»¶ä¸å­˜åœ¨")
        
        self.results['video_processing'] = video_processing
    
    def _review_model_inference(self):
        """æ£€æŸ¥æ¨¡å‹æ¨ç†GPUæ”¯æŒ"""
        logger.info("3. æ¨¡å‹æ¨ç†GPUæ”¯æŒæ£€æŸ¥")
        logger.info("-" * 40)
        
        model_inference = {
            'base_llm': {'gpu_support': False, 'device_management': False},
            'screenplay_engineer': {'gpu_support': False, 'device_management': False},
            'model_loader': {'gpu_support': False, 'device_management': False}
        }
        
        # æ£€æŸ¥åŸºç¡€LLMæ¨¡å‹
        base_llm_path = PROJECT_ROOT / 'src/models/base_llm.py'
        if base_llm_path.exists():
            try:
                with open(base_llm_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ£€æŸ¥GPUæ”¯æŒ
                if any(keyword in content.lower() for keyword in ['gpu', 'cuda', 'device', 'torch.cuda']):
                    model_inference['base_llm']['gpu_support'] = True
                    logger.info("âœ… åŸºç¡€LLM: åŒ…å«GPUæ”¯æŒä»£ç ")
                else:
                    logger.warning("âš ï¸ åŸºç¡€LLM: æœªå‘ç°GPUæ”¯æŒä»£ç ")
                
                # æ£€æŸ¥è®¾å¤‡ç®¡ç†
                if 'device' in content.lower() and ('to(' in content or '.cuda()' in content):
                    model_inference['base_llm']['device_management'] = True
                    logger.info("âœ… åŸºç¡€LLM: åŒ…å«è®¾å¤‡ç®¡ç†ä»£ç ")
                else:
                    logger.warning("âš ï¸ åŸºç¡€LLM: æœªå‘ç°è®¾å¤‡ç®¡ç†ä»£ç ")
                    
            except Exception as e:
                logger.error(f"åŸºç¡€LLMæ£€æŸ¥å¤±è´¥: {str(e)}")
        else:
            logger.warning("âš ï¸ åŸºç¡€LLMæ–‡ä»¶ä¸å­˜åœ¨")
        
        self.results['model_inference'] = model_inference
    
    def _review_memory_management(self):
        """æ£€æŸ¥GPUå†…å­˜ç®¡ç†"""
        logger.info("4. GPUå†…å­˜ç®¡ç†æ£€æŸ¥")
        logger.info("-" * 40)
        
        memory_management = {
            'memory_guard': {'gpu_memory_tracking': False, 'gpu_cleanup': False},
            'memory_optimizer': {'gpu_optimization': False, 'memory_pooling': False}
        }
        
        # æ£€æŸ¥å†…å­˜å®ˆæŠ¤
        memory_guard_path = PROJECT_ROOT / 'ui/performance/memory_guard.py'
        if memory_guard_path.exists():
            try:
                with open(memory_guard_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if 'gpu' in content.lower() or 'cuda' in content.lower():
                    memory_management['memory_guard']['gpu_memory_tracking'] = True
                    logger.info("âœ… å†…å­˜å®ˆæŠ¤: åŒ…å«GPUå†…å­˜è·Ÿè¸ª")
                else:
                    logger.warning("âš ï¸ å†…å­˜å®ˆæŠ¤: æœªå‘ç°GPUå†…å­˜è·Ÿè¸ª")
                    
            except Exception as e:
                logger.error(f"å†…å­˜å®ˆæŠ¤æ£€æŸ¥å¤±è´¥: {str(e)}")
        else:
            logger.warning("âš ï¸ å†…å­˜å®ˆæŠ¤æ–‡ä»¶ä¸å­˜åœ¨")
        
        self.results['memory_management'] = memory_management

    def _review_device_switching(self):
        """æ£€æŸ¥è®¾å¤‡åˆ‡æ¢é€»è¾‘"""
        logger.info("5. è®¾å¤‡åˆ‡æ¢é€»è¾‘æ£€æŸ¥")
        logger.info("-" * 40)

        device_switching = {
            'automatic_detection': False,
            'manual_switching': False,
            'device_validation': False,
            'error_handling': False
        }

        # æ£€æŸ¥è®¡ç®—å¸è½½å™¨ä¸­çš„è®¾å¤‡åˆ‡æ¢
        compute_offloader_path = PROJECT_ROOT / 'ui/hardware/compute_offloader.py'
        if compute_offloader_path.exists():
            try:
                with open(compute_offloader_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                if 'device' in content.lower() and 'switch' in content.lower():
                    device_switching['automatic_detection'] = True
                    logger.info("âœ… è®¾å¤‡åˆ‡æ¢: åŒ…å«è‡ªåŠ¨æ£€æµ‹é€»è¾‘")

                if 'set_device' in content or 'switch_device' in content:
                    device_switching['manual_switching'] = True
                    logger.info("âœ… è®¾å¤‡åˆ‡æ¢: åŒ…å«æ‰‹åŠ¨åˆ‡æ¢åŠŸèƒ½")

                if 'validate' in content.lower() and 'device' in content.lower():
                    device_switching['device_validation'] = True
                    logger.info("âœ… è®¾å¤‡åˆ‡æ¢: åŒ…å«è®¾å¤‡éªŒè¯")

                if 'try:' in content and 'except' in content:
                    device_switching['error_handling'] = True
                    logger.info("âœ… è®¾å¤‡åˆ‡æ¢: åŒ…å«é”™è¯¯å¤„ç†")

            except Exception as e:
                logger.error(f"è®¾å¤‡åˆ‡æ¢æ£€æŸ¥å¤±è´¥: {str(e)}")
        else:
            logger.warning("âš ï¸ è®¡ç®—å¸è½½å™¨æ–‡ä»¶ä¸å­˜åœ¨")

        self.results['device_switching'] = device_switching

    def _review_fallback_mechanisms(self):
        """æ£€æŸ¥é™çº§æœºåˆ¶"""
        logger.info("6. é™çº§æœºåˆ¶æ£€æŸ¥")
        logger.info("-" * 40)

        fallback_mechanisms = {
            'gpu_to_cpu_fallback': False,
            'graceful_degradation': False,
            'performance_monitoring': False,
            'user_notification': False
        }

        # æœç´¢æ‰€æœ‰Pythonæ–‡ä»¶ä¸­çš„é™çº§æœºåˆ¶
        python_files = list(PROJECT_ROOT.rglob('*.py'))
        fallback_patterns = [
            r'if.*cuda.*available.*else',
            r'try.*gpu.*except.*cpu',
            r'fallback.*cpu',
            r'device.*cpu.*default'
        ]

        fallback_found = False
        for py_file in python_files[:20]:  # é™åˆ¶æ£€æŸ¥æ–‡ä»¶æ•°é‡
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                for pattern in fallback_patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        fallback_found = True
                        fallback_mechanisms['gpu_to_cpu_fallback'] = True
                        logger.info(f"âœ… é™çº§æœºåˆ¶: åœ¨ {py_file.name} ä¸­å‘ç°GPUåˆ°CPUé™çº§")
                        break

                if fallback_found:
                    break

            except Exception:
                continue

        if not fallback_found:
            logger.warning("âš ï¸ é™çº§æœºåˆ¶: æœªå‘ç°GPUåˆ°CPUé™çº§é€»è¾‘")

        self.results['fallback_mechanisms'] = fallback_mechanisms

    def _review_performance_optimizations(self):
        """æ£€æŸ¥æ€§èƒ½ä¼˜åŒ–"""
        logger.info("7. æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥")
        logger.info("-" * 40)

        performance_optimizations = {
            'batch_processing': False,
            'memory_pooling': False,
            'async_operations': False,
            'kernel_optimization': False
        }

        # æ£€æŸ¥æ€§èƒ½ä¼˜åŒ–ç›¸å…³æ–‡ä»¶
        perf_files = [
            'ui/performance/gpu_accelerator.py',
            'ui/hardware/compute_offloader.py',
            'src/core/video_processor.py'
        ]

        for file_path in perf_files:
            full_path = PROJECT_ROOT / file_path
            if full_path.exists():
                try:
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if 'batch' in content.lower():
                        performance_optimizations['batch_processing'] = True
                        logger.info(f"âœ… æ€§èƒ½ä¼˜åŒ–: {file_path} åŒ…å«æ‰¹å¤„ç†ä¼˜åŒ–")

                    if 'pool' in content.lower() and 'memory' in content.lower():
                        performance_optimizations['memory_pooling'] = True
                        logger.info(f"âœ… æ€§èƒ½ä¼˜åŒ–: {file_path} åŒ…å«å†…å­˜æ± ä¼˜åŒ–")

                    if 'async' in content.lower() or 'await' in content:
                        performance_optimizations['async_operations'] = True
                        logger.info(f"âœ… æ€§èƒ½ä¼˜åŒ–: {file_path} åŒ…å«å¼‚æ­¥æ“ä½œ")

                except Exception as e:
                    logger.error(f"æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥å¤±è´¥ {file_path}: {str(e)}")

        self.results['performance_optimizations'] = performance_optimizations

    def _generate_recommendations(self):
        """ç”Ÿæˆä»£ç æ”¹è¿›å»ºè®®"""
        logger.info("8. ç”Ÿæˆä»£ç æ”¹è¿›å»ºè®®")
        logger.info("-" * 40)

        recommendations = []

        # åŸºäºGPUæ¨¡å—æ£€æŸ¥ç»“æœçš„å»ºè®®
        gpu_modules = self.results['gpu_modules']
        missing_modules = [name for name, info in gpu_modules.items() if not info['exists']]

        if missing_modules:
            recommendations.append({
                'type': 'missing_modules',
                'priority': 'high',
                'title': 'ç¼ºå¤±GPUæ¨¡å—',
                'description': f'ä»¥ä¸‹GPUæ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨: {", ".join(missing_modules)}',
                'action': 'åˆ›å»ºç¼ºå¤±çš„GPUæ¨¡å—æ–‡ä»¶å¹¶å®ç°ç›¸åº”åŠŸèƒ½'
            })

        # åŸºäºè§†é¢‘å¤„ç†æ£€æŸ¥ç»“æœçš„å»ºè®®
        video_processing = self.results['video_processing']
        if not video_processing['video_processor']['gpu_support']:
            recommendations.append({
                'type': 'video_processing',
                'priority': 'high',
                'title': 'è§†é¢‘å¤„ç†å™¨ç¼ºå°‘GPUåŠ é€Ÿ',
                'description': 'è§†é¢‘å¤„ç†å™¨æœªå®ç°GPUåŠ é€ŸåŠŸèƒ½',
                'action': 'åœ¨è§†é¢‘å¤„ç†å™¨ä¸­æ·»åŠ GPUåŠ é€Ÿä»£ç ï¼Œä½¿ç”¨CUDAæˆ–OpenCL'
            })

        # åŸºäºæ¨¡å‹æ¨ç†æ£€æŸ¥ç»“æœçš„å»ºè®®
        model_inference = self.results['model_inference']
        if not model_inference['base_llm']['gpu_support']:
            recommendations.append({
                'type': 'model_inference',
                'priority': 'high',
                'title': 'æ¨¡å‹æ¨ç†ç¼ºå°‘GPUæ”¯æŒ',
                'description': 'åŸºç¡€LLMæ¨¡å‹æœªå®ç°GPUæ¨ç†',
                'action': 'åœ¨æ¨¡å‹æ¨ç†ä¸­æ·»åŠ GPUè®¾å¤‡ç®¡ç†å’ŒCUDAæ”¯æŒ'
            })

        # åŸºäºé™çº§æœºåˆ¶æ£€æŸ¥ç»“æœçš„å»ºè®®
        fallback = self.results['fallback_mechanisms']
        if not fallback['gpu_to_cpu_fallback']:
            recommendations.append({
                'type': 'fallback',
                'priority': 'medium',
                'title': 'ç¼ºå°‘GPUé™çº§æœºåˆ¶',
                'description': 'æœªå‘ç°GPUä¸å¯ç”¨æ—¶çš„CPUé™çº§é€»è¾‘',
                'action': 'å®ç°GPUåˆ°CPUçš„è‡ªåŠ¨é™çº§æœºåˆ¶ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§'
            })

        # åŸºäºæ€§èƒ½ä¼˜åŒ–æ£€æŸ¥ç»“æœçš„å»ºè®®
        perf_opt = self.results['performance_optimizations']
        if not perf_opt['batch_processing']:
            recommendations.append({
                'type': 'performance',
                'priority': 'medium',
                'title': 'ç¼ºå°‘æ‰¹å¤„ç†ä¼˜åŒ–',
                'description': 'æœªå‘ç°æ‰¹å¤„ç†ä¼˜åŒ–ä»£ç ',
                'action': 'å®ç°æ‰¹å¤„ç†æœºåˆ¶ä»¥æé«˜GPUåˆ©ç”¨ç‡'
            })

        self.results['recommendations'] = recommendations

        logger.info("ç”Ÿæˆçš„ä»£ç æ”¹è¿›å»ºè®®:")
        for rec in recommendations:
            logger.info(f"  [{rec['priority'].upper()}] {rec['title']}: {rec['description']}")


if __name__ == "__main__":
    reviewer = GPUCodeReviewer()
    results = reviewer.run_code_review()

    print("\n" + "=" * 60)
    print("GPUä»£ç å®¡æŸ¥å®Œæˆï¼è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ gpu_code_review.log")
    print("=" * 60)
