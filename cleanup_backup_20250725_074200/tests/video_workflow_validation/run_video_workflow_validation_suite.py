#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘å¤„ç†å·¥ä½œæµéªŒè¯ç³»ç»Ÿä¸»è¿è¡Œè„šæœ¬
æ•´åˆæ‰€æœ‰è§†é¢‘å·¥ä½œæµæµ‹è¯•ï¼Œç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""

import os
import sys
import json
import time
import logging
import unittest
import traceback
from typing import Dict, List, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from tests.video_workflow_validation.test_end_to_end_workflow import EndToEndWorkflowTest
from tests.video_workflow_validation.test_video_format_compatibility import VideoFormatCompatibilityTest
from tests.video_workflow_validation.test_quality_validation import QualityValidationTest
from tests.video_workflow_validation.test_ui_integration import UIIntegrationTest
from tests.video_workflow_validation.test_exception_handling import ExceptionHandlingTest

class VideoWorkflowValidationSuite:
    """è§†é¢‘å·¥ä½œæµéªŒè¯ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯ç³»ç»Ÿ"""
        self.start_time = time.time()
        self.results = {
            "suite_info": {
                "start_time": datetime.now().isoformat(),
                "version": "1.0.0",
                "description": "VisionAI-ClipsMasterè§†é¢‘å¤„ç†å·¥ä½œæµéªŒè¯ç³»ç»Ÿ"
            },
            "test_results": {},
            "performance_metrics": {},
            "summary": {},
            "recommendations": []
        }
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        self.logger = logging.getLogger(__name__)
        
        self.logger.info("=" * 70)
        self.logger.info("VisionAI-ClipsMaster è§†é¢‘å·¥ä½œæµéªŒè¯ç³»ç»Ÿå¯åŠ¨")
        self.logger.info("=" * 70)
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = os.path.join(PROJECT_ROOT, "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(log_dir, f"video_workflow_validation_{timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰è§†é¢‘å·¥ä½œæµéªŒè¯æµ‹è¯•"""
        self.logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´çš„è§†é¢‘å·¥ä½œæµéªŒè¯æµ‹è¯•å¥—ä»¶...")
        
        test_modules = [
            ("end_to_end_workflow", EndToEndWorkflowTest, "ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•"),
            ("video_format_compatibility", VideoFormatCompatibilityTest, "è§†é¢‘æ ¼å¼å…¼å®¹æ€§æµ‹è¯•"),
            ("quality_validation", QualityValidationTest, "è´¨é‡éªŒè¯æµ‹è¯•"),
            ("ui_integration", UIIntegrationTest, "UIç•Œé¢é›†æˆæµ‹è¯•"),
            ("exception_handling", ExceptionHandlingTest, "å¼‚å¸¸å¤„ç†æµ‹è¯•")
        ]
        
        for test_name, test_class, description in test_modules:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"æ‰§è¡Œæµ‹è¯•æ¨¡å—: {description}")
            self.logger.info(f"{'='*60}")
            
            try:
                result = self._run_test_module(test_class, test_name)
                self.results["test_results"][test_name] = result
                
                if result["success"]:
                    self.logger.info(f"âœ… {description} å®Œæˆ")
                else:
                    self.logger.error(f"âŒ {description} å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                error_msg = f"æµ‹è¯•æ¨¡å— {test_name} æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                self.logger.error(error_msg)
                self.logger.error(traceback.format_exc())
                
                self.results["test_results"][test_name] = {
                    "success": False,
                    "error": error_msg,
                    "traceback": traceback.format_exc()
                }
        
        # ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡
        self._generate_performance_metrics()
        
        # ç”Ÿæˆç»¼åˆåˆ†æ
        self._generate_summary()
        
        # ç”Ÿæˆå»ºè®®
        self._generate_recommendations()
        
        # ä¿å­˜ç»“æœ
        self._save_comprehensive_report()
        
        return self.results
    
    def _run_test_module(self, test_class, test_name: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ¨¡å—"""
        try:
            # è®°å½•å¼€å§‹æ—¶é—´
            module_start_time = time.time()
            
            # åˆ›å»ºæµ‹è¯•å¥—ä»¶
            suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
            
            # è¿è¡Œæµ‹è¯•
            runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
            test_result = runner.run(suite)
            
            # è®°å½•ç»“æŸæ—¶é—´
            module_end_time = time.time()
            module_duration = module_end_time - module_start_time
            
            # æ”¶é›†ç»“æœ
            result = {
                "success": test_result.wasSuccessful(),
                "tests_run": test_result.testsRun,
                "failures": len(test_result.failures),
                "errors": len(test_result.errors),
                "skipped": len(test_result.skipped) if hasattr(test_result, 'skipped') else 0,
                "duration": module_duration,
                "details": {
                    "failures": [str(failure) for failure in test_result.failures],
                    "errors": [str(error) for error in test_result.errors]
                }
            }
            
            return result
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
    
    def _generate_performance_metrics(self):
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡"""
        try:
            total_duration = time.time() - self.start_time
            
            # æ”¶é›†å„æ¨¡å—çš„æ€§èƒ½æ•°æ®
            module_durations = {}
            total_tests = 0
            
            for module_name, result in self.results["test_results"].items():
                if result.get("success") is not None:
                    module_durations[module_name] = result.get("duration", 0)
                    total_tests += result.get("tests_run", 0)
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            self.results["performance_metrics"] = {
                "total_duration": total_duration,
                "total_tests": total_tests,
                "avg_test_duration": total_duration / total_tests if total_tests > 0 else 0,
                "module_durations": module_durations,
                "fastest_module": min(module_durations.items(), key=lambda x: x[1]) if module_durations else None,
                "slowest_module": max(module_durations.items(), key=lambda x: x[1]) if module_durations else None,
                "performance_score": self._calculate_performance_score(module_durations, total_duration)
            }
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            self.results["performance_metrics"] = {"error": str(e)}
    
    def _calculate_performance_score(self, module_durations: Dict[str, float], total_duration: float) -> float:
        """è®¡ç®—æ€§èƒ½åˆ†æ•°"""
        try:
            # åŸºäºé¢„æœŸæ—¶é—´è®¡ç®—æ€§èƒ½åˆ†æ•°
            expected_durations = {
                "end_to_end_workflow": 120,      # 2åˆ†é’Ÿ
                "video_format_compatibility": 180, # 3åˆ†é’Ÿ
                "quality_validation": 150,        # 2.5åˆ†é’Ÿ
                "ui_integration": 60,             # 1åˆ†é’Ÿ
                "exception_handling": 90          # 1.5åˆ†é’Ÿ
            }
            
            total_expected = sum(expected_durations.values())
            performance_ratio = total_expected / total_duration if total_duration > 0 else 0
            
            # è½¬æ¢ä¸º0-100åˆ†æ•°
            return min(100, max(0, performance_ratio * 100))
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—æ€§èƒ½åˆ†æ•°å¤±è´¥: {e}")
            return 0.0
    
    def _generate_summary(self):
        """ç”Ÿæˆæµ‹è¯•ç»“æœæ‘˜è¦"""
        total_modules = len(self.results["test_results"])
        successful_modules = 0
        total_tests = 0
        total_failures = 0
        total_errors = 0
        
        for module_name, result in self.results["test_results"].items():
            if result.get("success", False):
                successful_modules += 1
            
            total_tests += result.get("tests_run", 0)
            total_failures += result.get("failures", 0)
            total_errors += result.get("errors", 0)
        
        end_time = time.time()
        duration = end_time - self.start_time
        
        self.results["summary"] = {
            "total_modules": total_modules,
            "successful_modules": successful_modules,
            "module_success_rate": successful_modules / total_modules if total_modules > 0 else 0,
            "total_tests": total_tests,
            "total_failures": total_failures,
            "total_errors": total_errors,
            "test_success_rate": (total_tests - total_failures - total_errors) / total_tests if total_tests > 0 else 0,
            "duration_seconds": duration,
            "overall_success": total_failures == 0 and total_errors == 0 and successful_modules == total_modules
        }
        
        # è®°å½•æ‘˜è¦
        summary = self.results["summary"]
        self.logger.info(f"\n{'='*70}")
        self.logger.info("è§†é¢‘å·¥ä½œæµéªŒè¯ç»“æœæ‘˜è¦")
        self.logger.info(f"{'='*70}")
        self.logger.info(f"æ€»æ¨¡å—æ•°: {summary['total_modules']}")
        self.logger.info(f"æˆåŠŸæ¨¡å—: {summary['successful_modules']}")
        self.logger.info(f"æ¨¡å—æˆåŠŸç‡: {summary['module_success_rate']:.1%}")
        self.logger.info(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        self.logger.info(f"å¤±è´¥æµ‹è¯•: {summary['total_failures']}")
        self.logger.info(f"é”™è¯¯æµ‹è¯•: {summary['total_errors']}")
        self.logger.info(f"æµ‹è¯•æˆåŠŸç‡: {summary['test_success_rate']:.1%}")
        self.logger.info(f"æ€»è€—æ—¶: {summary['duration_seconds']:.1f}ç§’")
        self.logger.info(f"æ•´ä½“çŠ¶æ€: {'âœ… æˆåŠŸ' if summary['overall_success'] else 'âŒ å¤±è´¥'}")
    
    def _generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†ææµ‹è¯•ç»“æœå¹¶ç”Ÿæˆå»ºè®®
        test_results = self.results["test_results"]
        performance_metrics = self.results["performance_metrics"]
        
        # ç«¯åˆ°ç«¯å·¥ä½œæµå»ºè®®
        if "end_to_end_workflow" in test_results:
            workflow_result = test_results["end_to_end_workflow"]
            if not workflow_result.get("success", False):
                recommendations.append({
                    "category": "ç«¯åˆ°ç«¯å·¥ä½œæµ",
                    "priority": "é«˜",
                    "issue": "ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•å¤±è´¥",
                    "recommendation": "æ£€æŸ¥è§†é¢‘å¤„ç†ç®¡é“å’Œå­—å¹•é‡æ„æœåŠ¡çš„é›†æˆï¼Œç¡®ä¿å·¥ä½œæµå„é˜¶æ®µæ­£å¸¸è¿è¡Œ"
                })
        
        # è§†é¢‘æ ¼å¼å…¼å®¹æ€§å»ºè®®
        if "video_format_compatibility" in test_results:
            format_result = test_results["video_format_compatibility"]
            if not format_result.get("success", False):
                recommendations.append({
                    "category": "æ ¼å¼å…¼å®¹æ€§",
                    "priority": "ä¸­",
                    "issue": "è§†é¢‘æ ¼å¼å…¼å®¹æ€§æµ‹è¯•å¤±è´¥",
                    "recommendation": "å¢å¼ºè§†é¢‘æ ¼å¼æ£€æµ‹å’Œè½¬æ¢åŠŸèƒ½ï¼Œæ”¯æŒæ›´å¤šä¸»æµè§†é¢‘æ ¼å¼"
                })
        
        # è´¨é‡éªŒè¯å»ºè®®
        if "quality_validation" in test_results:
            quality_result = test_results["quality_validation"]
            if not quality_result.get("success", False):
                recommendations.append({
                    "category": "è§†é¢‘è´¨é‡",
                    "priority": "é«˜",
                    "issue": "è§†é¢‘è´¨é‡éªŒè¯å¤±è´¥",
                    "recommendation": "ä¼˜åŒ–è§†é¢‘å¤„ç†ç®—æ³•ï¼Œç¡®ä¿è¾“å‡ºè´¨é‡å’Œå­—å¹•åŒæ­¥ç²¾åº¦"
                })
        
        # UIé›†æˆå»ºè®®
        if "ui_integration" in test_results:
            ui_result = test_results["ui_integration"]
            if not ui_result.get("success", False):
                recommendations.append({
                    "category": "ç”¨æˆ·ç•Œé¢",
                    "priority": "ä¸­",
                    "issue": "UIé›†æˆæµ‹è¯•å¤±è´¥",
                    "recommendation": "æ”¹è¿›ç”¨æˆ·ç•Œé¢å“åº”æ€§å’Œé”™è¯¯å¤„ç†ï¼Œæå‡ç”¨æˆ·ä½“éªŒ"
                })
        
        # å¼‚å¸¸å¤„ç†å»ºè®®
        if "exception_handling" in test_results:
            exception_result = test_results["exception_handling"]
            if not exception_result.get("success", False):
                recommendations.append({
                    "category": "å¼‚å¸¸å¤„ç†",
                    "priority": "é«˜",
                    "issue": "å¼‚å¸¸å¤„ç†æµ‹è¯•å¤±è´¥",
                    "recommendation": "åŠ å¼ºå¼‚å¸¸å¤„ç†æœºåˆ¶å’Œæ¢å¤ç­–ç•¥ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§"
                })
        
        # æ€§èƒ½å»ºè®®
        if performance_metrics.get("performance_score", 0) < 70:
            recommendations.append({
                "category": "æ€§èƒ½ä¼˜åŒ–",
                "priority": "ä¸­",
                "issue": f"ç³»ç»Ÿæ€§èƒ½åˆ†æ•°è¾ƒä½: {performance_metrics.get('performance_score', 0):.1f}",
                "recommendation": "ä¼˜åŒ–è§†é¢‘å¤„ç†ç®—æ³•å’Œå†…å­˜ç®¡ç†ï¼Œæé«˜å¤„ç†é€Ÿåº¦"
            })
        
        # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œç»™å‡ºç§¯æå»ºè®®
        if not recommendations:
            recommendations.append({
                "category": "ç³»ç»ŸçŠ¶æ€",
                "priority": "ä¿¡æ¯",
                "issue": "æ‰€æœ‰æµ‹è¯•é€šè¿‡",
                "recommendation": "è§†é¢‘å·¥ä½œæµç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹å’Œæ€§èƒ½ä¼˜åŒ–"
            })
        
        self.results["recommendations"] = recommendations
        
        # è®°å½•å»ºè®®
        self.logger.info(f"\n{'='*70}")
        self.logger.info("ä¼˜åŒ–å»ºè®®")
        self.logger.info(f"{'='*70}")
        for i, rec in enumerate(recommendations, 1):
            self.logger.info(f"{i}. [{rec['priority']}] {rec['category']}: {rec['issue']}")
            self.logger.info(f"   å»ºè®®: {rec['recommendation']}")
    
    def _save_comprehensive_report(self):
        """ä¿å­˜ç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        try:
            # æ›´æ–°ç»“æŸæ—¶é—´
            self.results["suite_info"]["end_time"] = datetime.now().isoformat()
            self.results["suite_info"]["duration_seconds"] = time.time() - self.start_time
            
            # ä¿å­˜JSONæŠ¥å‘Š
            output_dir = os.path.join(PROJECT_ROOT, "test_output")
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_file = os.path.join(output_dir, f"video_workflow_validation_report_{timestamp}.json")
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            
            # ç”ŸæˆHTMLæŠ¥å‘Š
            html_file = os.path.join(output_dir, f"video_workflow_validation_report_{timestamp}.html")
            self._generate_html_report(html_file)
            
            # ç”Ÿæˆæ€§èƒ½åŸºå‡†æŠ¥å‘Š
            benchmark_file = os.path.join(output_dir, f"performance_benchmark_{timestamp}.json")
            self._generate_benchmark_report(benchmark_file)
            
            self.logger.info(f"\nğŸ“Š ç»¼åˆæµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ:")
            self.logger.info(f"   JSONæŠ¥å‘Š: {json_file}")
            self.logger.info(f"   HTMLæŠ¥å‘Š: {html_file}")
            self.logger.info(f"   æ€§èƒ½åŸºå‡†: {benchmark_file}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_html_report(self, html_file: str):
        """ç”ŸæˆHTMLæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
        try:
            summary = self.results["summary"]
            performance = self.results["performance_metrics"]
            
            html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VisionAI-ClipsMaster è§†é¢‘å·¥ä½œæµéªŒè¯æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; }}
        .summary {{ background-color: #f8f9fa; padding: 20px; margin: 20px 0; border-radius: 8px; border-left: 5px solid #007bff; }}
        .success {{ background-color: #d4edda; border-color: #28a745; }}
        .warning {{ background-color: #fff3cd; border-color: #ffc107; }}
        .error {{ background-color: #f8d7da; border-color: #dc3545; }}
        .metric-card {{ background: white; padding: 15px; margin: 10px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); display: inline-block; min-width: 200px; }}
        .recommendation {{ background-color: #e3f2fd; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #2196f3; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
        th {{ background-color: #f2f2f2; font-weight: bold; }}
        .status-success {{ color: #28a745; font-weight: bold; }}
        .status-error {{ color: #dc3545; font-weight: bold; }}
        .progress-bar {{ background-color: #e9ecef; border-radius: 4px; overflow: hidden; }}
        .progress-fill {{ background-color: #28a745; height: 20px; transition: width 0.3s ease; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¬ VisionAI-ClipsMaster è§†é¢‘å·¥ä½œæµéªŒè¯æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {self.results['suite_info']['end_time']}</p>
        <p>æµ‹è¯•ç‰ˆæœ¬: {self.results['suite_info']['version']}</p>
    </div>
    
    <div class="summary {'success' if summary['overall_success'] else 'error'}">
        <h2>ğŸ“Š æµ‹è¯•æ‘˜è¦</h2>
        <div class="metric-card">
            <h3>æ•´ä½“çŠ¶æ€</h3>
            <p class="{'status-success' if summary['overall_success'] else 'status-error'}">
                {'âœ… å…¨éƒ¨é€šè¿‡' if summary['overall_success'] else 'âŒ å­˜åœ¨é—®é¢˜'}
            </p>
        </div>
        <div class="metric-card">
            <h3>æ¨¡å—æˆåŠŸç‡</h3>
            <div class="progress-bar">
                <div class="progress-fill" style="width: {summary['module_success_rate']*100:.1f}%"></div>
            </div>
            <p>{summary['module_success_rate']:.1%} ({summary['successful_modules']}/{summary['total_modules']})</p>
        </div>
        <div class="metric-card">
            <h3>æµ‹è¯•æˆåŠŸç‡</h3>
            <div class="progress-bar">
                <div class="progress-fill" style="width: {summary['test_success_rate']*100:.1f}%"></div>
            </div>
            <p>{summary['test_success_rate']:.1%} ({summary['total_tests'] - summary['total_failures'] - summary['total_errors']}/{summary['total_tests']})</p>
        </div>
        <div class="metric-card">
            <h3>æ‰§è¡Œæ—¶é—´</h3>
            <p>{summary['duration_seconds']:.1f} ç§’</p>
        </div>
        <div class="metric-card">
            <h3>æ€§èƒ½åˆ†æ•°</h3>
            <p>{performance.get('performance_score', 0):.1f}/100</p>
        </div>
    </div>
    
    <h2>ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ</h2>
    <table>
        <tr>
            <th>æµ‹è¯•æ¨¡å—</th>
            <th>çŠ¶æ€</th>
            <th>æµ‹è¯•æ•°</th>
            <th>å¤±è´¥æ•°</th>
            <th>é”™è¯¯æ•°</th>
            <th>è€—æ—¶(ç§’)</th>
        </tr>
"""
            
            module_names = {
                "end_to_end_workflow": "ç«¯åˆ°ç«¯å·¥ä½œæµ",
                "video_format_compatibility": "è§†é¢‘æ ¼å¼å…¼å®¹æ€§",
                "quality_validation": "è´¨é‡éªŒè¯",
                "ui_integration": "UIç•Œé¢é›†æˆ",
                "exception_handling": "å¼‚å¸¸å¤„ç†"
            }
            
            for module_key, result in self.results["test_results"].items():
                module_name = module_names.get(module_key, module_key)
                status = "âœ… æˆåŠŸ" if result.get("success", False) else "âŒ å¤±è´¥"
                status_class = "status-success" if result.get("success", False) else "status-error"
                
                html_content += f"""
        <tr>
            <td>{module_name}</td>
            <td class="{status_class}">{status}</td>
            <td>{result.get('tests_run', 0)}</td>
            <td>{result.get('failures', 0)}</td>
            <td>{result.get('errors', 0)}</td>
            <td>{result.get('duration', 0):.1f}</td>
        </tr>
"""
            
            html_content += """
    </table>
    
    <h2>ğŸ’¡ ä¼˜åŒ–å»ºè®®</h2>
"""
            
            for i, rec in enumerate(self.results["recommendations"], 1):
                priority_class = {
                    "é«˜": "error",
                    "ä¸­": "warning", 
                    "ä½": "success",
                    "ä¿¡æ¯": "success"
                }.get(rec['priority'], "summary")
                
                html_content += f"""
    <div class="recommendation {priority_class}">
        <h4>{i}. [{rec['priority']}] {rec['category']}</h4>
        <p><strong>é—®é¢˜:</strong> {rec['issue']}</p>
        <p><strong>å»ºè®®:</strong> {rec['recommendation']}</p>
    </div>
"""
            
            html_content += """
</body>
</html>
"""
            
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
                
        except Exception as e:
            self.logger.error(f"ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {e}")
    
    def _generate_benchmark_report(self, benchmark_file: str):
        """ç”Ÿæˆæ€§èƒ½åŸºå‡†æŠ¥å‘Š"""
        try:
            benchmark_data = {
                "timestamp": datetime.now().isoformat(),
                "system_info": {
                    "platform": sys.platform,
                    "python_version": sys.version
                },
                "performance_metrics": self.results["performance_metrics"],
                "baseline_comparison": {
                    "expected_total_duration": 600,  # 10åˆ†é’ŸåŸºå‡†
                    "actual_total_duration": self.results["performance_metrics"].get("total_duration", 0),
                    "performance_ratio": self.results["performance_metrics"].get("performance_score", 0) / 100
                }
            }
            
            with open(benchmark_file, 'w', encoding='utf-8') as f:
                json.dump(benchmark_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ€§èƒ½åŸºå‡†æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºå¹¶è¿è¡ŒéªŒè¯ç³»ç»Ÿ
        validation_suite = VideoWorkflowValidationSuite()
        results = validation_suite.run_all_tests()
        
        # è¿”å›é€€å‡ºç 
        if results["summary"]["overall_success"]:
            print("\nğŸ‰ æ‰€æœ‰è§†é¢‘å·¥ä½œæµéªŒè¯æµ‹è¯•é€šè¿‡ï¼")
            sys.exit(0)
        else:
            print("\nâš ï¸  éƒ¨åˆ†è§†é¢‘å·¥ä½œæµéªŒè¯æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nğŸ’¥ è§†é¢‘å·¥ä½œæµéªŒè¯ç³»ç»Ÿæ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        sys.exit(2)

if __name__ == "__main__":
    main()
