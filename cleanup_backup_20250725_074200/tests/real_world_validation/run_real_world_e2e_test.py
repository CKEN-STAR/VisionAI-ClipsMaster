#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster çœŸå®ä¸–ç•Œç«¯åˆ°ç«¯æµ‹è¯•ä¸»æ‰§è¡Œå™¨

æ­¤è„šæœ¬æ‰§è¡Œå®Œæ•´çš„çœŸå®ç¯å¢ƒç«¯åˆ°ç«¯åŠŸèƒ½éªŒè¯æµ‹è¯•ï¼Œ
ä»è§†é¢‘ä¸Šä¼ åˆ°å‰ªæ˜ å·¥ç¨‹æ–‡ä»¶ç”Ÿæˆçš„å…¨é“¾è·¯æµ‹è¯•ã€‚
"""

import os
import sys
import time
import logging
import argparse
from typing import Dict, List, Any, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from video_preprocessing.test_video_upload import RealWorldVideoUploadTester
from ai_script_reconstruction.test_content_understanding import RealWorldContentUnderstandingTester
from workflow_integration.test_complete_pipeline import CompletePipelineTester
from performance_monitoring.test_memory_constraints import MemoryConstraintTester
from jianying_compatibility.test_import_compatibility import JianyingImportCompatibilityTester
from reporting.real_world_report_generator import RealWorldReportGenerator
from test_data.create_test_dataset import RealWorldTestDatasetCreator

# å¯¼å…¥å·¥å…·æ¨¡å—
from src.utils.log_handler import LogHandler

logger = logging.getLogger(__name__)


class RealWorldE2ETestRunner:
    """çœŸå®ä¸–ç•Œç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–æµ‹è¯•æ‰§è¡Œå™¨"""
        self.config_path = config_path or "tests/real_world_validation/real_world_config.yaml"
        self.config = self._load_config()
        
        # è®¾ç½®æ—¥å¿—
        self.logger = LogHandler.get_logger(
            name=__name__,
            level=self.config.get('test_environment', {}).get('log_level', 'INFO')
        )
        
        # åˆå§‹åŒ–æµ‹è¯•ç»„ä»¶
        self.video_upload_tester = RealWorldVideoUploadTester(self.config_path)
        self.content_understanding_tester = RealWorldContentUnderstandingTester(self.config_path)
        self.pipeline_tester = CompletePipelineTester(self.config_path)
        self.memory_tester = MemoryConstraintTester(self.config_path)
        self.compatibility_tester = JianyingImportCompatibilityTester(self.config_path)
        self.report_generator = RealWorldReportGenerator(self.config_path)
        self.dataset_creator = RealWorldTestDatasetCreator(self.config_path)
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.all_test_results = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.get('file_paths', {}).get('output', {}).get('reports_dir', 'tests/reports/real_world_validation'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        try:
            import yaml
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {self.config_path}: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'test_environment': {'log_level': 'INFO'},
            'file_paths': {'output': {'reports_dir': 'tests/reports/real_world_validation'}}
        }
    
    def run_complete_real_world_test(self, test_videos: List[str] = None, 
                                   test_duration_hours: float = 2.0,
                                   skip_data_creation: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„çœŸå®ä¸–ç•Œæµ‹è¯•
        
        Args:
            test_videos: æµ‹è¯•è§†é¢‘æ–‡ä»¶åˆ—è¡¨
            test_duration_hours: æµ‹è¯•æŒç»­æ—¶é—´
            skip_data_creation: æ˜¯å¦è·³è¿‡æµ‹è¯•æ•°æ®åˆ›å»º
            
        Returns:
            Dict[str, Any]: æµ‹è¯•ç»“æœæ‘˜è¦
        """
        test_start_time = time.time()
        
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹VisionAI-ClipsMasterçœŸå®ä¸–ç•Œç«¯åˆ°ç«¯æµ‹è¯•")
        self.logger.info("=" * 80)
        
        try:
            # é˜¶æ®µ1ï¼šå‡†å¤‡æµ‹è¯•æ•°æ®
            if not skip_data_creation:
                self.logger.info("é˜¶æ®µ1: å‡†å¤‡æµ‹è¯•æ•°æ®")
                test_dataset = self._prepare_test_data()
                if test_videos is None:
                    test_videos = [video.name for video in test_dataset.videos if os.path.exists(video.name)]
            else:
                test_videos = test_videos or self._get_existing_test_videos()
            
            if not test_videos:
                raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•è§†é¢‘æ–‡ä»¶")
            
            self.logger.info(f"ä½¿ç”¨æµ‹è¯•è§†é¢‘: {len(test_videos)} ä¸ªæ–‡ä»¶")
            
            # é˜¶æ®µ2ï¼šè§†é¢‘ä¸Šä¼ å’Œé¢„å¤„ç†æµ‹è¯•
            self.logger.info("é˜¶æ®µ2: è§†é¢‘ä¸Šä¼ å’Œé¢„å¤„ç†æµ‹è¯•")
            upload_results = self._run_video_upload_tests(test_videos)
            
            # é˜¶æ®µ3ï¼šAIå†…å®¹ç†è§£æµ‹è¯•
            self.logger.info("é˜¶æ®µ3: AIå†…å®¹ç†è§£æµ‹è¯•")
            understanding_results = self._run_content_understanding_tests(test_videos)
            
            # é˜¶æ®µ4ï¼šå®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•
            self.logger.info("é˜¶æ®µ4: å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•")
            pipeline_results = self._run_complete_pipeline_tests(test_videos)
            
            # é˜¶æ®µ5ï¼šæ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•
            self.logger.info("é˜¶æ®µ5: æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•")
            performance_results = self._run_performance_tests(test_videos, test_duration_hours)
            
            # é˜¶æ®µ6ï¼šå‰ªæ˜ å…¼å®¹æ€§æµ‹è¯•
            self.logger.info("é˜¶æ®µ6: å‰ªæ˜ å…¼å®¹æ€§æµ‹è¯•")
            compatibility_results = self._run_compatibility_tests(pipeline_results)
            
            # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ
            all_results = {
                'video_upload': upload_results,
                'content_understanding': understanding_results,
                'complete_pipeline': pipeline_results,
                'performance_monitoring': performance_results,
                'jianying_compatibility': compatibility_results
            }
            
            # é˜¶æ®µ7ï¼šç”Ÿæˆç»¼åˆæŠ¥å‘Š
            self.logger.info("é˜¶æ®µ7: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
            report_files = self._generate_comprehensive_report(all_results, test_start_time)
            
            # è®¡ç®—æ€»ä½“ç»“æœ
            test_summary = self._calculate_test_summary(all_results, test_start_time)
            
            self.logger.info("=" * 80)
            self.logger.info("çœŸå®ä¸–ç•Œç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ")
            self.logger.info(f"æ€»ä½“æˆåŠŸç‡: {test_summary['overall_success_rate']:.2%}")
            self.logger.info(f"æ€»æµ‹è¯•æ—¶é—´: {test_summary['total_duration']:.1f} ç§’")
            self.logger.info(f"ç”ŸæˆæŠ¥å‘Š: {list(report_files.values())}")
            self.logger.info("=" * 80)
            
            return {
                'success': test_summary['overall_success'],
                'summary': test_summary,
                'detailed_results': all_results,
                'report_files': report_files
            }
            
        except Exception as e:
            self.logger.error(f"çœŸå®ä¸–ç•Œç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'summary': {'overall_success_rate': 0.0, 'total_duration': time.time() - test_start_time},
                'detailed_results': {},
                'report_files': {}
            }
    
    def _prepare_test_data(self):
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        try:
            self.logger.info("åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
            dataset = self.dataset_creator.create_complete_test_dataset()
            self.logger.info(f"æµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆ: {dataset.dataset_id}")
            return dataset
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºæµ‹è¯•æ•°æ®é›†å¤±è´¥: {str(e)}")
            return None
    
    def _get_existing_test_videos(self) -> List[str]:
        """è·å–ç°æœ‰æµ‹è¯•è§†é¢‘"""
        videos_dir = Path("tests/real_world_validation/test_data/videos")
        if not videos_dir.exists():
            return []
        
        video_files = []
        for ext in ['*.mp4', '*.mov', '*.avi', '*.mkv']:
            video_files.extend(videos_dir.glob(ext))
        
        return [str(f) for f in video_files if f.stat().st_size > 1024]  # è¿‡æ»¤æ‰å¤ªå°çš„æ–‡ä»¶
    
    def _run_video_upload_tests(self, test_videos: List[str]) -> List[Dict[str, Any]]:
        """è¿è¡Œè§†é¢‘ä¸Šä¼ æµ‹è¯•"""
        results = []
        
        for video_file in test_videos:
            try:
                self.logger.info(f"æµ‹è¯•è§†é¢‘ä¸Šä¼ : {Path(video_file).name}")
                result = self.video_upload_tester.test_real_video_upload(video_file)
                results.append(result.__dict__)
            except Exception as e:
                self.logger.error(f"è§†é¢‘ä¸Šä¼ æµ‹è¯•å¤±è´¥: {video_file}, é”™è¯¯: {str(e)}")
                results.append({
                    'test_name': 'video_upload',
                    'video_file_path': video_file,
                    'success': False,
                    'error_message': str(e)
                })
        
        return results
    
    def _run_content_understanding_tests(self, test_videos: List[str]) -> List[Dict[str, Any]]:
        """è¿è¡Œå†…å®¹ç†è§£æµ‹è¯•"""
        results = []
        
        for video_file in test_videos:
            try:
                self.logger.info(f"æµ‹è¯•å†…å®¹ç†è§£: {Path(video_file).name}")
                result = self.content_understanding_tester.test_real_video_content_understanding(video_file)
                results.append(result.__dict__)
            except Exception as e:
                self.logger.error(f"å†…å®¹ç†è§£æµ‹è¯•å¤±è´¥: {video_file}, é”™è¯¯: {str(e)}")
                results.append({
                    'test_name': 'content_understanding',
                    'video_file_path': video_file,
                    'success': False,
                    'error_message': str(e)
                })
        
        return results
    
    def _run_complete_pipeline_tests(self, test_videos: List[str]) -> List[Dict[str, Any]]:
        """è¿è¡Œå®Œæ•´æµæ°´çº¿æµ‹è¯•"""
        results = []
        
        for video_file in test_videos:
            try:
                self.logger.info(f"æµ‹è¯•å®Œæ•´æµæ°´çº¿: {Path(video_file).name}")
                result = self.pipeline_tester.test_complete_pipeline(video_file)
                results.append(result.__dict__)
            except Exception as e:
                self.logger.error(f"å®Œæ•´æµæ°´çº¿æµ‹è¯•å¤±è´¥: {video_file}, é”™è¯¯: {str(e)}")
                results.append({
                    'test_name': 'complete_pipeline',
                    'video_file_path': video_file,
                    'success': False,
                    'error_message': str(e)
                })
        
        return results
    
    def _run_performance_tests(self, test_videos: List[str], test_duration_hours: float) -> List[Dict[str, Any]]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        results = []
        
        try:
            self.logger.info(f"å¼€å§‹æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•ï¼ŒæŒç»­æ—¶é—´: {test_duration_hours} å°æ—¶")
            
            # å†…å­˜é™åˆ¶æµ‹è¯•
            memory_result = self.memory_tester.test_memory_constrained_processing(
                test_videos, test_duration_hours
            )
            results.append(memory_result.__dict__)
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
            results.append({
                'test_name': 'performance_monitoring',
                'success': False,
                'error_message': str(e)
            })
        
        return results
    
    def _run_compatibility_tests(self, pipeline_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡Œå…¼å®¹æ€§æµ‹è¯•"""
        results = []
        
        # ä»æµæ°´çº¿ç»“æœä¸­æå–ç”Ÿæˆçš„draftinfoæ–‡ä»¶
        draftinfo_files = []
        for result in pipeline_results:
            if result.get('success', False) and 'final_outputs' in result:
                draftinfo_path = result['final_outputs'].get('draftinfo_file', '')
                if draftinfo_path and os.path.exists(draftinfo_path):
                    draftinfo_files.append(draftinfo_path)
        
        if not draftinfo_files:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„å‰ªæ˜ å·¥ç¨‹æ–‡ä»¶è¿›è¡Œå…¼å®¹æ€§æµ‹è¯•")
            return results
        
        for draftinfo_file in draftinfo_files:
            try:
                self.logger.info(f"æµ‹è¯•å‰ªæ˜ å…¼å®¹æ€§: {Path(draftinfo_file).name}")
                result = self.compatibility_tester.test_real_world_import_compatibility(draftinfo_file)
                results.append(result.__dict__)
            except Exception as e:
                self.logger.error(f"å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {draftinfo_file}, é”™è¯¯: {str(e)}")
                results.append({
                    'test_name': 'jianying_compatibility',
                    'draftinfo_file_path': draftinfo_file,
                    'success': False,
                    'error_message': str(e)
                })
        
        return results
    
    def _generate_comprehensive_report(self, all_results: Dict[str, List], test_start_time: float) -> Dict[str, str]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        try:
            # å±•å¹³æ‰€æœ‰æµ‹è¯•ç»“æœ
            flattened_results = []
            for category, results in all_results.items():
                for result in results:
                    result['test_category'] = category
                    flattened_results.append(result)
            
            # ç”Ÿæˆæµ‹è¯•å…ƒæ•°æ®
            test_metadata = {
                'test_start_time': test_start_time,
                'test_end_time': time.time(),
                'test_categories': list(all_results.keys()),
                'total_tests': sum(len(results) for results in all_results.values()),
                'config_file': self.config_path
            }
            
            # ç”ŸæˆæŠ¥å‘Š
            report_files = self.report_generator.generate_comprehensive_report(
                flattened_results, test_metadata
            )
            
            return report_files
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç»¼åˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return {}
    
    def _calculate_test_summary(self, all_results: Dict[str, List], test_start_time: float) -> Dict[str, Any]:
        """è®¡ç®—æµ‹è¯•æ‘˜è¦"""
        total_tests = 0
        successful_tests = 0
        
        category_summaries = {}
        
        for category, results in all_results.items():
            category_total = len(results)
            category_successful = sum(1 for result in results if result.get('success', False))
            
            total_tests += category_total
            successful_tests += category_successful
            
            category_summaries[category] = {
                'total': category_total,
                'successful': category_successful,
                'success_rate': category_successful / category_total if category_total > 0 else 0
            }
        
        overall_success_rate = successful_tests / total_tests if total_tests > 0 else 0
        total_duration = time.time() - test_start_time
        
        return {
            'total_tests': total_tests,
            'successful_tests': successful_tests,
            'failed_tests': total_tests - successful_tests,
            'overall_success_rate': overall_success_rate,
            'overall_success': overall_success_rate >= 0.8,  # 80%æˆåŠŸç‡é˜ˆå€¼
            'total_duration': total_duration,
            'category_summaries': category_summaries
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='VisionAI-ClipsMaster çœŸå®ä¸–ç•Œç«¯åˆ°ç«¯æµ‹è¯•')
    parser.add_argument('--videos', nargs='+', help='æŒ‡å®šæµ‹è¯•è§†é¢‘æ–‡ä»¶')
    parser.add_argument('--duration', type=float, default=2.0, help='æ€§èƒ½æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆå°æ—¶ï¼‰')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--skip-data-creation', action='store_true', help='è·³è¿‡æµ‹è¯•æ•°æ®åˆ›å»º')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--performance-test', action='store_true', help='ä»…è¿è¡Œæ€§èƒ½æµ‹è¯•')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # åˆ›å»ºæµ‹è¯•æ‰§è¡Œå™¨
    runner = RealWorldE2ETestRunner(args.config)
    
    try:
        # è¿è¡Œæµ‹è¯•
        if args.performance_test:
            # ä»…è¿è¡Œæ€§èƒ½æµ‹è¯•
            test_videos = args.videos or runner._get_existing_test_videos()
            if not test_videos:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•è§†é¢‘æ–‡ä»¶")
                sys.exit(1)
            
            print(f"ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼ŒæŒç»­æ—¶é—´: {args.duration} å°æ—¶")
            performance_results = runner._run_performance_tests(test_videos, args.duration)
            
            success_count = sum(1 for result in performance_results if result.get('success', False))
            print(f"\næ€§èƒ½æµ‹è¯•å®Œæˆ:")
            print(f"æ€»æµ‹è¯•æ•°: {len(performance_results)}")
            print(f"æˆåŠŸæ•°: {success_count}")
            print(f"æˆåŠŸç‡: {success_count/len(performance_results):.2%}")
        else:
            # è¿è¡Œå®Œæ•´æµ‹è¯•
            print("ğŸš€ å¼€å§‹VisionAI-ClipsMasterçœŸå®ä¸–ç•Œç«¯åˆ°ç«¯æµ‹è¯•")
            
            result = runner.run_complete_real_world_test(
                test_videos=args.videos,
                test_duration_hours=args.duration,
                skip_data_creation=args.skip_data_creation
            )
            
            # è¾“å‡ºç»“æœæ‘˜è¦
            if result['success']:
                summary = result['summary']
                print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
                print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
                print(f"æˆåŠŸæ•°: {summary['successful_tests']}")
                print(f"å¤±è´¥æ•°: {summary['failed_tests']}")
                print(f"æ€»ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.2%}")
                print(f"æµ‹è¯•æ—¶é•¿: {summary['total_duration']:.1f} ç§’")
                
                # æ˜¾ç¤ºå„ç±»åˆ«ç»“æœ
                print(f"\nğŸ“Š å„ç±»åˆ«æµ‹è¯•ç»“æœ:")
                for category, cat_summary in summary['category_summaries'].items():
                    print(f"  {category}: {cat_summary['successful']}/{cat_summary['total']} ({cat_summary['success_rate']:.2%})")
                
                # æ˜¾ç¤ºæŠ¥å‘Šæ–‡ä»¶
                if result['report_files']:
                    print(f"\nğŸ“„ ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶:")
                    for format_type, file_path in result['report_files'].items():
                        print(f"  {format_type.upper()}: {file_path}")
                
                sys.exit(0 if summary['overall_success'] else 1)
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}")
                sys.exit(1)
    
    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
