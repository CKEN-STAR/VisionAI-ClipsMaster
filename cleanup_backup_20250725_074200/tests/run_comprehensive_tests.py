#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster å…¨é¢æµ‹è¯•æ‰§è¡Œè„šæœ¬

åŠŸèƒ½:
1. æ‰§è¡Œ6å¤§æµ‹è¯•é¢†åŸŸçš„å®Œæ•´æµ‹è¯•å¥—ä»¶
2. ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
3. è®¡ç®—æµ‹è¯•é€šè¿‡ç‡å’Œæ€§èƒ½æŒ‡æ ‡
4. æä¾›ä¿®å¤å»ºè®®å’Œä¼˜åŒ–æ–¹å‘
"""

import os
import sys
import time
import json
import subprocess
from datetime import datetime
from typing import Dict, Any, List
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class ComprehensiveTestRunner:
    """å…¨é¢æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.test_categories = {
            "core_functionality": {
                "name": "æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•",
                "priority": "P0",
                "tests": [
                    "tests/core_functionality/test_screenplay_reconstruction.py",
                    "tests/core_functionality/test_dual_model_system.py", 
                    "tests/core_functionality/test_training_system.py"
                ],
                "target_pass_rate": 0.95
            },
            "performance": {
                "name": "æ€§èƒ½ä¸èµ„æºæµ‹è¯•",
                "priority": "P0", 
                "tests": [
                    "tests/performance/test_resource_management.py"
                ],
                "target_pass_rate": 0.90
            },
            "ui_interface": {
                "name": "UIç•Œé¢æµ‹è¯•",
                "priority": "P1",
                "tests": [
                    "tests/ui_interface/test_ui_components.py"
                ],
                "target_pass_rate": 0.85
            },
            "export_compatibility": {
                "name": "å¯¼å‡ºå…¼å®¹æ€§æµ‹è¯•", 
                "priority": "P0",
                "tests": [
                    "tests/export_compatibility/test_jianying_export.py"
                ],
                "target_pass_rate": 0.95
            },
            "exception_recovery": {
                "name": "å¼‚å¸¸å¤„ç†ä¸æ¢å¤æµ‹è¯•",
                "priority": "P1",
                "tests": [
                    "tests/exception_recovery/test_stability_recovery.py"
                ],
                "target_pass_rate": 0.90
            },
            "security_compliance": {
                "name": "å®‰å…¨ä¸åˆè§„æµ‹è¯•",
                "priority": "P2", 
                "tests": [
                    "tests/security_compliance/test_security_compliance.py"
                ],
                "target_pass_rate": 0.80
            }
        }
        
        self.results = {}
        self.start_time = None
        self.end_time = None

    def run_all_tests(self, skip_slow_tests: bool = False) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹VisionAI-ClipsMasterå…¨é¢æµ‹è¯•")
        print("=" * 80)
        
        self.start_time = time.time()
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        if skip_slow_tests:
            os.environ["SKIP_LONG_TESTS"] = "true"
        
        # æŒ‰ä¼˜å…ˆçº§æ‰§è¡Œæµ‹è¯•
        priority_order = ["P0", "P1", "P2"]
        
        for priority in priority_order:
            print(f"\nğŸ“‹ æ‰§è¡Œ{priority}ä¼˜å…ˆçº§æµ‹è¯•...")
            
            for category_id, category_info in self.test_categories.items():
                if category_info["priority"] == priority:
                    print(f"\nğŸ” {category_info['name']} ({priority})")
                    print("-" * 50)
                    
                    category_result = self._run_category_tests(category_id, category_info)
                    self.results[category_id] = category_result
                    
                    # æ˜¾ç¤ºå³æ—¶ç»“æœ
                    self._print_category_summary(category_id, category_result)
        
        self.end_time = time.time()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_report = self._generate_comprehensive_report()
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_test_report(comprehensive_report)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        self._print_final_summary(comprehensive_report)
        
        return comprehensive_report

    def _run_category_tests(self, category_id: str, category_info: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç±»åˆ«"""
        category_result = {
            "name": category_info["name"],
            "priority": category_info["priority"],
            "target_pass_rate": category_info["target_pass_rate"],
            "test_results": [],
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "skipped_tests": 0,
            "execution_time": 0,
            "pass_rate": 0.0,
            "status": "unknown"
        }
        
        start_time = time.time()
        
        for test_file in category_info["tests"]:
            print(f"  â–¶ï¸ æ‰§è¡Œ: {os.path.basename(test_file)}")
            
            test_result = self._run_single_test(test_file)
            category_result["test_results"].append(test_result)
            
            # ç´¯è®¡ç»Ÿè®¡
            category_result["total_tests"] += test_result["total"]
            category_result["passed_tests"] += test_result["passed"]
            category_result["failed_tests"] += test_result["failed"]
            category_result["skipped_tests"] += test_result["skipped"]
        
        category_result["execution_time"] = time.time() - start_time
        
        # è®¡ç®—é€šè¿‡ç‡
        if category_result["total_tests"] > 0:
            category_result["pass_rate"] = category_result["passed_tests"] / category_result["total_tests"]
        
        # ç¡®å®šçŠ¶æ€
        if category_result["pass_rate"] >= category_result["target_pass_rate"]:
            category_result["status"] = "PASS"
        else:
            category_result["status"] = "FAIL"
        
        return category_result

    def _run_single_test(self, test_file: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ–‡ä»¶"""
        test_result = {
            "file": test_file,
            "total": 0,
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "execution_time": 0,
            "output": "",
            "errors": []
        }
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨pytestè¿è¡Œæµ‹è¯•
            cmd = [
                sys.executable, "-m", "pytest",
                test_file,
                "-v",
                "--tb=short",
                "--json-report",
                "--json-report-file=/tmp/pytest_report.json"
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            test_result["output"] = result.stdout
            test_result["execution_time"] = time.time() - start_time
            
            # è§£æpytestè¾“å‡º
            if result.returncode == 0:
                # æµ‹è¯•å…¨éƒ¨é€šè¿‡
                lines = result.stdout.split('\n')
                for line in lines:
                    if "passed" in line and "failed" in line:
                        # è§£æç»Ÿè®¡è¡Œ
                        parts = line.split()
                        for i, part in enumerate(parts):
                            if part == "passed":
                                test_result["passed"] = int(parts[i-1])
                            elif part == "failed":
                                test_result["failed"] = int(parts[i-1])
                            elif part == "skipped":
                                test_result["skipped"] = int(parts[i-1])
                        break
            else:
                # æœ‰æµ‹è¯•å¤±è´¥æˆ–é”™è¯¯
                test_result["errors"].append(result.stderr)
                
                # å°è¯•è§£æéƒ¨åˆ†ç»“æœ
                lines = result.stdout.split('\n')
                for line in lines:
                    if "FAILED" in line:
                        test_result["failed"] += 1
                    elif "PASSED" in line:
                        test_result["passed"] += 1
                    elif "SKIPPED" in line:
                        test_result["skipped"] += 1
            
            test_result["total"] = test_result["passed"] + test_result["failed"] + test_result["skipped"]
            
        except subprocess.TimeoutExpired:
            test_result["errors"].append("æµ‹è¯•æ‰§è¡Œè¶…æ—¶")
            test_result["execution_time"] = 300
        except Exception as e:
            test_result["errors"].append(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            test_result["execution_time"] = time.time() - start_time
        
        return test_result

    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        total_execution_time = self.end_time - self.start_time
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "skipped_tests": 0,
            "total_categories": len(self.test_categories),
            "passed_categories": 0,
            "failed_categories": 0
        }
        
        for category_result in self.results.values():
            overall_stats["total_tests"] += category_result["total_tests"]
            overall_stats["passed_tests"] += category_result["passed_tests"]
            overall_stats["failed_tests"] += category_result["failed_tests"]
            overall_stats["skipped_tests"] += category_result["skipped_tests"]
            
            if category_result["status"] == "PASS":
                overall_stats["passed_categories"] += 1
            else:
                overall_stats["failed_categories"] += 1
        
        # è®¡ç®—æ€»ä½“é€šè¿‡ç‡
        overall_pass_rate = 0.0
        if overall_stats["total_tests"] > 0:
            overall_pass_rate = overall_stats["passed_tests"] / overall_stats["total_tests"]
        
        # ç¡®å®šæ€»ä½“çŠ¶æ€
        overall_status = "PASS" if overall_pass_rate >= 0.95 else "FAIL"
        
        # ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡
        performance_metrics = self._calculate_performance_metrics()
        
        # ç”Ÿæˆä¿®å¤å»ºè®®
        fix_recommendations = self._generate_fix_recommendations()
        
        comprehensive_report = {
            "test_summary": {
                "start_time": datetime.fromtimestamp(self.start_time).isoformat(),
                "end_time": datetime.fromtimestamp(self.end_time).isoformat(),
                "execution_time": total_execution_time,
                "overall_status": overall_status,
                "overall_pass_rate": overall_pass_rate,
                "target_pass_rate": 0.95
            },
            "statistics": overall_stats,
            "category_results": self.results,
            "performance_metrics": performance_metrics,
            "fix_recommendations": fix_recommendations,
            "production_readiness": {
                "ready": overall_pass_rate >= 0.95,
                "confidence_level": self._calculate_confidence_level(overall_pass_rate),
                "deployment_recommendation": self._get_deployment_recommendation(overall_pass_rate)
            }
        }
        
        return comprehensive_report

    def _calculate_performance_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            "test_execution_efficiency": 0.0,
            "category_balance": 0.0,
            "priority_coverage": {"P0": 0, "P1": 0, "P2": 0},
            "bottleneck_categories": []
        }
        
        # è®¡ç®—æ‰§è¡Œæ•ˆç‡
        total_time = self.end_time - self.start_time
        total_tests = sum(result["total_tests"] for result in self.results.values())
        if total_tests > 0:
            metrics["test_execution_efficiency"] = total_tests / total_time
        
        # è®¡ç®—ä¼˜å…ˆçº§è¦†ç›–
        for category_result in self.results.values():
            priority = category_result["priority"]
            metrics["priority_coverage"][priority] += category_result["total_tests"]
        
        # è¯†åˆ«ç“¶é¢ˆç±»åˆ«
        for category_id, category_result in self.results.items():
            if category_result["execution_time"] > 60:  # è¶…è¿‡1åˆ†é’Ÿ
                metrics["bottleneck_categories"].append({
                    "category": category_id,
                    "execution_time": category_result["execution_time"],
                    "test_count": category_result["total_tests"]
                })
        
        return metrics

    def _generate_fix_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        for category_id, category_result in self.results.items():
            if category_result["status"] == "FAIL":
                recommendation = {
                    "category": category_id,
                    "priority": category_result["priority"],
                    "current_pass_rate": category_result["pass_rate"],
                    "target_pass_rate": category_result["target_pass_rate"],
                    "gap": category_result["target_pass_rate"] - category_result["pass_rate"],
                    "suggested_actions": []
                }
                
                # æ ¹æ®ç±»åˆ«ç”Ÿæˆå…·ä½“å»ºè®®
                if category_id == "core_functionality":
                    recommendation["suggested_actions"] = [
                        "æ£€æŸ¥AIå‰§æœ¬é‡æ„é€»è¾‘å®ç°",
                        "ä¼˜åŒ–åŒè¯­è¨€æ¨¡å‹åˆ‡æ¢æ€§èƒ½",
                        "éªŒè¯è®­ç»ƒç³»ç»Ÿæ•°æ®æµç¨‹"
                    ]
                elif category_id == "performance":
                    recommendation["suggested_actions"] = [
                        "ä¼˜åŒ–å†…å­˜ä½¿ç”¨ç­–ç•¥",
                        "æ”¹è¿›å¯åŠ¨æ—¶é—´æ€§èƒ½",
                        "å¢å¼ºèµ„æºç®¡ç†æœºåˆ¶"
                    ]
                elif category_id == "ui_interface":
                    recommendation["suggested_actions"] = [
                        "ä¿®å¤UIç»„ä»¶å…¼å®¹æ€§é—®é¢˜",
                        "ä¼˜åŒ–ç•Œé¢å“åº”æ—¶é—´",
                        "å®Œå–„ä¸»é¢˜åˆ‡æ¢åŠŸèƒ½"
                    ]
                elif category_id == "export_compatibility":
                    recommendation["suggested_actions"] = [
                        "ä¿®å¤å‰ªæ˜ å¯¼å‡ºæ ¼å¼é—®é¢˜",
                        "æé«˜æ—¶é—´è½´ç²¾åº¦",
                        "å®Œå–„å¤šæ ¼å¼æ”¯æŒ"
                    ]
                elif category_id == "exception_recovery":
                    recommendation["suggested_actions"] = [
                        "å¢å¼ºå¼‚å¸¸å¤„ç†æœºåˆ¶",
                        "æ”¹è¿›æ–­ç‚¹ç»­å‰ªåŠŸèƒ½",
                        "æé«˜æ¢å¤æˆåŠŸç‡"
                    ]
                elif category_id == "security_compliance":
                    recommendation["suggested_actions"] = [
                        "å®Œå–„æ°´å°æ£€æµ‹ç®—æ³•",
                        "åŠ å¼ºæ•°æ®å®‰å…¨ä¿æŠ¤",
                        "æ”¹è¿›åˆè§„æ€§æ£€æŸ¥"
                    ]
                
                recommendations.append(recommendation)
        
        return recommendations

    def _calculate_confidence_level(self, pass_rate: float) -> str:
        """è®¡ç®—ç½®ä¿¡åº¦ç­‰çº§"""
        if pass_rate >= 0.98:
            return "Very High"
        elif pass_rate >= 0.95:
            return "High"
        elif pass_rate >= 0.90:
            return "Medium"
        elif pass_rate >= 0.80:
            return "Low"
        else:
            return "Very Low"

    def _get_deployment_recommendation(self, pass_rate: float) -> str:
        """è·å–éƒ¨ç½²å»ºè®®"""
        if pass_rate >= 0.98:
            return "å¼ºçƒˆæ¨èç«‹å³éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ"
        elif pass_rate >= 0.95:
            return "æ¨èéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ç›‘æ§å…³é”®æŒ‡æ ‡"
        elif pass_rate >= 0.90:
            return "å»ºè®®å…ˆéƒ¨ç½²åˆ°é¢„ç”Ÿäº§ç¯å¢ƒè¿›è¡ŒéªŒè¯"
        elif pass_rate >= 0.80:
            return "éœ€è¦ä¿®å¤å…³é”®é—®é¢˜åå†è€ƒè™‘éƒ¨ç½²"
        else:
            return "ä¸å»ºè®®éƒ¨ç½²ï¼Œéœ€è¦å¤§å¹…æ”¹è¿›"

    def _print_category_summary(self, category_id: str, result: Dict[str, Any]):
        """æ‰“å°ç±»åˆ«æµ‹è¯•æ‘˜è¦"""
        status_emoji = "âœ…" if result["status"] == "PASS" else "âŒ"
        print(f"    {status_emoji} {result['name']}: {result['passed_tests']}/{result['total_tests']} "
              f"({result['pass_rate']:.1%}) - {result['execution_time']:.1f}s")

    def _print_final_summary(self, report: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆæµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ¯ VisionAI-ClipsMaster å…¨é¢æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        summary = report["test_summary"]
        stats = report["statistics"]
        
        print(f"ğŸ“Š æ€»ä½“ç»“æœ: {summary['overall_status']}")
        print(f"ğŸ“ˆ é€šè¿‡ç‡: {summary['overall_pass_rate']:.1%} (ç›®æ ‡: {summary['target_pass_rate']:.0%})")
        print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {summary['execution_time']:.1f}ç§’")
        print(f"ğŸ§ª æµ‹è¯•ç»Ÿè®¡: {stats['passed_tests']}/{stats['total_tests']} é€šè¿‡")
        
        print(f"\nğŸ“‹ åˆ†ç±»ç»“æœ:")
        for category_id, result in report["category_results"].items():
            status_emoji = "âœ…" if result["status"] == "PASS" else "âŒ"
            print(f"  {status_emoji} {result['name']} ({result['priority']}): {result['pass_rate']:.1%}")
        
        # ç”Ÿäº§å°±ç»ªæ€§è¯„ä¼°
        readiness = report["production_readiness"]
        ready_emoji = "ğŸš€" if readiness["ready"] else "âš ï¸"
        print(f"\n{ready_emoji} ç”Ÿäº§å°±ç»ªæ€§: {'æ˜¯' if readiness['ready'] else 'å¦'}")
        print(f"ğŸ¯ ç½®ä¿¡åº¦: {readiness['confidence_level']}")
        print(f"ğŸ’¡ éƒ¨ç½²å»ºè®®: {readiness['deployment_recommendation']}")
        
        # ä¿®å¤å»ºè®®
        if report["fix_recommendations"]:
            print(f"\nğŸ”§ ä¿®å¤å»ºè®®:")
            for rec in report["fix_recommendations"]:
                print(f"  â€¢ {rec['category']} ({rec['priority']}): é€šè¿‡ç‡å·®è· {rec['gap']:.1%}")

    def _save_test_report(self, report: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONæŠ¥å‘Š
        json_file = f"tests/reports/comprehensive_test_report_{timestamp}.json"
        os.makedirs(os.path.dirname(json_file), exist_ok=True)
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {json_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="VisionAI-ClipsMaster å…¨é¢æµ‹è¯•æ‰§è¡Œå™¨")
    parser.add_argument("--skip-slow", action="store_true", help="è·³è¿‡é•¿æ—¶é—´æµ‹è¯•")
    parser.add_argument("--category", help="åªè¿è¡ŒæŒ‡å®šç±»åˆ«çš„æµ‹è¯•")
    
    args = parser.parse_args()
    
    runner = ComprehensiveTestRunner()
    
    if args.category:
        # è¿è¡ŒæŒ‡å®šç±»åˆ«
        if args.category in runner.test_categories:
            category_info = runner.test_categories[args.category]
            result = runner._run_category_tests(args.category, category_info)
            runner._print_category_summary(args.category, result)
        else:
            print(f"âŒ æœªçŸ¥æµ‹è¯•ç±»åˆ«: {args.category}")
            print(f"å¯ç”¨ç±»åˆ«: {', '.join(runner.test_categories.keys())}")
            return 1
    else:
        # è¿è¡Œå…¨éƒ¨æµ‹è¯•
        report = runner.run_all_tests(skip_slow_tests=args.skip_slow)
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        if report["production_readiness"]["ready"]:
            return 0
        else:
            return 1


if __name__ == "__main__":
    sys.exit(main())
