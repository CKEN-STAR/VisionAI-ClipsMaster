#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸€é”®è¿è¡Œå®Œæ•´è®­ç»ƒéªŒè¯æµ‹è¯•
æŒ‰ç…§ç”¨æˆ·è¦æ±‚çš„5ä¸ªæ ¸å¿ƒæµ‹è¯•æ¨¡å—é¡ºåºæ‰§è¡Œæ‰€æœ‰æµ‹è¯•
"""

import os
import sys
import time
import logging
import subprocess
from datetime import datetime
from pathlib import Path

class CompleteTrainingValidationRunner:
    """å®Œæ•´è®­ç»ƒéªŒè¯æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¿è¡Œå™¨"""
        self.setup_logging()
        self.start_time = time.time()
        self.test_results = {}
        
        self.logger.info("ğŸš€ å®Œæ•´è®­ç»ƒéªŒè¯æµ‹è¯•è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / f"complete_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger("CompleteValidationRunner")
    
    def run_all_tests(self) -> dict:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ¯ VisionAI-ClipsMaster å®Œæ•´è®­ç»ƒéªŒè¯æµ‹è¯•")
        print("=" * 80)
        print("æŒ‰ç…§ç”¨æˆ·è¦æ±‚æ‰§è¡Œ5ä¸ªæ ¸å¿ƒæµ‹è¯•æ¨¡å—ï¼š")
        print("1. è®­ç»ƒæ¨¡å—åŠŸèƒ½éªŒè¯")
        print("2. å­¦ä¹ æ•ˆæœé‡åŒ–æµ‹è¯•")
        print("3. GPUåŠ é€Ÿæ€§èƒ½æµ‹è¯•")
        print("4. è®­ç»ƒç¨³å®šæ€§éªŒè¯")
        print("5. è¾“å‡ºéªŒè¯")
        print("=" * 80)
        
        try:
            # 1. è¿è¡Œç»¼åˆè®­ç»ƒéªŒè¯æµ‹è¯•
            self.logger.info("ğŸ“‹ 1/4 è¿è¡Œç»¼åˆè®­ç»ƒéªŒè¯æµ‹è¯•...")
            print("\nğŸ”§ æ­¥éª¤ 1/4: è¿è¡Œç»¼åˆè®­ç»ƒéªŒè¯æµ‹è¯•")
            result1 = self._run_script("comprehensive_training_validation_system.py")
            self.test_results["comprehensive_validation"] = result1
            
            # 2. è¿è¡Œè®­ç»ƒæ•ˆæœè¯„ä¼°
            self.logger.info("ğŸ“Š 2/4 è¿è¡Œè®­ç»ƒæ•ˆæœè¯„ä¼°...")
            print("\nğŸ“Š æ­¥éª¤ 2/4: è¿è¡Œè®­ç»ƒæ•ˆæœè¯„ä¼°")
            result2 = self._run_script("training_effectiveness_evaluator.py")
            self.test_results["effectiveness_evaluation"] = result2
            
            # 3. è¿è¡ŒGPUæ€§èƒ½æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
            self.logger.info("ğŸ® 3/4 è¿è¡ŒGPUæ€§èƒ½æµ‹è¯•...")
            print("\nğŸ® æ­¥éª¤ 3/4: è¿è¡ŒGPUæ€§èƒ½æµ‹è¯•")
            result3 = self._run_script("gpu_training_performance_test.py")
            self.test_results["gpu_performance"] = result3
            
            # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            self.logger.info("ğŸ“‹ 4/4 ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š...")
            print("\nğŸ“‹ æ­¥éª¤ 4/4: ç”Ÿæˆæœ€ç»ˆç»¼åˆæŠ¥å‘Š")
            result4 = self._run_script("final_training_validation_report.py")
            self.test_results["final_report"] = result4
            
            # è®¡ç®—æ€»è€—æ—¶
            total_duration = time.time() - self.start_time
            
            # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
            self._display_test_summary(total_duration)
            
            return {
                "success": True,
                "total_duration": total_duration,
                "test_results": self.test_results,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            error_msg = f"å®Œæ•´æµ‹è¯•è¿è¡Œå¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            print(f"\nâŒ {error_msg}")
            return {"success": False, "error": error_msg}
    
    def _run_script(self, script_name: str) -> dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•è„šæœ¬"""
        script_path = Path(script_name)
        
        if not script_path.exists():
            error_msg = f"æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨: {script_name}"
            self.logger.error(error_msg)
            return {"success": False, "error": error_msg}
        
        try:
            self.logger.info(f"æ‰§è¡Œè„šæœ¬: {script_name}")
            start_time = time.time()
            
            # è¿è¡Œè„šæœ¬
            result = subprocess.run(
                [sys.executable, script_name],
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            duration = time.time() - start_time
            
            if result.returncode == 0:
                self.logger.info(f"âœ… {script_name} æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {duration:.2f}ç§’")
                return {
                    "success": True,
                    "duration": duration,
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }
            else:
                error_msg = f"{script_name} æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}"
                self.logger.error(error_msg)
                self.logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                return {
                    "success": False,
                    "error": error_msg,
                    "returncode": result.returncode,
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }
                
        except subprocess.TimeoutExpired:
            error_msg = f"{script_name} æ‰§è¡Œè¶…æ—¶"
            self.logger.error(error_msg)
            return {"success": False, "error": error_msg}
        except Exception as e:
            error_msg = f"{script_name} æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            self.logger.error(error_msg)
            return {"success": False, "error": error_msg}
    
    def _display_test_summary(self, total_duration: float):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“Š å®Œæ•´è®­ç»ƒéªŒè¯æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 80)
        
        # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥çš„æµ‹è¯•
        successful_tests = sum(1 for result in self.test_results.values() if result.get("success", False))
        total_tests = len(self.test_results)
        
        print(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        print(f"âœ… æˆåŠŸæµ‹è¯•: {successful_tests}/{total_tests}")
        print(f"âŒ å¤±è´¥æµ‹è¯•: {total_tests - successful_tests}/{total_tests}")
        
        # æ˜¾ç¤ºå„ä¸ªæµ‹è¯•çš„çŠ¶æ€
        print("\nğŸ“‹ è¯¦ç»†æµ‹è¯•çŠ¶æ€:")
        test_names = {
            "comprehensive_validation": "ç»¼åˆè®­ç»ƒéªŒè¯æµ‹è¯•",
            "effectiveness_evaluation": "è®­ç»ƒæ•ˆæœè¯„ä¼°",
            "gpu_performance": "GPUæ€§èƒ½æµ‹è¯•",
            "final_report": "æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ"
        }
        
        for test_key, test_name in test_names.items():
            if test_key in self.test_results:
                result = self.test_results[test_key]
                status = "âœ… æˆåŠŸ" if result.get("success", False) else "âŒ å¤±è´¥"
                duration = result.get("duration", 0)
                print(f"  {test_name}: {status} ({duration:.2f}ç§’)")
                
                if not result.get("success", False) and "error" in result:
                    print(f"    é”™è¯¯: {result['error']}")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æŠ¥å‘Šä½ç½®
        print("\nğŸ“ ç”Ÿæˆçš„æµ‹è¯•æŠ¥å‘Š:")
        report_dirs = [
            "test_output/training_validation",
            "test_output/training_effectiveness", 
            "test_output/gpu_performance",
            "test_output/final_validation_report"
        ]
        
        for report_dir in report_dirs:
            if Path(report_dir).exists():
                print(f"  ğŸ“Š {report_dir}")
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        print("\nğŸ¯ å…³é”®æµ‹è¯•æŒ‡æ ‡:")
        
        # å°è¯•ä»æœ€ç»ˆæŠ¥å‘Šä¸­æå–å…³é”®æŒ‡æ ‡
        final_report_dir = Path("test_output/final_validation_report")
        if final_report_dir.exists():
            try:
                import json
                latest_json = max(final_report_dir.glob("final_training_validation_report_*.json"), 
                                key=lambda f: f.stat().st_mtime)
                
                with open(latest_json, 'r', encoding='utf-8') as f:
                    report_data = json.load(f)
                
                executive_summary = report_data.get("executive_summary", {})
                print(f"  ğŸ† æ€»ä½“çŠ¶æ€: {executive_summary.get('overall_status', 'UNKNOWN')}")
                
                performance_highlights = executive_summary.get("performance_highlights", {})
                print(f"  ğŸ“ˆ æµ‹è¯•è¦†ç›–ç‡: {performance_highlights.get('test_coverage', 'N/A')}")
                print(f"  ğŸ“Š æ”¹è¿›ç‡: {performance_highlights.get('improvement_rate', 'N/A')}")
                print(f"  ğŸ”’ ç¨³å®šæ€§: {performance_highlights.get('stability_status', 'N/A')}")
                
                achievements = executive_summary.get("key_achievements", [])
                if achievements:
                    print(f"  ğŸ‰ å…³é”®æˆå°±: {len(achievements)}é¡¹")
                
            except Exception as e:
                print(f"  âš ï¸ æ— æ³•è¯»å–æœ€ç»ˆæŠ¥å‘Š: {str(e)}")
        
        print("\n" + "=" * 80)
        
        if successful_tests == total_tests:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•æˆåŠŸå®Œæˆï¼VisionAI-ClipsMasterè®­ç»ƒéªŒè¯æµ‹è¯•é€šè¿‡ï¼")
        else:
            print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯ã€‚")
        
        print("=" * 80)
    
    def check_prerequisites(self) -> bool:
        """æ£€æŸ¥è¿è¡Œå‰ææ¡ä»¶"""
        self.logger.info("ğŸ” æ£€æŸ¥è¿è¡Œå‰ææ¡ä»¶...")
        
        # æ£€æŸ¥å¿…è¦çš„è„šæœ¬æ–‡ä»¶
        required_scripts = [
            "comprehensive_training_validation_system.py",
            "training_effectiveness_evaluator.py", 
            "gpu_training_performance_test.py",
            "final_training_validation_report.py"
        ]
        
        missing_scripts = []
        for script in required_scripts:
            if not Path(script).exists():
                missing_scripts.append(script)
        
        if missing_scripts:
            self.logger.error(f"ç¼ºå°‘å¿…è¦çš„æµ‹è¯•è„šæœ¬: {missing_scripts}")
            return False
        
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        if sys.version_info < (3, 7):
            self.logger.error("éœ€è¦Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
            return False
        
        # æ£€æŸ¥å¿…è¦çš„ç›®å½•
        required_dirs = ["src", "data", "configs"]
        for dir_name in required_dirs:
            if not Path(dir_name).exists():
                self.logger.warning(f"å»ºè®®çš„ç›®å½•ä¸å­˜åœ¨: {dir_name}")
        
        self.logger.info("âœ… å‰ææ¡ä»¶æ£€æŸ¥é€šè¿‡")
        return True


def main():
    """ä¸»å‡½æ•°"""
    runner = CompleteTrainingValidationRunner()
    
    try:
        # æ£€æŸ¥å‰ææ¡ä»¶
        if not runner.check_prerequisites():
            print("âŒ å‰ææ¡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿æ‰€æœ‰å¿…è¦æ–‡ä»¶å­˜åœ¨")
            return
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        results = runner.run_all_tests()
        
        if results.get("success", False):
            print(f"\nğŸŠ å®Œæ•´è®­ç»ƒéªŒè¯æµ‹è¯•æˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹ test_output/ ç›®å½•")
        else:
            print(f"\nğŸ’¥ æµ‹è¯•è¿è¡Œå¤±è´¥: {results.get('error', 'Unknown error')}")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nğŸ’¥ è¿è¡Œå™¨å¼‚å¸¸: {str(e)}")
        runner.logger.error(f"è¿è¡Œå™¨å¼‚å¸¸: {str(e)}")


if __name__ == "__main__":
    main()
