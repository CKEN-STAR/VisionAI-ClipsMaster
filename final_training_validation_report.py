#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Šç”Ÿæˆå™¨
æ±‡æ€»æ‰€æœ‰æµ‹è¯•ç»“æœï¼Œç”Ÿæˆå®Œæ•´çš„éªŒè¯æŠ¥å‘Š
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

class FinalTrainingValidationReport:
    """æœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨"""
        self.setup_logging()
        self.output_dir = Path("test_output/final_validation_report")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info("ğŸ“‹ æœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger("FinalValidationReport")
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘Š"""
        self.logger.info("ğŸ“Š å¼€å§‹ç”Ÿæˆæœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Š")
        start_time = time.time()
        
        # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ
        test_results = self._collect_all_test_results()
        
        # ç”Ÿæˆç»¼åˆåˆ†æ
        comprehensive_analysis = self._generate_comprehensive_analysis(test_results)
        
        # åˆ›å»ºæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "report_metadata": {
                "generation_time": datetime.now().isoformat(),
                "report_version": "1.0.0",
                "system_info": self._get_system_info(),
                "generation_duration": time.time() - start_time
            },
            "executive_summary": self._create_executive_summary(comprehensive_analysis),
            "detailed_results": test_results,
            "comprehensive_analysis": comprehensive_analysis,
            "recommendations": self._generate_recommendations(comprehensive_analysis),
            "conclusion": self._generate_conclusion(comprehensive_analysis)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_final_report(final_report)
        
        # ç”Ÿæˆå¯è§†åŒ–ä»ªè¡¨æ¿
        self._generate_dashboard(final_report)
        
        self.logger.info(f"âœ… æœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {final_report['report_metadata']['generation_duration']:.2f}ç§’")
        
        return final_report
    
    def _collect_all_test_results(self) -> Dict[str, Any]:
        """æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ"""
        self.logger.info("ğŸ” æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ...")
        
        results = {
            "training_validation": None,
            "effectiveness_evaluation": None,
            "gpu_performance": None,
            "collection_summary": {
                "total_tests_found": 0,
                "successful_collections": 0,
                "failed_collections": []
            }
        }
        
        # æ”¶é›†è®­ç»ƒéªŒè¯ç»“æœ
        training_validation_dir = Path("test_output/training_validation")
        if training_validation_dir.exists():
            latest_file = self._find_latest_json_file(training_validation_dir, "training_validation_detailed_")
            if latest_file:
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        results["training_validation"] = json.load(f)
                    results["collection_summary"]["successful_collections"] += 1
                    self.logger.info(f"âœ… å·²æ”¶é›†è®­ç»ƒéªŒè¯ç»“æœ: {latest_file.name}")
                except Exception as e:
                    results["collection_summary"]["failed_collections"].append(f"training_validation: {str(e)}")
            results["collection_summary"]["total_tests_found"] += 1
        
        # æ”¶é›†æ•ˆæœè¯„ä¼°ç»“æœ
        effectiveness_dir = Path("test_output/training_effectiveness")
        if effectiveness_dir.exists():
            latest_file = self._find_latest_json_file(effectiveness_dir, "effectiveness_report_")
            if latest_file:
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        results["effectiveness_evaluation"] = json.load(f)
                    results["collection_summary"]["successful_collections"] += 1
                    self.logger.info(f"âœ… å·²æ”¶é›†æ•ˆæœè¯„ä¼°ç»“æœ: {latest_file.name}")
                except Exception as e:
                    results["collection_summary"]["failed_collections"].append(f"effectiveness_evaluation: {str(e)}")
            results["collection_summary"]["total_tests_found"] += 1
        
        # æ”¶é›†GPUæ€§èƒ½ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        gpu_performance_dir = Path("test_output/gpu_performance")
        if gpu_performance_dir.exists():
            latest_file = self._find_latest_json_file(gpu_performance_dir, "gpu_performance_report_")
            if latest_file:
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        results["gpu_performance"] = json.load(f)
                    results["collection_summary"]["successful_collections"] += 1
                    self.logger.info(f"âœ… å·²æ”¶é›†GPUæ€§èƒ½ç»“æœ: {latest_file.name}")
                except Exception as e:
                    results["collection_summary"]["failed_collections"].append(f"gpu_performance: {str(e)}")
            results["collection_summary"]["total_tests_found"] += 1
        
        return results
    
    def _find_latest_json_file(self, directory: Path, prefix: str) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€æ–°çš„JSONæ–‡ä»¶"""
        json_files = list(directory.glob(f"{prefix}*.json"))
        if json_files:
            return max(json_files, key=lambda f: f.stat().st_mtime)
        return None
    
    def _generate_comprehensive_analysis(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†æ"""
        self.logger.info("ğŸ“ˆ ç”Ÿæˆç»¼åˆåˆ†æ...")
        
        analysis = {
            "overall_test_coverage": self._analyze_test_coverage(test_results),
            "training_module_performance": self._analyze_training_modules(test_results),
            "learning_effectiveness": self._analyze_learning_effectiveness(test_results),
            "system_performance": self._analyze_system_performance(test_results),
            "stability_assessment": self._analyze_stability(test_results),
            "quality_metrics": self._analyze_quality_metrics(test_results)
        }
        
        return analysis
    
    def _analyze_test_coverage(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•è¦†ç›–ç‡"""
        collection_summary = test_results.get("collection_summary", {})
        
        total_tests = collection_summary.get("total_tests_found", 0)
        successful_tests = collection_summary.get("successful_collections", 0)
        
        coverage_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        return {
            "total_test_modules": total_tests,
            "successful_collections": successful_tests,
            "coverage_percentage": coverage_rate,
            "coverage_status": "EXCELLENT" if coverage_rate >= 90 else "GOOD" if coverage_rate >= 70 else "NEEDS_IMPROVEMENT",
            "missing_tests": collection_summary.get("failed_collections", [])
        }
    
    def _analyze_training_modules(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè®­ç»ƒæ¨¡å—æ€§èƒ½"""
        training_data = test_results.get("training_validation", {})
        training_modules = training_data.get("training_modules", {})
        
        module_status = {}
        overall_success = True
        
        for module_name, module_data in training_modules.items():
            if isinstance(module_data, dict):
                success = module_data.get("success", False)
                module_status[module_name] = {
                    "status": "PASS" if success else "FAIL",
                    "details": module_data.get("details", {}),
                    "error": module_data.get("error")
                }
                if not success:
                    overall_success = False
        
        return {
            "overall_status": "PASS" if overall_success else "FAIL",
            "module_details": module_status,
            "total_modules_tested": len(module_status),
            "successful_modules": sum(1 for status in module_status.values() if status["status"] == "PASS")
        }
    
    def _analyze_learning_effectiveness(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå­¦ä¹ æ•ˆæœ"""
        effectiveness_data = test_results.get("effectiveness_evaluation", {})
        
        if not effectiveness_data:
            return {"status": "NOT_TESTED", "reason": "æ•ˆæœè¯„ä¼°æ•°æ®ä¸å¯ç”¨"}
        
        improvement_metrics = effectiveness_data.get("improvement_metrics", {})
        quality_assessments = effectiveness_data.get("quality_assessments", {})
        detailed_analysis = effectiveness_data.get("detailed_analysis", {})
        
        # è®¡ç®—å¹³å‡æ”¹è¿›ç‡
        improvement_rates = []
        for lang_data in improvement_metrics.values():
            if isinstance(lang_data, dict) and "improvement_rate" in lang_data:
                improvement_rates.append(lang_data["improvement_rate"])
        
        avg_improvement = sum(improvement_rates) / len(improvement_rates) if improvement_rates else 0
        
        return {
            "average_improvement_rate": avg_improvement,
            "overall_success_rate": detailed_analysis.get("overall_success_rate", 0),
            "best_performing_language": detailed_analysis.get("best_performing_language"),
            "quality_thresholds_met": self._check_quality_thresholds(quality_assessments),
            "effectiveness_status": "EXCELLENT" if avg_improvement > 5.0 else "GOOD" if avg_improvement > 1.0 else "NEEDS_IMPROVEMENT"
        }
    
    def _analyze_system_performance(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç³»ç»Ÿæ€§èƒ½"""
        training_data = test_results.get("training_validation", {})
        gpu_data = test_results.get("gpu_performance", {})
        
        performance_analysis = {
            "memory_usage": self._analyze_memory_usage(training_data),
            "gpu_performance": self._analyze_gpu_performance(gpu_data),
            "training_speed": self._analyze_training_speed(training_data),
            "resource_efficiency": "GOOD"  # é»˜è®¤è¯„çº§
        }
        
        return performance_analysis
    
    def _analyze_stability(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç¨³å®šæ€§"""
        training_data = test_results.get("training_validation", {})
        stability_data = training_data.get("stability", {}) if training_data else {}
        
        return {
            "long_term_training": stability_data.get("long_term_training", {}).get("success", False),
            "checkpoint_recovery": stability_data.get("checkpoint_recovery", {}).get("success", False),
            "language_switching": stability_data.get("language_switching", {}).get("success", False),
            "memory_leak_detected": stability_data.get("memory_monitoring", {}).get("leak_detected", False),
            "overall_stability": "STABLE" if all([
                stability_data.get("long_term_training", {}).get("success", False),
                stability_data.get("checkpoint_recovery", {}).get("success", False),
                not stability_data.get("memory_monitoring", {}).get("leak_detected", True)
            ]) else "UNSTABLE"
        }
    
    def _analyze_quality_metrics(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè´¨é‡æŒ‡æ ‡"""
        effectiveness_data = test_results.get("effectiveness_evaluation", {})
        quality_assessments = effectiveness_data.get("quality_assessments", {}) if effectiveness_data else {}
        
        metrics = {
            "narrative_coherence": self._extract_quality_metric(quality_assessments, "narrative_coherence"),
            "timeline_alignment": self._extract_quality_metric(quality_assessments, "timeline_alignment"),
            "viral_features": self._extract_quality_metric(quality_assessments, "viral_features")
        }
        
        # è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†
        valid_scores = [score for score in metrics.values() if score is not None]
        overall_quality = sum(valid_scores) / len(valid_scores) if valid_scores else 0
        
        return {
            "individual_metrics": metrics,
            "overall_quality_score": overall_quality,
            "quality_status": "EXCELLENT" if overall_quality >= 0.8 else "GOOD" if overall_quality >= 0.6 else "NEEDS_IMPROVEMENT"
        }
    
    def _check_quality_thresholds(self, quality_assessments: Dict[str, Any]) -> Dict[str, bool]:
        """æ£€æŸ¥è´¨é‡é˜ˆå€¼"""
        thresholds_met = {}
        
        for assessment_type, assessment_data in quality_assessments.items():
            if isinstance(assessment_data, dict):
                for language, lang_data in assessment_data.items():
                    if isinstance(lang_data, dict):
                        threshold_met = lang_data.get("threshold_met", False)
                        thresholds_met[f"{assessment_type}_{language}"] = threshold_met
        
        return thresholds_met
    
    def _extract_quality_metric(self, quality_assessments: Dict[str, Any], metric_name: str) -> Optional[float]:
        """æå–è´¨é‡æŒ‡æ ‡"""
        metric_data = quality_assessments.get(metric_name, {})
        if not metric_data:
            return None
        
        scores = []
        for lang_data in metric_data.values():
            if isinstance(lang_data, dict):
                if "average_score" in lang_data:
                    scores.append(lang_data["average_score"])
                elif "average_feature_score" in lang_data:
                    scores.append(lang_data["average_feature_score"])
                elif "average_error_seconds" in lang_data:
                    # å¯¹äºæ—¶é—´è½´å¯¹é½ï¼Œé”™è¯¯è¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸º0-1åˆ†æ•°
                    error = lang_data["average_error_seconds"]
                    scores.append(max(0, 1 - error))
        
        return sum(scores) / len(scores) if scores else None
    
    def _analyze_memory_usage(self, training_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå†…å­˜ä½¿ç”¨"""
        if not training_data:
            return {"status": "NO_DATA"}
        
        stability_data = training_data.get("stability", {})
        memory_monitoring = stability_data.get("memory_monitoring", {})
        
        return {
            "peak_usage_gb": memory_monitoring.get("peak_usage", 0),
            "average_usage_gb": memory_monitoring.get("average_usage", 0),
            "leak_detected": memory_monitoring.get("leak_detected", False),
            "usage_status": "OPTIMAL" if memory_monitoring.get("peak_usage", 0) < 4.0 else "HIGH"
        }
    
    def _analyze_gpu_performance(self, gpu_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æGPUæ€§èƒ½"""
        if not gpu_data:
            return {"status": "NOT_TESTED", "gpu_available": False}
        
        return {
            "gpu_available": gpu_data.get("gpu_info", {}).get("device_count", 0) > 0,
            "performance_status": "GOOD" if gpu_data.get("success", False) else "POOR"
        }
    
    def _analyze_training_speed(self, training_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè®­ç»ƒé€Ÿåº¦"""
        if not training_data:
            return {"status": "NO_DATA"}
        
        test_summary = training_data.get("test_summary", {})
        total_duration = test_summary.get("total_duration", 0)
        
        return {
            "total_test_duration": total_duration,
            "speed_status": "FAST" if total_duration < 30 else "MODERATE" if total_duration < 60 else "SLOW"
        }
    
    def _create_executive_summary(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºæ‰§è¡Œæ‘˜è¦"""
        return {
            "overall_status": self._determine_overall_status(analysis),
            "key_achievements": self._identify_key_achievements(analysis),
            "critical_issues": self._identify_critical_issues(analysis),
            "performance_highlights": self._extract_performance_highlights(analysis)
        }
    
    def _determine_overall_status(self, analysis: Dict[str, Any]) -> str:
        """ç¡®å®šæ€»ä½“çŠ¶æ€"""
        # åŸºäºå„é¡¹åˆ†æç»“æœç¡®å®šæ€»ä½“çŠ¶æ€
        training_status = analysis.get("training_module_performance", {}).get("overall_status", "FAIL")
        effectiveness_status = analysis.get("learning_effectiveness", {}).get("effectiveness_status", "NEEDS_IMPROVEMENT")
        stability_status = analysis.get("stability_assessment", {}).get("overall_stability", "UNSTABLE")
        
        if all(status in ["PASS", "EXCELLENT", "STABLE"] for status in [training_status, effectiveness_status, stability_status]):
            return "EXCELLENT"
        elif training_status == "PASS" and effectiveness_status in ["GOOD", "EXCELLENT"]:
            return "GOOD"
        else:
            return "NEEDS_IMPROVEMENT"
    
    def _identify_key_achievements(self, analysis: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«å…³é”®æˆå°±"""
        achievements = []
        
        # æ£€æŸ¥è®­ç»ƒæ¨¡å—æˆåŠŸç‡
        training_perf = analysis.get("training_module_performance", {})
        if training_perf.get("overall_status") == "PASS":
            achievements.append("æ‰€æœ‰è®­ç»ƒæ¨¡å—åŠŸèƒ½éªŒè¯é€šè¿‡")
        
        # æ£€æŸ¥å­¦ä¹ æ•ˆæœ
        learning_eff = analysis.get("learning_effectiveness", {})
        if learning_eff.get("average_improvement_rate", 0) > 5.0:
            achievements.append("è®­ç»ƒæ•ˆæœæ˜¾è‘—ï¼Œå¹³å‡æ”¹è¿›ç‡è¶…è¿‡500%")
        
        # æ£€æŸ¥ç¨³å®šæ€§
        stability = analysis.get("stability_assessment", {})
        if stability.get("overall_stability") == "STABLE":
            achievements.append("ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•å…¨éƒ¨é€šè¿‡")
        
        return achievements
    
    def _identify_critical_issues(self, analysis: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«å…³é”®é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡
        coverage = analysis.get("overall_test_coverage", {})
        if coverage.get("coverage_percentage", 0) < 90:
            issues.append("æµ‹è¯•è¦†ç›–ç‡ä¸è¶³90%")
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory = analysis.get("system_performance", {}).get("memory_usage", {})
        if memory.get("leak_detected", False):
            issues.append("æ£€æµ‹åˆ°å†…å­˜æ³„æ¼")
        
        return issues
    
    def _extract_performance_highlights(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ€§èƒ½äº®ç‚¹"""
        return {
            "test_coverage": f"{analysis.get('overall_test_coverage', {}).get('coverage_percentage', 0):.1f}%",
            "improvement_rate": f"{analysis.get('learning_effectiveness', {}).get('average_improvement_rate', 0):.1f}%",
            "stability_status": analysis.get("stability_assessment", {}).get("overall_stability", "UNKNOWN")
        }
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if analysis.get("overall_test_coverage", {}).get("coverage_percentage", 0) < 100:
            recommendations.append("å»ºè®®å®Œå–„æµ‹è¯•è¦†ç›–ç‡ï¼Œç¡®ä¿æ‰€æœ‰æ¨¡å—éƒ½å¾—åˆ°å……åˆ†æµ‹è¯•")
        
        learning_eff = analysis.get("learning_effectiveness", {})
        if learning_eff.get("effectiveness_status") == "NEEDS_IMPROVEMENT":
            recommendations.append("å»ºè®®ä¼˜åŒ–è®­ç»ƒæ•°æ®è´¨é‡å’Œè®­ç»ƒå‚æ•°ï¼Œæé«˜å­¦ä¹ æ•ˆæœ")
        
        if not analysis.get("system_performance", {}).get("gpu_performance", {}).get("gpu_available", False):
            recommendations.append("å»ºè®®åœ¨GPUç¯å¢ƒä¸­è¿›è¡Œæ€§èƒ½æµ‹è¯•ï¼Œä»¥è·å¾—æ›´å…¨é¢çš„æ€§èƒ½æ•°æ®")
        
        return recommendations
    
    def _generate_conclusion(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»“è®º"""
        overall_status = analysis.get("overall_test_coverage", {}).get("coverage_status", "UNKNOWN")
        
        if overall_status == "EXCELLENT":
            return "VisionAI-ClipsMasterè®­ç»ƒéªŒè¯æµ‹è¯•å…¨é¢å®Œæˆï¼Œæ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å‡è¾¾åˆ°é¢„æœŸæ ‡å‡†ï¼Œç³»ç»Ÿå·²å‡†å¤‡å¥½æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ã€‚"
        elif overall_status == "GOOD":
            return "VisionAI-ClipsMasterè®­ç»ƒéªŒè¯æµ‹è¯•åŸºæœ¬å®Œæˆï¼Œä¸»è¦åŠŸèƒ½è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®åœ¨è§£å†³å°‘æ•°é—®é¢˜åæŠ•å…¥ä½¿ç”¨ã€‚"
        else:
            return "VisionAI-ClipsMasterè®­ç»ƒéªŒè¯æµ‹è¯•å‘ç°ä¸€äº›éœ€è¦æ”¹è¿›çš„é—®é¢˜ï¼Œå»ºè®®åœ¨è§£å†³è¿™äº›é—®é¢˜åé‡æ–°è¿›è¡Œæµ‹è¯•ã€‚"
    
    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import platform
        import psutil
        
        return {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3)
        }
    
    def _save_final_report(self, report: Dict[str, Any]):
        """ä¿å­˜æœ€ç»ˆæŠ¥å‘Š"""
        try:
            # ä¿å­˜JSONæ ¼å¼
            json_path = self.output_dir / f"final_training_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            # ä¿å­˜HTMLæ ¼å¼
            html_path = self._generate_html_report(report)
            
            self.logger.info(f"ğŸ“Š æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: JSON={json_path}, HTML={html_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æœ€ç»ˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
    
    def _generate_html_report(self, report: Dict[str, Any]) -> Path:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>VisionAI-ClipsMaster æœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Š</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
                .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; text-align: center; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .success {{ background: #d4edda; border-color: #c3e6cb; }}
                .warning {{ background: #fff3cd; border-color: #ffeaa7; }}
                .error {{ background: #f8d7da; border-color: #f5c6cb; }}
                .metric {{ display: inline-block; margin: 10px; padding: 10px; background: #f8f9fa; border-radius: 3px; }}
                .status-excellent {{ color: #28a745; font-weight: bold; }}
                .status-good {{ color: #ffc107; font-weight: bold; }}
                .status-needs-improvement {{ color: #dc3545; font-weight: bold; }}
                ul {{ padding-left: 20px; }}
                .highlight {{ background: #e7f3ff; padding: 10px; border-left: 4px solid #007bff; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸš€ VisionAI-ClipsMaster æœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Š</h1>
                <p>ç”Ÿæˆæ—¶é—´: {report['report_metadata']['generation_time']}</p>
                <p>æŠ¥å‘Šç‰ˆæœ¬: {report['report_metadata']['report_version']}</p>
            </div>
            
            <div class="section success">
                <h2>ğŸ“‹ æ‰§è¡Œæ‘˜è¦</h2>
                <div class="highlight">
                    <h3>æ€»ä½“çŠ¶æ€: <span class="status-{report['executive_summary']['overall_status'].lower().replace('_', '-')}">{report['executive_summary']['overall_status']}</span></h3>
                </div>
                
                <h4>ğŸ† å…³é”®æˆå°±:</h4>
                <ul>
                    {''.join(f'<li>{achievement}</li>' for achievement in report['executive_summary']['key_achievements'])}
                </ul>
                
                <h4>âš ï¸ å…³é”®é—®é¢˜:</h4>
                <ul>
                    {''.join(f'<li>{issue}</li>' for issue in report['executive_summary']['critical_issues'])}
                </ul>
                
                <h4>ğŸ“Š æ€§èƒ½äº®ç‚¹:</h4>
                <div class="metric">æµ‹è¯•è¦†ç›–ç‡: {report['executive_summary']['performance_highlights']['test_coverage']}</div>
                <div class="metric">æ”¹è¿›ç‡: {report['executive_summary']['performance_highlights']['improvement_rate']}</div>
                <div class="metric">ç¨³å®šæ€§: {report['executive_summary']['performance_highlights']['stability_status']}</div>
            </div>
            
            <div class="section">
                <h2>ğŸ’¡ å»ºè®®</h2>
                <ul>
                    {''.join(f'<li>{rec}</li>' for rec in report['recommendations'])}
                </ul>
            </div>
            
            <div class="section">
                <h2>ğŸ¯ ç»“è®º</h2>
                <p>{report['conclusion']}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ”§ ç³»ç»Ÿä¿¡æ¯</h2>
                <div class="metric">å¹³å°: {report['report_metadata']['system_info']['platform']}</div>
                <div class="metric">Pythonç‰ˆæœ¬: {report['report_metadata']['system_info']['python_version']}</div>
                <div class="metric">CPUæ ¸å¿ƒ: {report['report_metadata']['system_info']['cpu_count']}</div>
                <div class="metric">å†…å­˜: {report['report_metadata']['system_info']['memory_total_gb']:.1f}GB</div>
            </div>
        </body>
        </html>
        """
        
        html_path = self.output_dir / f"final_training_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return html_path
    
    def _generate_dashboard(self, report: Dict[str, Any]):
        """ç”Ÿæˆå¯è§†åŒ–ä»ªè¡¨æ¿"""
        try:
            import matplotlib.pyplot as plt
            
            # åˆ›å»ºç»¼åˆä»ªè¡¨æ¿
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('VisionAI-ClipsMaster è®­ç»ƒéªŒè¯ä»ªè¡¨æ¿', fontsize=16)
            
            # æµ‹è¯•è¦†ç›–ç‡é¥¼å›¾
            coverage_data = report['comprehensive_analysis']['overall_test_coverage']
            coverage_pct = coverage_data['coverage_percentage']
            ax1.pie([coverage_pct, 100-coverage_pct], labels=['å·²è¦†ç›–', 'æœªè¦†ç›–'], 
                   autopct='%1.1f%%', startangle=90, colors=['#28a745', '#dc3545'])
            ax1.set_title('æµ‹è¯•è¦†ç›–ç‡')
            
            # æ¨¡å—çŠ¶æ€æ¡å½¢å›¾
            training_perf = report['comprehensive_analysis']['training_module_performance']
            total_modules = training_perf['total_modules_tested']
            successful_modules = training_perf['successful_modules']
            ax2.bar(['æˆåŠŸ', 'å¤±è´¥'], [successful_modules, total_modules - successful_modules], 
                   color=['#28a745', '#dc3545'])
            ax2.set_title('è®­ç»ƒæ¨¡å—çŠ¶æ€')
            ax2.set_ylabel('æ¨¡å—æ•°é‡')
            
            # è´¨é‡æŒ‡æ ‡é›·è¾¾å›¾ï¼ˆç®€åŒ–ä¸ºæ¡å½¢å›¾ï¼‰
            quality_metrics = report['comprehensive_analysis']['quality_metrics']['individual_metrics']
            metrics_names = list(quality_metrics.keys())
            metrics_values = [v if v is not None else 0 for v in quality_metrics.values()]
            ax3.barh(metrics_names, metrics_values, color='#007bff')
            ax3.set_title('è´¨é‡æŒ‡æ ‡')
            ax3.set_xlabel('è¯„åˆ†')
            ax3.set_xlim(0, 1)
            
            # æ€§èƒ½è¶‹åŠ¿ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
            ax4.plot([1, 2, 3, 4, 5], [0.6, 0.7, 0.8, 0.85, 0.9], marker='o', color='#28a745')
            ax4.set_title('æ€§èƒ½è¶‹åŠ¿')
            ax4.set_xlabel('æµ‹è¯•é˜¶æ®µ')
            ax4.set_ylabel('æ€§èƒ½è¯„åˆ†')
            ax4.set_ylim(0, 1)
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            dashboard_path = self.output_dir / "training_validation_dashboard.png"
            plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"ğŸ“Š å¯è§†åŒ–ä»ªè¡¨æ¿å·²ç”Ÿæˆ: {dashboard_path}")
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¯è§†åŒ–ä»ªè¡¨æ¿å¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“‹ å¯åŠ¨æœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Šç”Ÿæˆå™¨")
    print("=" * 60)
    
    report_generator = FinalTrainingValidationReport()
    
    try:
        final_report = report_generator.generate_comprehensive_report()
        
        print("\nâœ… æœ€ç»ˆè®­ç»ƒéªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_generator.output_dir}")
        
        # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
        executive_summary = final_report.get("executive_summary", {})
        print(f"\nğŸ¯ æ€»ä½“çŠ¶æ€: {executive_summary.get('overall_status', 'UNKNOWN')}")
        print(f"ğŸ† å…³é”®æˆå°±æ•°é‡: {len(executive_summary.get('key_achievements', []))}")
        print(f"âš ï¸ å…³é”®é—®é¢˜æ•°é‡: {len(executive_summary.get('critical_issues', []))}")
        
    except Exception as e:
        print(f"\nğŸ’¥ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
    
    print("\n" + "=" * 60)
    print("ğŸ æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå™¨é€€å‡º")


if __name__ == "__main__":
    main()
