#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
VisionAI-ClipsMaster é¡¹ç›®ä½“ç§¯ä¼˜åŒ–å·¥å…·
æä¾›ä¸‰ç§ä¼˜åŒ–ç­–ç•¥ï¼šæ¿€è¿›ã€å¹³è¡¡ã€ä¿å®ˆ
"""

import os
import sys
import json
import shutil
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProjectOptimizer:
    """é¡¹ç›®ä½“ç§¯ä¼˜åŒ–å™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "optimization_backup"
        
        # ä¼˜åŒ–ç­–ç•¥é…ç½®
        self.strategies = {
            "aggressive": {
                "name": "æ¿€è¿›ä¼˜åŒ–",
                "target_size": "â‰¤3GB",
                "compression_ratio": "91%",
                "description": "æœ€å¤§å‹ç¼©ï¼Œé€‚åˆåœ¨çº¿åˆ†å‘"
            },
            "balanced": {
                "name": "å¹³è¡¡ä¼˜åŒ–", 
                "target_size": "â‰¤8GB",
                "compression_ratio": "76%",
                "description": "ä½“ç§¯ä¸æ€§èƒ½å…¼é¡¾"
            },
            "conservative": {
                "name": "ä¿å®ˆä¼˜åŒ–",
                "target_size": "â‰¤15GB", 
                "compression_ratio": "55%",
                "description": "æœ€å°é£é™©ï¼Œç¡®ä¿ç¨³å®šæ€§"
            }
        }
    
    def create_backup(self, files_to_backup: List[str]):
        """åˆ›å»ºå¤‡ä»½"""
        if not self.backup_dir.exists():
            self.backup_dir.mkdir(parents=True)
            
        backup_manifest = []
        
        for file_pattern in files_to_backup:
            source_path = self.project_root / file_pattern
            if source_path.exists():
                if source_path.is_dir():
                    backup_path = self.backup_dir / file_pattern
                    if not backup_path.exists():
                        shutil.copytree(source_path, backup_path)
                        backup_manifest.append({"type": "dir", "source": file_pattern, "backup": str(backup_path)})
                else:
                    backup_path = self.backup_dir / file_pattern
                    backup_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(source_path, backup_path)
                    backup_manifest.append({"type": "file", "source": file_pattern, "backup": str(backup_path)})
        
        # ä¿å­˜å¤‡ä»½æ¸…å•
        with open(self.backup_dir / "backup_manifest.json", 'w', encoding='utf-8') as f:
            json.dump(backup_manifest, f, indent=2, ensure_ascii=False)
            
        logger.info(f"å¤‡ä»½å®Œæˆ: {len(backup_manifest)} é¡¹")
    
    def aggressive_optimization(self):
        """æ¿€è¿›ä¼˜åŒ–ç­–ç•¥"""
        logger.info("ğŸš€ æ‰§è¡Œæ¿€è¿›ä¼˜åŒ–ç­–ç•¥...")
        
        # 1. æ¨¡å‹æ–‡ä»¶å¤„ç†
        model_files = [
            "models/models/qwen/base/*.safetensors",
            "models/models/mistral/base/*.safetensors"
        ]
        
        # å¤‡ä»½å…³é”®é…ç½®
        self.create_backup([
            "models/configs/",
            "configs/model_config.yaml"
        ])
        
        # ç§»é™¤å¤§å‹æ¨¡å‹æ–‡ä»¶
        removed_size = 0
        for pattern in model_files:
            for file_path in self.project_root.glob(pattern):
                if file_path.exists():
                    size = file_path.stat().st_size
                    file_path.unlink()
                    removed_size += size
                    logger.info(f"ç§»é™¤: {file_path} ({size//1024//1024}MB)")
        
        # 2. Gitå†å²æ¸…ç†
        self._clean_git_history()
        
        # 3. å·¥å…·é“¾ä¼˜åŒ–
        tool_dirs = ["tools/ffmpeg", "llama.cpp", "downloads/"]
        for tool_dir in tool_dirs:
            tool_path = self.project_root / tool_dir
            if tool_path.exists():
                shutil.rmtree(tool_path)
                logger.info(f"ç§»é™¤å·¥å…·ç›®å½•: {tool_dir}")
        
        # 4. æµ‹è¯•æ•°æ®æ¸…ç†
        self._clean_test_data("aggressive")
        
        # 5. åˆ›å»ºæŒ‰éœ€ä¸‹è½½é…ç½®
        self._setup_on_demand_download()
        
        logger.info(f"æ¿€è¿›ä¼˜åŒ–å®Œæˆï¼Œé¢„è®¡å‡å°‘: {removed_size//1024//1024//1024:.1f}GB")
    
    def balanced_optimization(self):
        """å¹³è¡¡ä¼˜åŒ–ç­–ç•¥"""
        logger.info("âš–ï¸ æ‰§è¡Œå¹³è¡¡ä¼˜åŒ–ç­–ç•¥...")
        
        # 1. ä¿ç•™é‡åŒ–æ¨¡å‹ï¼Œç§»é™¤åŸå§‹æ¨¡å‹
        self.create_backup(["models/models/"])
        
        # ç§»é™¤åŸå§‹safetensorsï¼Œä¿ç•™é‡åŒ–ç‰ˆæœ¬
        for safetensor_file in self.project_root.glob("models/**/base/*.safetensors"):
            safetensor_file.unlink()
            logger.info(f"ç§»é™¤åŸå§‹æ¨¡å‹: {safetensor_file}")
        
        # 2. Git LFSè¿ç§»
        self._setup_git_lfs()
        
        # 3. ä¾èµ–é¡¹ä¼˜åŒ–
        redundant_dirs = ["test_venv/", "spacy_wheels/"]
        for dir_name in redundant_dirs:
            dir_path = self.project_root / dir_name
            if dir_path.exists():
                shutil.rmtree(dir_path)
                logger.info(f"ç§»é™¤å†—ä½™ç›®å½•: {dir_name}")
        
        # 4. å‹ç¼©æµ‹è¯•æ•°æ®
        self._compress_test_data()
        
        logger.info("å¹³è¡¡ä¼˜åŒ–å®Œæˆ")
    
    def conservative_optimization(self):
        """ä¿å®ˆä¼˜åŒ–ç­–ç•¥"""
        logger.info("ğŸ›¡ï¸ æ‰§è¡Œä¿å®ˆä¼˜åŒ–ç­–ç•¥...")
        
        # 1. æ¸…ç†ç¼–è¯‘ç¼“å­˜
        cache_removed = self._clean_cache_files()
        
        # 2. æ¸…ç†æ—¥å¿—æ–‡ä»¶
        log_removed = self._clean_log_files()
        
        # 3. æ¸…ç†å¤‡ä»½æ–‡ä»¶
        backup_removed = self._clean_backup_files()
        
        # 4. å‹ç¼©å¤§å‹æµ‹è¯•æ–‡ä»¶
        test_compressed = self._compress_large_test_files()
        
        total_saved = cache_removed + log_removed + backup_removed + test_compressed
        logger.info(f"ä¿å®ˆä¼˜åŒ–å®Œæˆï¼ŒèŠ‚çœç©ºé—´: {total_saved//1024//1024}MB")
    
    def _clean_git_history(self):
        """æ¸…ç†Gitå†å²ä¸­çš„å¤§æ–‡ä»¶"""
        logger.info("æ¸…ç†Gitå†å²...")
        
        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…éœ€è¦è°¨æ…æ“ä½œ
        git_commands = [
            "git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch *.safetensors *.bin *.pkl' --prune-empty --tag-name-filter cat -- --all",
            "git for-each-ref --format='delete %(refname)' refs/original | git update-ref --stdin",
            "git reflog expire --expire=now --all",
            "git gc --prune=now --aggressive"
        ]
        
        logger.warning("Gitå†å²æ¸…ç†éœ€è¦æ‰‹åŠ¨æ‰§è¡Œä»¥ä¸‹å‘½ä»¤:")
        for cmd in git_commands:
            logger.warning(f"  {cmd}")
    
    def _clean_test_data(self, strategy: str):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        if strategy == "aggressive":
            # ç§»é™¤å¤§å‹æµ‹è¯•æ–‡ä»¶
            for test_file in self.project_root.glob("tests/**/*.mp4"):
                if test_file.stat().st_size > 10 * 1024 * 1024:  # >10MB
                    test_file.unlink()
                    logger.info(f"ç§»é™¤å¤§å‹æµ‹è¯•æ–‡ä»¶: {test_file}")
            
            for pkl_file in self.project_root.glob("**/*.pkl"):
                if pkl_file.stat().st_size > 50 * 1024 * 1024:  # >50MB
                    pkl_file.unlink()
                    logger.info(f"ç§»é™¤å¤§å‹pklæ–‡ä»¶: {pkl_file}")
    
    def _setup_on_demand_download(self):
        """è®¾ç½®æŒ‰éœ€ä¸‹è½½"""
        download_config = {
            "models": {
                "qwen2.5-7b": {
                    "url": "https://modelscope.cn/models/qwen/Qwen2.5-7B-Instruct-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf",
                    "size": "4.1GB",
                    "priority": 1
                },
                "mistral-7b": {
                    "url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.q4_k_m.gguf",
                    "size": "4.1GB", 
                    "priority": 1
                }
            },
            "tools": {
                "ffmpeg": {
                    "url": "https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-win64-gpl.zip",
                    "size": "100MB",
                    "priority": 2
                }
            }
        }
        
        config_path = self.project_root / "configs" / "download_manifest.json"
        config_path.parent.mkdir(exist_ok=True)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(download_config, f, indent=2, ensure_ascii=False)
            
        logger.info("æŒ‰éœ€ä¸‹è½½é…ç½®å·²åˆ›å»º")
    
    def _setup_git_lfs(self):
        """è®¾ç½®Git LFS"""
        gitattributes_content = """
# Git LFS é…ç½®
*.safetensors filter=lfs diff=lfs merge=lfs -text
*.bin filter=lfs diff=lfs merge=lfs -text
*.pkl filter=lfs diff=lfs merge=lfs -text
*.mp4 filter=lfs diff=lfs merge=lfs -text
*.zip filter=lfs diff=lfs merge=lfs -text
"""
        
        gitattributes_path = self.project_root / ".gitattributes"
        with open(gitattributes_path, 'w', encoding='utf-8') as f:
            f.write(gitattributes_content)
            
        logger.info("Git LFSé…ç½®å·²åˆ›å»º")
    
    def _clean_cache_files(self) -> int:
        """æ¸…ç†ç¼“å­˜æ–‡ä»¶"""
        total_removed = 0
        
        # Pythonç¼“å­˜
        for cache_dir in self.project_root.rglob("__pycache__"):
            if cache_dir.is_dir():
                size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
                shutil.rmtree(cache_dir)
                total_removed += size
        
        # .pycæ–‡ä»¶
        for pyc_file in self.project_root.rglob("*.pyc"):
            total_removed += pyc_file.stat().st_size
            pyc_file.unlink()
        
        logger.info(f"æ¸…ç†ç¼“å­˜æ–‡ä»¶: {total_removed//1024//1024}MB")
        return total_removed
    
    def _clean_log_files(self) -> int:
        """æ¸…ç†æ—¥å¿—æ–‡ä»¶"""
        total_removed = 0
        
        # æ¸…ç†30å¤©å‰çš„æ—¥å¿—
        import time
        cutoff_time = time.time() - (30 * 24 * 3600)
        
        for log_file in self.project_root.glob("logs/**/*.log"):
            if log_file.stat().st_mtime < cutoff_time:
                total_removed += log_file.stat().st_size
                log_file.unlink()
        
        logger.info(f"æ¸…ç†æ—¥å¿—æ–‡ä»¶: {total_removed//1024//1024}MB")
        return total_removed
    
    def _clean_backup_files(self) -> int:
        """æ¸…ç†å¤‡ä»½æ–‡ä»¶"""
        total_removed = 0
        
        backup_patterns = ["*.bak", "*.backup", "*~"]
        for pattern in backup_patterns:
            for backup_file in self.project_root.rglob(pattern):
                total_removed += backup_file.stat().st_size
                backup_file.unlink()
        
        logger.info(f"æ¸…ç†å¤‡ä»½æ–‡ä»¶: {total_removed//1024//1024}MB")
        return total_removed
    
    def _compress_test_data(self):
        """å‹ç¼©æµ‹è¯•æ•°æ®"""
        import gzip
        
        for large_file in self.project_root.glob("data/**/*.pkl"):
            if large_file.stat().st_size > 100 * 1024 * 1024:  # >100MB
                compressed_file = large_file.with_suffix(large_file.suffix + '.gz')
                
                with open(large_file, 'rb') as f_in:
                    with gzip.open(compressed_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                large_file.unlink()
                logger.info(f"å‹ç¼©: {large_file} -> {compressed_file}")
    
    def _compress_large_test_files(self) -> int:
        """å‹ç¼©å¤§å‹æµ‹è¯•æ–‡ä»¶"""
        # å®ç°ç±»ä¼¼_compress_test_dataçš„é€»è¾‘
        return 0

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="VisionAI-ClipsMaster é¡¹ç›®ä½“ç§¯ä¼˜åŒ–å·¥å…·")
    parser.add_argument("strategy", choices=["aggressive", "balanced", "conservative"], 
                       help="ä¼˜åŒ–ç­–ç•¥")
    parser.add_argument("--dry-run", action="store_true", help="ä»…æ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„æ“ä½œ")
    parser.add_argument("--backup", action="store_true", default=True, help="åˆ›å»ºå¤‡ä»½")
    
    args = parser.parse_args()
    
    optimizer = ProjectOptimizer()
    
    # æ˜¾ç¤ºç­–ç•¥ä¿¡æ¯
    strategy_info = optimizer.strategies[args.strategy]
    print(f"ğŸ¯ ä¼˜åŒ–ç­–ç•¥: {strategy_info['name']}")
    print(f"ğŸ“Š ç›®æ ‡å¤§å°: {strategy_info['target_size']}")
    print(f"ğŸ“ˆ å‹ç¼©æ¯”ä¾‹: {strategy_info['compression_ratio']}")
    print(f"ğŸ“ è¯´æ˜: {strategy_info['description']}")
    print()
    
    if args.dry_run:
        print("ğŸ” é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…ä¿®æ”¹æ–‡ä»¶")
        return
    
    # ç¡®è®¤æ“ä½œ
    confirm = input("ç¡®è®¤æ‰§è¡Œä¼˜åŒ–? (y/N): ")
    if confirm.lower() != 'y':
        print("æ“ä½œå·²å–æ¶ˆ")
        return
    
    # æ‰§è¡Œä¼˜åŒ–
    if args.strategy == "aggressive":
        optimizer.aggressive_optimization()
    elif args.strategy == "balanced":
        optimizer.balanced_optimization()
    elif args.strategy == "conservative":
        optimizer.conservative_optimization()
    
    print("âœ… ä¼˜åŒ–å®Œæˆ!")

if __name__ == "__main__":
    main()
