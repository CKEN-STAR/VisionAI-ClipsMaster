#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPU/CPUå…¼å®¹æ€§ç®¡ç†å™¨
è‡ªåŠ¨æ£€æµ‹å’Œæ— ç¼åˆ‡æ¢GPU/CPUï¼Œç¡®ä¿ç³»ç»Ÿåœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½æ­£å¸¸è¿è¡Œ
"""

import os
import sys
import platform
import subprocess
import logging
import psutil
import torch
from typing import Dict, Any, Optional, List
from pathlib import Path

class GPUCPUManager:
    """GPU/CPUå…¼å®¹æ€§ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–GPU/CPUç®¡ç†å™¨"""
        self.logger = self._setup_logger()
        self.system_info = self._detect_system_info()
        self.gpu_info = self._detect_gpu_info()
        self.cpu_info = self._detect_cpu_info()
        self.recommended_device = self._determine_best_device()
        
        self.logger.info("ğŸ”§ GPU/CPUå…¼å®¹æ€§ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ’» ç³»ç»Ÿ: {self.system_info['platform']}")
        self.logger.info(f"ğŸ¯ æ¨èè®¾å¤‡: {self.recommended_device}")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("GPUCPUManager")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _detect_system_info(self) -> Dict[str, Any]:
        """æ£€æµ‹ç³»ç»Ÿä¿¡æ¯"""
        return {
            "platform": platform.system(),
            "platform_version": platform.version(),
            "architecture": platform.architecture()[0],
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3)
        }
    
    def _detect_gpu_info(self) -> Dict[str, Any]:
        """æ£€æµ‹GPUä¿¡æ¯"""
        gpu_info = {
            "cuda_available": False,
            "cuda_version": None,
            "gpu_count": 0,
            "gpu_devices": [],
            "total_memory_gb": 0,
            "driver_version": None
        }
        
        try:
            # æ£€æµ‹CUDA
            if torch.cuda.is_available():
                gpu_info["cuda_available"] = True
                gpu_info["cuda_version"] = torch.version.cuda
                gpu_info["gpu_count"] = torch.cuda.device_count()
                
                # è·å–æ¯ä¸ªGPUçš„è¯¦ç»†ä¿¡æ¯
                for i in range(gpu_info["gpu_count"]):
                    props = torch.cuda.get_device_properties(i)
                    device_info = {
                        "id": i,
                        "name": props.name,
                        "memory_gb": props.total_memory / (1024**3),
                        "compute_capability": f"{props.major}.{props.minor}"
                    }
                    gpu_info["gpu_devices"].append(device_info)
                    gpu_info["total_memory_gb"] += device_info["memory_gb"]
                
                self.logger.info(f"âœ… æ£€æµ‹åˆ° {gpu_info['gpu_count']} ä¸ªGPUè®¾å¤‡")
                for device in gpu_info["gpu_devices"]:
                    self.logger.info(f"  ğŸ“± {device['name']}: {device['memory_gb']:.1f}GB")
            else:
                self.logger.info("âŒ æœªæ£€æµ‹åˆ°CUDAæ”¯æŒ")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ GPUæ£€æµ‹å¤±è´¥: {str(e)}")
        
        # å°è¯•æ£€æµ‹NVIDIAé©±åŠ¨
        try:
            if platform.system() == "Windows":
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=driver_version", "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    gpu_info["driver_version"] = result.stdout.strip().split('\n')[0]
            else:
                result = subprocess.run(
                    ["nvidia-smi", "--query-gpu=driver_version", "--format=csv,noheader,nounits"],
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0:
                    gpu_info["driver_version"] = result.stdout.strip().split('\n')[0]
        except:
            pass
        
        return gpu_info
    
    def _detect_cpu_info(self) -> Dict[str, Any]:
        """æ£€æµ‹CPUä¿¡æ¯"""
        cpu_info = {
            "cores": psutil.cpu_count(logical=False),
            "threads": psutil.cpu_count(logical=True),
            "frequency_mhz": psutil.cpu_freq().current if psutil.cpu_freq() else 0,
            "usage_percent": psutil.cpu_percent(interval=1),
            "architecture": platform.machine(),
            "supports_avx": self._check_cpu_features()
        }
        
        self.logger.info(f"ğŸ’» CPU: {cpu_info['cores']}æ ¸{cpu_info['threads']}çº¿ç¨‹")
        self.logger.info(f"âš¡ é¢‘ç‡: {cpu_info['frequency_mhz']:.0f}MHz")
        
        return cpu_info
    
    def _check_cpu_features(self) -> Dict[str, bool]:
        """æ£€æŸ¥CPUç‰¹æ€§æ”¯æŒ"""
        features = {
            "avx": False,
            "avx2": False,
            "sse4": False
        }
        
        try:
            if platform.system() == "Windows":
                # Windowsä¸‹æ£€æŸ¥CPUç‰¹æ€§
                import cpuinfo
                info = cpuinfo.get_cpu_info()
                flags = info.get('flags', [])
                features["avx"] = 'avx' in flags
                features["avx2"] = 'avx2' in flags
                features["sse4"] = 'sse4_1' in flags or 'sse4_2' in flags
        except:
            # å¦‚æœæ— æ³•æ£€æµ‹ï¼Œå‡è®¾æ”¯æŒåŸºæœ¬ç‰¹æ€§
            features["sse4"] = True
        
        return features
    
    def _determine_best_device(self) -> str:
        """ç¡®å®šæœ€ä½³è®¾å¤‡"""
        # ä¼˜å…ˆçº§ï¼šGPU > CPU
        if self.gpu_info["cuda_available"] and self.gpu_info["gpu_count"] > 0:
            # æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ
            best_gpu = max(self.gpu_info["gpu_devices"], key=lambda x: x["memory_gb"])
            if best_gpu["memory_gb"] >= 4.0:  # è‡³å°‘4GBæ˜¾å­˜
                return f"cuda:{best_gpu['id']}"
            else:
                self.logger.warning(f"âš ï¸ GPUæ˜¾å­˜ä¸è¶³ ({best_gpu['memory_gb']:.1f}GB < 4GB)")
        
        # æ£€æŸ¥CPUæ˜¯å¦è¶³å¤Ÿå¼ºå¤§
        if self.cpu_info["cores"] >= 4 and self.system_info["memory_total_gb"] >= 8:
            return "cpu"
        else:
            self.logger.warning("âš ï¸ CPUé…ç½®è¾ƒä½ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
            return "cpu"  # ä»ç„¶è¿”å›CPUï¼Œä½†ä¼šæœ‰æ€§èƒ½è­¦å‘Š
    
    def get_optimal_config(self, task_type: str = "training") -> Dict[str, Any]:
        """è·å–æœ€ä¼˜é…ç½®"""
        device = self.recommended_device
        
        if device.startswith("cuda"):
            # GPUé…ç½®
            gpu_id = int(device.split(":")[1])
            gpu_device = self.gpu_info["gpu_devices"][gpu_id]
            
            config = {
                "device": device,
                "device_type": "gpu",
                "batch_size": min(8, max(2, int(gpu_device["memory_gb"] / 2))),
                "num_workers": min(4, self.cpu_info["threads"]),
                "mixed_precision": gpu_device["memory_gb"] >= 6,
                "gradient_accumulation": max(1, 8 // min(8, max(2, int(gpu_device["memory_gb"] / 2)))),
                "memory_limit_gb": gpu_device["memory_gb"] * 0.8  # 80%çš„æ˜¾å­˜
            }
        else:
            # CPUé…ç½®
            config = {
                "device": "cpu",
                "device_type": "cpu",
                "batch_size": max(1, min(4, self.cpu_info["cores"] // 2)),
                "num_workers": max(1, min(4, self.cpu_info["cores"] // 2)),
                "mixed_precision": False,
                "gradient_accumulation": max(2, 8 // max(1, min(4, self.cpu_info["cores"] // 2))),
                "memory_limit_gb": self.system_info["memory_total_gb"] * 0.6  # 60%çš„å†…å­˜
            }
        
        # ä»»åŠ¡ç‰¹å®šè°ƒæ•´
        if task_type == "inference":
            config["batch_size"] = max(1, config["batch_size"] // 2)
        elif task_type == "training":
            # è®­ç»ƒæ—¶æ›´ä¿å®ˆçš„å†…å­˜ä½¿ç”¨
            config["memory_limit_gb"] *= 0.8
        
        return config
    
    def auto_configure_torch(self) -> Dict[str, Any]:
        """è‡ªåŠ¨é…ç½®PyTorch"""
        config = self.get_optimal_config()
        
        try:
            # è®¾ç½®è®¾å¤‡
            device = torch.device(config["device"])
            
            # è®¾ç½®çº¿ç¨‹æ•°ï¼ˆä»…åœ¨æœªè®¾ç½®æ—¶ï¼‰
            if config["device_type"] == "cpu":
                try:
                    torch.set_num_threads(config["num_workers"])
                except RuntimeError:
                    # å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥é”™è¯¯
                    pass
                try:
                    torch.set_num_interop_threads(config["num_workers"])
                except RuntimeError:
                    # å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥é”™è¯¯
                    pass
            
            # è®¾ç½®å†…å­˜ç­–ç•¥
            if config["device_type"] == "gpu":
                torch.cuda.set_per_process_memory_fraction(0.8)
                torch.cuda.empty_cache()
            
            self.logger.info(f"âœ… PyTorché…ç½®å®Œæˆ: {config['device']}")
            self.logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
            self.logger.info(f"ğŸ”§ å·¥ä½œçº¿ç¨‹: {config['num_workers']}")
            
            return {
                "success": True,
                "device": device,
                "config": config
            }
            
        except Exception as e:
            self.logger.error(f"âŒ PyTorché…ç½®å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "fallback_device": torch.device("cpu")
            }
    
    def check_compatibility(self, requirements: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æŸ¥å…¼å®¹æ€§"""
        compatibility = {
            "compatible": True,
            "warnings": [],
            "errors": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥å†…å­˜è¦æ±‚
        required_memory = requirements.get("memory_gb", 4)
        if self.recommended_device.startswith("cuda"):
            gpu_id = int(self.recommended_device.split(":")[1])
            available_memory = self.gpu_info["gpu_devices"][gpu_id]["memory_gb"]
        else:
            available_memory = self.system_info["memory_total_gb"]
        
        if available_memory < required_memory:
            compatibility["errors"].append(
                f"å†…å­˜ä¸è¶³: éœ€è¦{required_memory}GBï¼Œå¯ç”¨{available_memory:.1f}GB"
            )
            compatibility["compatible"] = False
        
        # æ£€æŸ¥CUDAç‰ˆæœ¬
        required_cuda = requirements.get("cuda_version")
        if required_cuda and self.gpu_info["cuda_available"]:
            current_cuda = self.gpu_info["cuda_version"]
            if current_cuda != required_cuda:
                compatibility["warnings"].append(
                    f"CUDAç‰ˆæœ¬ä¸åŒ¹é…: éœ€è¦{required_cuda}ï¼Œå½“å‰{current_cuda}"
                )
        
        # æ€§èƒ½å»ºè®®
        if self.recommended_device == "cpu" and self.cpu_info["cores"] < 8:
            compatibility["recommendations"].append(
                "å»ºè®®ä½¿ç”¨8æ ¸ä»¥ä¸ŠCPUä»¥è·å¾—æ›´å¥½æ€§èƒ½"
            )
        
        if self.gpu_info["cuda_available"] and not self.recommended_device.startswith("cuda"):
            compatibility["recommendations"].append(
                "æ£€æµ‹åˆ°GPUä½†æœªä½¿ç”¨ï¼Œå¯èƒ½æ˜¯æ˜¾å­˜ä¸è¶³"
            )
        
        return compatibility
    
    def get_system_report(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸæŠ¥å‘Š"""
        return {
            "system_info": self.system_info,
            "gpu_info": self.gpu_info,
            "cpu_info": self.cpu_info,
            "recommended_device": self.recommended_device,
            "optimal_config": self.get_optimal_config(),
            "torch_config": self.auto_configure_torch()
        }
