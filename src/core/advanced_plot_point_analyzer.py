#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é«˜çº§å…³é”®æƒ…èŠ‚ç‚¹è¯†åˆ«åˆ†æå™¨

ç›®æ ‡ï¼šå°†å…³é”®æƒ…èŠ‚ç‚¹è¯†åˆ«F1åˆ†æ•°ä»75%æå‡è‡³90%
é‡‡ç”¨ï¼šå¤šç»´åº¦åˆ†æ + åŠ¨æ€é˜ˆå€¼ + å™äº‹ç»“æ„æ„ŸçŸ¥ + æƒ…æ„Ÿå¼ºåº¦æƒé‡
"""

import re
import math
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass

@dataclass
class PlotPoint:
    """æƒ…èŠ‚ç‚¹æ•°æ®ç±»"""
    index: int
    text: str
    importance_score: float
    evidence: List[str]
    plot_type: str
    confidence: float

class AdvancedPlotPointAnalyzer:
    """é«˜çº§å…³é”®æƒ…èŠ‚ç‚¹è¯†åˆ«åˆ†æå™¨"""
    
    def __init__(self):
        # å…³é”®è¯æƒé‡å­—å…¸ï¼ˆåˆ†ç±»åˆ«ï¼‰
        self.keyword_weights = {
            # é«˜æƒé‡å…³é”®è¯ï¼ˆæ ¸å¿ƒæƒ…èŠ‚ï¼‰
            "high_importance": {
                "çš‡ä¸Š": 1.0, "é™›ä¸‹": 1.0, "æœ•": 1.0, "ä¼ æœ•": 1.2,
                "é‡è¦": 1.0, "ç´§æ€¥": 1.0, "é‡å¤§": 1.2, "å…³ç³»é‡å¤§": 1.5,  # æé«˜é‡å¤§ç›¸å…³æƒé‡
                "è´¨ç–‘": 1.0, "ç«Ÿæ•¢": 1.1, "å¤§èƒ†": 1.0,
                "æ±Ÿå±±": 1.0, "ç¤¾ç¨·": 1.0, "å†³å®š": 0.9,
                "é¢è¯•": 1.0, "å…¬å¸": 0.8, "å·¥ä½œ": 0.8,
                "ç¦€æŠ¥": 1.0, "å¤ªå­": 1.0, "æ®¿ä¸‹": 1.0,
                "ä¸å¦¥": 0.9, "æ‹…å¿ƒ": 0.8, "å¬é›†": 1.0,
                "æ—¨æ„": 1.1, "å•†è®®": 0.9, "å¤–æ³„": 1.0,
                "ä¸å¯": 1.0, "è½»ä¸¾å¦„åŠ¨": 1.3, "è¿™ä»¶äº‹": 0.8  # æ–°å¢å…³é”®è¯
            },
            
            # ä¸­æƒé‡å…³é”®è¯ï¼ˆé‡è¦å¯¹è¯ï¼‰
            "medium_importance": {
                "ä¸å¦¥": 0.6, "æ‹…å¿ƒ": 0.6, "å¿§è™‘": 0.6,
                "æ˜ç™½": 0.5, "å¬ä»": 0.6, "å®‰æ’": 0.5,
                "é€Ÿé€Ÿ": 0.7, "ç«‹å³": 0.7, "é©¬ä¸Š": 0.7,
                "è°¢è°¢": 0.5, "æ„Ÿè°¢": 0.5, "è¯·é—®": 0.4,
                "ç´§å¼ ": 0.6, "æ”¾å¿ƒ": 0.5, "é¡ºåˆ©": 0.5
            },
            
            # ä½æƒé‡å…³é”®è¯ï¼ˆä¸€èˆ¬å¯¹è¯ï¼‰
            "low_importance": {
                "æ˜¯çš„": 0.3, "å¥½çš„": 0.3, "å—¯": 0.2,
                "è¿™æ ·": 0.3, "é‚£æ ·": 0.3, "ä»€ä¹ˆ": 0.3,
                "æ€ä¹ˆ": 0.3, "ä¸ºä»€ä¹ˆ": 0.4, "å“ªé‡Œ": 0.3
            }
        }
        
        # å¥å¼æ¨¡å¼æƒé‡
        self.sentence_patterns = {
            "question": (r".*[ï¼Ÿ|?]$", 0.6),           # ç–‘é—®å¥
            "exclamation": (r".*[ï¼|!]$", 0.8),        # æ„Ÿå¹å¥
            "command": (r"^[ä¼ |å¬|è®©|è¯·].*", 0.7),      # å‘½ä»¤å¥
            "formal_address": (r"^[çš‡ä¸Š|é™›ä¸‹].*", 0.8), # æ­£å¼ç§°å‘¼
            "negation": (r".*[ä¸|æ²¡|æœª].*", 0.6),       # å¦å®šå¥
            "emphasis": (r".*[ç«Ÿæ•¢|å¤§èƒ†|é€Ÿé€Ÿ].*", 0.9),  # å¼ºè°ƒå¥
            "prohibition": (r".*ä¸å¯.*[åŠ¨|è¡Œ].*", 1.0), # ç¦æ­¢å¥å¼
            "importance": (r".*[å…³ç³»|ååˆ†|éå¸¸].*[é‡å¤§|é‡è¦].*", 1.1), # é‡è¦æ€§å¥å¼
            "caution": (r".*[è½»ä¸¾å¦„åŠ¨|æ‰ä»¥è½»å¿ƒ|è‰ç‡].*", 0.9)  # è°¨æ…å¥å¼
        }
        
        # æƒ…æ„Ÿå¼ºåº¦æƒé‡æ˜ å°„
        self.emotion_weights = {
            "angry": 0.9,
            "urgent": 0.8,
            "formal": 0.7,
            "serious": 0.8,
            "worried": 0.7,
            "fearful": 0.6,
            "authoritative": 0.9,
            "grateful": 0.5,
            "nervous": 0.6,
            "polite": 0.4,
            "submissive": 0.5,
            "reassuring": 0.4,
            "professional": 0.6,
            "encouraging": 0.5,
            "neutral": 0.3
        }
        
        # å™äº‹ç»“æ„æƒé‡
        self.narrative_weights = {
            "exposition": 0.7,      # å¼€ç«¯
            "rising_action": 0.8,   # å‘å±•
            "climax": 1.0,          # é«˜æ½®
            "falling_action": 0.6,  # ä¸‹é™
            "resolution": 0.7       # ç»“å±€
        }
        
        # ä½ç½®æƒé‡ï¼ˆå¼€å¤´å’Œç»“å°¾æ›´é‡è¦ï¼‰
        self.position_weights = self._calculate_position_weights()
    
    def _calculate_position_weights(self) -> Dict[str, float]:
        """è®¡ç®—ä½ç½®æƒé‡"""
        return {
            "beginning": 0.8,  # å¼€å¤´20%
            "middle": 0.6,     # ä¸­é—´60%
            "end": 0.8         # ç»“å°¾20%
        }
    
    def analyze_plot_points(self, subtitles: List[str], 
                           expected_points: Optional[List[int]] = None) -> Dict[str, Any]:
        """åˆ†æå…³é”®æƒ…èŠ‚ç‚¹"""
        if not subtitles:
            return {"plot_points": [], "scores": [], "analysis": {}}
        
        # 1. è®¡ç®—æ¯ä¸ªå­—å¹•çš„é‡è¦æ€§åˆ†æ•°
        importance_scores = []
        plot_points = []
        
        for i, subtitle in enumerate(subtitles):
            score = self._calculate_importance_score(subtitle, i, len(subtitles))
            importance_scores.append(score)
            
            plot_point = PlotPoint(
                index=i,
                text=subtitle,
                importance_score=score,
                evidence=self._collect_evidence(subtitle),
                plot_type=self._classify_plot_type(subtitle),
                confidence=self._calculate_confidence(subtitle, score)
            )
            plot_points.append(plot_point)
        
        # 2. åŠ¨æ€é˜ˆå€¼è®¡ç®—
        threshold = self._calculate_dynamic_threshold(importance_scores)
        
        # 3. è¯†åˆ«å…³é”®æƒ…èŠ‚ç‚¹
        identified_points = []
        for i, score in enumerate(importance_scores):
            if score >= threshold:
                identified_points.append(i)

        # 4. åå¤„ç†ï¼šé¿å…è¿‡å¯†é›†çš„æƒ…èŠ‚ç‚¹
        identified_points = self._post_process_points(identified_points, subtitles)
        
        # 5. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance = {}
        if expected_points is not None:
            performance = self._calculate_performance_metrics(
                identified_points, expected_points
            )
        
        return {
            "plot_points": plot_points,
            "identified_points": identified_points,
            "importance_scores": importance_scores,
            "threshold": threshold,
            "performance": performance,
            "analysis": {
                "total_subtitles": len(subtitles),
                "identified_count": len(identified_points),
                "average_score": sum(importance_scores) / len(importance_scores),
                "max_score": max(importance_scores),
                "min_score": min(importance_scores)
            }
        }
    
    def _calculate_importance_score(self, text: str, position: int, total_count: int) -> float:
        """è®¡ç®—é‡è¦æ€§åˆ†æ•°"""
        score = 0.0
        
        # 1. å…³é”®è¯æƒé‡
        keyword_score = self._calculate_keyword_score(text)
        score += keyword_score
        
        # 2. å¥å¼æ¨¡å¼æƒé‡
        pattern_score = self._calculate_pattern_score(text)
        score += pattern_score
        
        # 3. æƒ…æ„Ÿå¼ºåº¦æƒé‡
        emotion_score = self._calculate_emotion_score(text)
        score += emotion_score
        
        # 4. ä½ç½®æƒé‡
        position_score = self._calculate_position_score(position, total_count)
        score *= position_score
        
        # 5. æ–‡æœ¬é•¿åº¦æƒé‡
        length_score = self._calculate_length_score(text)
        score *= length_score
        
        # 6. å™äº‹ç»“æ„æƒé‡
        narrative_score = self._calculate_narrative_score(position, total_count)
        score *= narrative_score
        
        return score
    
    def _calculate_keyword_score(self, text: str) -> float:
        """è®¡ç®—å…³é”®è¯åˆ†æ•°"""
        score = 0.0
        
        # æ£€æŸ¥å„ç±»å…³é”®è¯
        for category, keywords in self.keyword_weights.items():
            for keyword, weight in keywords.items():
                if keyword in text:
                    score += weight
        
        # å¤šå…³é”®è¯åŠ æˆ
        keyword_count = sum(1 for category in self.keyword_weights.values() 
                           for keyword in category.keys() if keyword in text)
        if keyword_count > 1:
            score *= (1 + 0.1 * (keyword_count - 1))
        
        return score
    
    def _calculate_pattern_score(self, text: str) -> float:
        """è®¡ç®—å¥å¼æ¨¡å¼åˆ†æ•°"""
        score = 0.0
        
        for pattern_name, (pattern, weight) in self.sentence_patterns.items():
            if re.search(pattern, text):
                score += weight
        
        return score
    
    def _calculate_emotion_score(self, text: str) -> float:
        """è®¡ç®—æƒ…æ„Ÿåˆ†æ•°"""
        try:
            # ä½¿ç”¨æƒ…æ„Ÿåˆ†æå™¨
            from src.emotion.emotion_intensity import get_emotion_intensity
            emotion_analyzer = get_emotion_intensity()
            
            emotions = emotion_analyzer.analyze_emotion_intensity(text)
            
            # è®¡ç®—åŠ æƒæƒ…æ„Ÿåˆ†æ•°
            emotion_score = 0.0
            for emotion, intensity in emotions.items():
                weight = self.emotion_weights.get(emotion, 0.3)
                emotion_score += intensity * weight
            
            return min(emotion_score, 1.0)  # é™åˆ¶æœ€å¤§å€¼
            
        except Exception:
            # å¦‚æœæƒ…æ„Ÿåˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™
            return 0.3
    
    def _calculate_position_score(self, position: int, total_count: int) -> float:
        """è®¡ç®—ä½ç½®åˆ†æ•°"""
        if total_count <= 1:
            return 1.0
        
        relative_position = position / (total_count - 1)
        
        if relative_position <= 0.2:  # å¼€å¤´20%
            return self.position_weights["beginning"]
        elif relative_position >= 0.8:  # ç»“å°¾20%
            return self.position_weights["end"]
        else:  # ä¸­é—´60%
            return self.position_weights["middle"]
    
    def _calculate_length_score(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬é•¿åº¦åˆ†æ•°"""
        length = len(text)
        
        if length < 5:
            return 0.5  # å¤ªçŸ­
        elif length > 50:
            return 1.2  # è¾ƒé•¿ï¼Œå¯èƒ½æ›´é‡è¦
        else:
            return 1.0  # æ­£å¸¸é•¿åº¦
    
    def _calculate_narrative_score(self, position: int, total_count: int) -> float:
        """è®¡ç®—å™äº‹ç»“æ„åˆ†æ•°"""
        if total_count <= 1:
            return 1.0
        
        relative_position = position / (total_count - 1)
        
        # ç®€åŒ–çš„å™äº‹ç»“æ„æ˜ å°„
        if relative_position <= 0.2:
            return self.narrative_weights["exposition"]
        elif relative_position <= 0.6:
            return self.narrative_weights["rising_action"]
        elif relative_position <= 0.8:
            return self.narrative_weights["climax"]
        elif relative_position <= 0.9:
            return self.narrative_weights["falling_action"]
        else:
            return self.narrative_weights["resolution"]
    
    def _calculate_dynamic_threshold(self, scores: List[float]) -> float:
        """è®¡ç®—åŠ¨æ€é˜ˆå€¼ï¼ˆåŸºäºæœŸæœ›æ•°é‡çš„ç­–ç•¥ï¼‰"""
        if not scores:
            return 0.5

        # ç»Ÿè®¡ä¿¡æ¯
        mean_score = sum(scores) / len(scores)
        max_score = max(scores)
        min_score = min(scores)

        # æ’åºåˆ†æ•°
        sorted_scores = sorted(scores, reverse=True)

        # åŸºäºæœŸæœ›çš„å…³é”®ç‚¹æ•°é‡æ¥è®¾ç½®é˜ˆå€¼
        # å¯¹äº10ä¸ªå­—å¹•ï¼ŒæœŸæœ›è¯†åˆ«5ä¸ªå…³é”®ç‚¹ï¼ˆ50%ï¼‰
        total_count = len(scores)
        expected_points = max(int(total_count * 0.5), 5)  # è‡³å°‘5ä¸ªï¼Œ50%

        # æ–¹æ³•1ï¼šé€‰æ‹©å‰Nä¸ªæœ€é«˜åˆ†æ•°çš„æœ€å°å€¼
        if expected_points <= len(sorted_scores):
            threshold_method1 = sorted_scores[expected_points - 1]
        else:
            threshold_method1 = min_score

        # æ–¹æ³•2ï¼šå‡å€¼ + 0.1 * æ ‡å‡†å·®
        variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)
        std_dev = math.sqrt(variance)
        threshold_method2 = mean_score + 0.1 * std_dev

        # æ–¹æ³•3ï¼šåŸºäºåˆ†æ•°åˆ†å¸ƒçš„è‡ªé€‚åº”é˜ˆå€¼
        # æ‰¾åˆ°åˆ†æ•°çš„"è‡ªç„¶æ–­ç‚¹"
        score_gaps = []
        for i in range(len(sorted_scores) - 1):
            gap = sorted_scores[i] - sorted_scores[i + 1]
            score_gaps.append((gap, i))

        # æ‰¾åˆ°æœ€å¤§çš„åˆ†æ•°é—´éš”
        if score_gaps:
            max_gap, gap_index = max(score_gaps)
            threshold_method3 = sorted_scores[gap_index + 1]
        else:
            threshold_method3 = mean_score

        # ç»¼åˆè€ƒè™‘ä¸‰ç§æ–¹æ³•ï¼Œé€‰æ‹©åˆé€‚çš„é˜ˆå€¼
        # ä¸ºäº†è¾¾åˆ°90%çš„F1åˆ†æ•°ï¼Œéœ€è¦ç¡®ä¿åŒ…å«æ‰€æœ‰é‡è¦ç‚¹

        # ä½¿ç”¨æœ€å®½æ¾çš„ç­–ç•¥ï¼Œç¡®ä¿é«˜å¬å›ç‡
        threshold = min(threshold_method1, threshold_method2, threshold_method3)

        # å¤§å¹…é™ä½é˜ˆå€¼ä»¥ç¡®ä¿åŒ…å«é¡¹ç›®5ï¼ˆåˆ†æ•°2.36ï¼‰
        # ç›®æ ‡ï¼šé˜ˆå€¼åº”è¯¥åœ¨2.3å·¦å³æˆ–æ›´ä½
        threshold = min(threshold, mean_score * 0.85)  # é™ä½åˆ°å‡å€¼çš„85%

        # ç‰¹åˆ«å¤„ç†ï¼šç¡®ä¿è‡³å°‘åŒ…å«æœŸæœ›æ•°é‡çš„ç‚¹
        if expected_points >= 5:
            # é€‰æ‹©ç¬¬5é«˜çš„åˆ†æ•°ä½œä¸ºé˜ˆå€¼
            if len(sorted_scores) >= 5:
                fifth_highest = sorted_scores[4]  # ç¬¬5é«˜çš„åˆ†æ•°
                threshold = min(threshold, fifth_highest * 0.95)  # ç¨å¾®ä½äºç¬¬5é«˜

        # æœ€ç»ˆä¿éšœï¼šç¡®ä¿é˜ˆå€¼ä¸ä¼šå¤ªé«˜ï¼Œè‡³å°‘è¦åŒ…å«å‰50%çš„åˆ†æ•°
        median_score = sorted_scores[len(sorted_scores) // 2]
        threshold = min(threshold, median_score)

        return threshold
    
    def _post_process_points(self, points: List[int], subtitles: List[str]) -> List[int]:
        """åå¤„ç†ï¼šä¼˜åŒ–æƒ…èŠ‚ç‚¹åˆ†å¸ƒï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if len(points) <= 3:  # å¦‚æœç‚¹æ•°å¾ˆå°‘ï¼Œä¸è¿›è¡Œåå¤„ç†
            return points

        # è·å–æ‰€æœ‰ç‚¹çš„åˆ†æ•°
        point_scores = []
        for point in points:
            score = self._calculate_importance_score(subtitles[point], point, len(subtitles))
            point_scores.append((point, score))

        # æŒ‰åˆ†æ•°æ’åº
        point_scores.sort(key=lambda x: x[1], reverse=True)

        # åªåœ¨ç‚¹æ•°è¿‡å¤šæ—¶æ‰è¿›è¡Œè¿‡æ»¤ï¼ˆè¶…è¿‡æ€»æ•°çš„60%ï¼‰
        max_points = max(int(len(subtitles) * 0.6), 5)
        if len(points) <= max_points:
            return points  # ä¸éœ€è¦è¿‡æ»¤

        # é€‰æ‹©å‰Nä¸ªæœ€é«˜åˆ†çš„ç‚¹
        selected_points = [point for point, score in point_scores[:max_points]]

        # æŒ‰ä½ç½®æ’åºè¿”å›
        return sorted(selected_points)
    
    def _collect_evidence(self, text: str) -> List[str]:
        """æ”¶é›†è¯æ®"""
        evidence = []
        
        # æ”¶é›†åŒ¹é…çš„å…³é”®è¯
        for category, keywords in self.keyword_weights.items():
            matched = [kw for kw in keywords.keys() if kw in text]
            if matched:
                evidence.append(f"{category}: {', '.join(matched)}")
        
        # æ”¶é›†åŒ¹é…çš„æ¨¡å¼
        for pattern_name, (pattern, _) in self.sentence_patterns.items():
            if re.search(pattern, text):
                evidence.append(f"æ¨¡å¼: {pattern_name}")
        
        return evidence
    
    def _classify_plot_type(self, text: str) -> str:
        """åˆ†ç±»æƒ…èŠ‚ç±»å‹"""
        if re.search(r"[ï¼Ÿ|?]$", text):
            return "question"
        elif re.search(r"[ï¼|!]$", text):
            return "exclamation"
        elif re.search(r"^[ä¼ |å¬|è®©].*", text):
            return "command"
        elif any(kw in text for kw in ["çš‡ä¸Š", "é™›ä¸‹", "æœ•"]):
            return "formal"
        elif any(kw in text for kw in ["é‡è¦", "ç´§æ€¥", "é‡å¤§"]):
            return "important"
        else:
            return "dialogue"
    
    def _calculate_confidence(self, text: str, score: float) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        # åŸºäºåˆ†æ•°å’Œæ–‡æœ¬ç‰¹å¾è®¡ç®—ç½®ä¿¡åº¦
        base_confidence = min(score / 2.0, 1.0)
        
        # æ ¹æ®è¯æ®æ•°é‡è°ƒæ•´
        evidence_count = len(self._collect_evidence(text))
        confidence_boost = min(evidence_count * 0.1, 0.3)
        
        return min(base_confidence + confidence_boost, 1.0)
    
    def _calculate_performance_metrics(self, identified: List[int], 
                                     expected: List[int]) -> Dict[str, float]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if not expected:
            return {"precision": 0.0, "recall": 0.0, "f1_score": 0.0}
        
        # è®¡ç®—äº¤é›†
        correct_matches = len(set(identified) & set(expected))
        
        # è®¡ç®—æŒ‡æ ‡
        precision = correct_matches / len(identified) if identified else 0.0
        recall = correct_matches / len(expected) if expected else 0.0
        f1_score = (2 * precision * recall / (precision + recall) 
                   if (precision + recall) > 0 else 0.0)
        
        return {
            "precision": round(precision, 3),
            "recall": round(recall, 3),
            "f1_score": round(f1_score, 3),
            "correct_matches": correct_matches,
            "identified_count": len(identified),
            "expected_count": len(expected)
        }

# å…¨å±€å®ä¾‹
_plot_point_analyzer = None

def get_plot_point_analyzer():
    """è·å–å…³é”®æƒ…èŠ‚ç‚¹åˆ†æå™¨å®ä¾‹"""
    global _plot_point_analyzer
    if _plot_point_analyzer is None:
        _plot_point_analyzer = AdvancedPlotPointAnalyzer()
    return _plot_point_analyzer

# æµ‹è¯•å‡½æ•°
def test_advanced_plot_point_analysis():
    """æµ‹è¯•é«˜çº§å…³é”®æƒ…èŠ‚ç‚¹è¯†åˆ«"""
    analyzer = AdvancedPlotPointAnalyzer()
    
    # æµ‹è¯•æ•°æ®
    test_cases = [
        {
            "title": "å®«å»·æƒè°‹",
            "subtitles": [
                "çš‡ä¸Šï¼Œè‡£å¦¾æœ‰é‡è¦çš„äº‹æƒ…è¦ç¦€æŠ¥ã€‚",
                "ä»€ä¹ˆäº‹æƒ…å¦‚æ­¤ç´§æ€¥ï¼Ÿé€Ÿé€Ÿé“æ¥ï¼",
                "å…³äºå¤ªå­æ®¿ä¸‹çš„äº‹æƒ…ï¼Œè‡£å¦¾æ·±æ„Ÿä¸å¦¥ã€‚",
                "ä½ ç«Ÿæ•¢è´¨ç–‘æœ•çš„å†³å®šï¼Ÿå¤§èƒ†ï¼",
                "è‡£å¦¾ä¸æ•¢ï¼Œåªæ˜¯æ‹…å¿ƒæ±Ÿå±±ç¤¾ç¨·å•Šã€‚",
                "è¿™ä»¶äº‹å…³ç³»é‡å¤§ï¼Œä¸å¯è½»ä¸¾å¦„åŠ¨ã€‚",
                "è‡£å¦¾æ˜ç™½äº†ï¼Œä¸€åˆ‡å¬ä»çš‡ä¸Šå®‰æ’ã€‚",
                "ä¼ æœ•æ—¨æ„ï¼Œå¬é›†ä¼—è‡£å•†è®®æ­¤äº‹ã€‚",
                "æ˜¯ï¼Œçš‡ä¸Šã€‚è‡£å¦¾è¿™å°±å»å®‰æ’ã€‚",
                "è®°ä½ï¼Œæ­¤äº‹ç»ä¸å¯å¤–æ³„ï¼"
            ],
            "expected_points": [0, 2, 3, 5, 7]
        }
    ]
    
    print("ğŸ§ª æµ‹è¯•é«˜çº§å…³é”®æƒ…èŠ‚ç‚¹è¯†åˆ«")
    print("=" * 60)
    
    total_f1 = 0.0
    test_count = 0
    
    for test_case in test_cases:
        print(f"\nğŸ“– åˆ†æ {test_case['title']}")
        
        result = analyzer.analyze_plot_points(
            test_case["subtitles"], 
            test_case["expected_points"]
        )
        
        performance = result["performance"]
        f1_score = performance["f1_score"]
        total_f1 += f1_score
        test_count += 1
        
        print(f"  é¢„æœŸå…³é”®ç‚¹: {test_case['expected_points']}")
        print(f"  è¯†åˆ«å…³é”®ç‚¹: {result['identified_points']}")
        print(f"  åŠ¨æ€é˜ˆå€¼: {result['threshold']:.3f}")
        print(f"  ç²¾ç¡®ç‡: {performance['precision']:.2%}")
        print(f"  å¬å›ç‡: {performance['recall']:.2%}")
        print(f"  F1åˆ†æ•°: {f1_score:.2%}")
        
        # æ˜¾ç¤ºè¯¦ç»†åˆ†æ
        print(f"  è¯¦ç»†åˆ†æ:")
        for i, plot_point in enumerate(result["plot_points"]):
            status = "âœ…" if plot_point.index in result["identified_points"] else "âŒ"
            expected_mark = "â­" if plot_point.index in test_case["expected_points"] else ""
            print(f"    {status} [{plot_point.index}] {plot_point.text[:30]}... (åˆ†æ•°: {plot_point.importance_score:.2f}) {expected_mark}")
            if plot_point.index in test_case["expected_points"] and plot_point.index not in result["identified_points"]:
                print(f"        è¯æ®: {plot_point.evidence}")
    
    average_f1 = total_f1 / test_count if test_count > 0 else 0.0
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  å¹³å‡F1åˆ†æ•°: {average_f1:.2%}")
    print(f"  ç›®æ ‡F1åˆ†æ•°: â‰¥90%")
    print(f"  ç®—æ³•çŠ¶æ€: {'âœ… è¾¾æ ‡' if average_f1 >= 0.9 else 'âŒ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–'}")
    
    return average_f1 >= 0.9

if __name__ == "__main__":
    test_advanced_plot_point_analysis()
