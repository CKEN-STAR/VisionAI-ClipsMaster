#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
VisionAI-ClipsMaster æ™ºèƒ½æ¨¡å‹ç‰ˆæœ¬é€‰æ‹©å™¨
åŸºäºç¡¬ä»¶é…ç½®è‡ªåŠ¨æ¨èæœ€é€‚åˆçš„æ¨¡å‹ç‰ˆæœ¬
"""

import json
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from enum import Enum

from .quantization_analysis import (
    QuantizationAnalyzer, HardwareDetector, ModelVariant, 
    HardwareProfile, QuantizationType
)

logger = logging.getLogger(__name__)

class SelectionStrategy(Enum):
    """é€‰æ‹©ç­–ç•¥æšä¸¾"""
    AUTO_RECOMMEND = "auto_recommend"    # è‡ªåŠ¨æ¨è
    MANUAL_SELECT = "manual_select"      # æ‰‹åŠ¨é€‰æ‹©
    HYBRID_MODE = "hybrid_mode"          # æ··åˆæ¨¡å¼

class DeploymentTarget(Enum):
    """éƒ¨ç½²ç›®æ ‡æšä¸¾"""
    HIGH_PERFORMANCE = "high_performance"    # é«˜æ€§èƒ½
    BALANCED = "balanced"                    # å¹³è¡¡
    LIGHTWEIGHT = "lightweight"             # è½»é‡åŒ–
    ULTRA_LIGHT = "ultra_light"             # è¶…è½»é‡

@dataclass
class ModelRecommendation:
    """æ¨¡å‹æ¨èç»“æœ"""
    model_name: str
    variant: ModelVariant
    confidence_score: float
    reasoning: List[str]
    compatibility_assessment: Dict
    alternative_options: List[Dict]
    deployment_notes: List[str]

class IntelligentModelSelector:
    """æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨"""

    def __init__(self):
        self.analyzer = QuantizationAnalyzer()
        self.detector = HardwareDetector()
        self.selection_rules = self._initialize_selection_rules()
        self._last_recommendation = None  # ç¼“å­˜ä¸Šæ¬¡æ¨èç»“æœ
        self._last_model_name = None      # ç¼“å­˜ä¸Šæ¬¡è¯·æ±‚çš„æ¨¡å‹åç§°
        self._hardware_cache = None       # ç¼“å­˜ç¡¬ä»¶æ£€æµ‹ç»“æœ
        self._cache_timestamp = None      # ç¼“å­˜æ—¶é—´æˆ³

        # è´¨é‡é˜ˆå€¼é…ç½®
        self.quality_thresholds = {
            "high": 0.95,
            "medium": 0.85,
            "low": 0.75,
            "minimal": 0.65
        }

    def clear_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜çŠ¶æ€ï¼Œç¡®ä¿çŠ¶æ€éš”ç¦»"""
        logger.info("ğŸ”§ æ¸…é™¤æ™ºèƒ½é€‰æ‹©å™¨ç¼“å­˜çŠ¶æ€")
        self._last_recommendation = None
        self._last_model_name = None
        self._hardware_cache = None
        self._cache_timestamp = None

        # å¼ºåŒ–æ¸…é™¤ï¼šåˆ é™¤æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€æ±¡æŸ“æº
        if hasattr(self, '_cached_variants'):
            delattr(self, '_cached_variants')
        if hasattr(self, '_last_strategy'):
            delattr(self, '_last_strategy')
        if hasattr(self, '_recommendation_cache'):
            delattr(self, '_recommendation_cache')

        logger.info("âœ… æ™ºèƒ½é€‰æ‹©å™¨çŠ¶æ€å·²æ¸…é™¤")
    
    def _initialize_selection_rules(self) -> Dict:
        """åˆå§‹åŒ–é€‰æ‹©è§„åˆ™"""
        return {
            "memory_thresholds": {
                "ultra_light": 4.0,      # 4GBä»¥ä¸‹
                "lightweight": 8.0,      # 4-8GB
                "balanced": 16.0,        # 8-16GB
                "high_performance": 32.0  # 16GB+
            },
            "storage_thresholds": {
                "ultra_light": 10.0,     # 10GBä»¥ä¸‹
                "lightweight": 20.0,     # 10-20GB
                "balanced": 50.0,        # 20-50GB
                "high_performance": 100.0 # 50GB+
            },
            "quality_requirements": {
                "research": 0.95,        # ç ”ç©¶ç”¨é€”
                "production": 0.90,      # ç”Ÿäº§ç¯å¢ƒ
                "development": 0.85,     # å¼€å‘æµ‹è¯•
                "demo": 0.80            # æ¼”ç¤ºç”¨é€”
            }
        }
    
    def recommend_model_version(
        self,
        model_name: str,
        strategy: SelectionStrategy = SelectionStrategy.AUTO_RECOMMEND,
        deployment_target: Optional[DeploymentTarget] = None,
        quality_requirement: str = "production",
        hardware_override: Optional[HardwareProfile] = None
    ) -> ModelRecommendation:
        """æ¨èæ¨¡å‹ç‰ˆæœ¬"""

        logger.info(f"ğŸ¤– å¼€å§‹æ¨èæ¨¡å‹ç‰ˆæœ¬: {model_name}")

        # é‡è¦ä¿®å¤ï¼šæ£€æŸ¥æ¨¡å‹åç§°å˜åŒ–ï¼Œå¦‚æœå˜åŒ–åˆ™æ¸…é™¤ç¼“å­˜
        if self._last_model_name and self._last_model_name != model_name:
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹åç§°å˜åŒ–: {self._last_model_name} -> {model_name}ï¼Œæ¸…é™¤ç¼“å­˜")
            self.clear_cache()

        # è®°å½•å½“å‰æ¨¡å‹åç§°
        self._last_model_name = model_name

        # é¢å¤–éªŒè¯ï¼šç¡®ä¿è¯·æ±‚çš„æ¨¡å‹åç§°æœ‰æ•ˆ
        if model_name not in ["mistral-7b", "qwen2.5-7b"]:
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°éæ ‡å‡†æ¨¡å‹åç§°: {model_name}")
            # æ ‡å‡†åŒ–æ¨¡å‹åç§°
            if "mistral" in model_name.lower():
                model_name = "mistral-7b"
                logger.info(f"ğŸ”„ æ ‡å‡†åŒ–ä¸ºè‹±æ–‡æ¨¡å‹: {model_name}")
            elif "qwen" in model_name.lower():
                model_name = "qwen2.5-7b"
                logger.info(f"ğŸ”„ æ ‡å‡†åŒ–ä¸ºä¸­æ–‡æ¨¡å‹: {model_name}")
            else:
                logger.error(f"âŒ æ— æ³•è¯†åˆ«çš„æ¨¡å‹åç§°: {model_name}")
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")

        # æ›´æ–°æ ‡å‡†åŒ–åçš„æ¨¡å‹åç§°
        self._last_model_name = model_name

        # æ£€æµ‹ç¡¬ä»¶é…ç½®ï¼ˆä½¿ç”¨ç¼“å­˜æœºåˆ¶ä¼˜åŒ–æ€§èƒ½ï¼‰
        hardware = hardware_override or self._get_hardware_with_cache()

        # éªŒè¯æ¨¡å‹åç§°æœ‰æ•ˆæ€§
        if model_name not in self.analyzer.model_variants:
            logger.error(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")

        variants = self.analyzer.model_variants[model_name]
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(variants)} ä¸ªæ¨¡å‹å˜ä½“: {[v.name for v in variants]}")

        # æ ¹æ®ç­–ç•¥é€‰æ‹©
        recommendation = None
        if strategy == SelectionStrategy.AUTO_RECOMMEND:
            recommendation = self._auto_recommend(model_name, variants, hardware, deployment_target, quality_requirement)
        elif strategy == SelectionStrategy.MANUAL_SELECT:
            recommendation = self._manual_select_options(model_name, variants, hardware)
        else:  # HYBRID_MODE
            recommendation = self._hybrid_recommend(model_name, variants, hardware, deployment_target, quality_requirement)

        # éªŒè¯æ¨èç»“æœçš„ä¸€è‡´æ€§
        if recommendation and recommendation.model_name != model_name:
            logger.error(f"âŒ æ¨èç»“æœæ¨¡å‹åç§°ä¸ä¸€è‡´: è¯·æ±‚={model_name}, æ¨è={recommendation.model_name}")
            # å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ­£ç¡®çš„æ¨è
            self.clear_cache()
            recommendation = self._auto_recommend(model_name, variants, hardware, deployment_target, quality_requirement)

        # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ¨èå†…å®¹ä¸æ¨¡å‹åç§°ä¸¥æ ¼åŒ¹é…
        if recommendation:
            variant_name = recommendation.variant.name.lower()
            if model_name == "mistral-7b" and "mistral" not in variant_name:
                logger.error(f"âŒ è‹±æ–‡æ¨¡å‹æ¨èé”™è¯¯: è¯·æ±‚=mistral-7b, æ¨èå˜ä½“={recommendation.variant.name}")
                raise ValueError(f"æ¨èç»“æœä¸è¯·æ±‚æ¨¡å‹ä¸åŒ¹é…: {model_name} vs {recommendation.variant.name}")
            elif model_name == "qwen2.5-7b" and "qwen" not in variant_name:
                logger.error(f"âŒ ä¸­æ–‡æ¨¡å‹æ¨èé”™è¯¯: è¯·æ±‚=qwen2.5-7b, æ¨èå˜ä½“={recommendation.variant.name}")
                raise ValueError(f"æ¨èç»“æœä¸è¯·æ±‚æ¨¡å‹ä¸åŒ¹é…: {model_name} vs {recommendation.variant.name}")

            logger.info(f"âœ… æ¨èç»“æœéªŒè¯é€šè¿‡: {model_name} -> {recommendation.variant.name}")

        # ç¼“å­˜æ¨èç»“æœ
        self._last_recommendation = recommendation

        logger.info(f"âœ… æ¨èå®Œæˆ: {recommendation.variant.name if recommendation else 'None'}")
        return recommendation

    def _get_hardware_with_cache(self) -> HardwareProfile:
        """è·å–ç¡¬ä»¶é…ç½®ï¼ˆå¸¦ç¼“å­˜æœºåˆ¶ï¼‰"""
        import time
        current_time = time.time()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ·æ–°ç¡¬ä»¶é…ç½®
        force_refresh = self._should_force_refresh_hardware()

        # å¦‚æœç¼“å­˜å­˜åœ¨ä¸”æœªè¿‡æœŸï¼ˆ5åˆ†é’Ÿå†…ï¼‰ä¸”ä¸éœ€è¦å¼ºåˆ¶åˆ·æ–°ï¼Œä½¿ç”¨ç¼“å­˜
        if (not force_refresh and self._hardware_cache and self._cache_timestamp and
            current_time - self._cache_timestamp < 300):  # 5åˆ†é’Ÿç¼“å­˜
            logger.debug("ğŸ”„ ä½¿ç”¨ç¼“å­˜çš„ç¡¬ä»¶é…ç½®")
            return self._hardware_cache

        # é‡æ–°æ£€æµ‹ç¡¬ä»¶é…ç½®
        if force_refresh:
            logger.info("ğŸ”„ å¼ºåˆ¶åˆ·æ–°ç¡¬ä»¶é…ç½®ï¼ˆæ£€æµ‹åˆ°è®¾å¤‡å˜åŒ–ï¼‰")
        else:
            logger.info("ğŸ” é‡æ–°æ£€æµ‹ç¡¬ä»¶é…ç½®")

        hardware = self.detector.detect_hardware()

        # æ£€æµ‹ç¡¬ä»¶é…ç½®æ˜¯å¦å‘ç”Ÿé‡å¤§å˜åŒ–
        if self._hardware_cache:
            self._log_hardware_changes(self._hardware_cache, hardware)

        # æ›´æ–°ç¼“å­˜
        self._hardware_cache = hardware
        self._cache_timestamp = current_time

        logger.info(f"ğŸ’¾ ç¡¬ä»¶é…ç½®å·²ç¼“å­˜: GPU={hardware.gpu_memory_gb}GB, RAM={hardware.system_ram_gb}GB")
        return hardware

    def _should_force_refresh_hardware(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ·æ–°ç¡¬ä»¶é…ç½®"""
        try:
            # æ£€æŸ¥GPUçŠ¶æ€å˜åŒ–çš„å¿«é€ŸæŒ‡æ ‡
            import torch

            # å¦‚æœä¹‹å‰æ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦æ£€æµ‹
            if not self._hardware_cache:
                return True

            # æ£€æŸ¥CUDAå¯ç”¨æ€§æ˜¯å¦å‘ç”Ÿå˜åŒ–
            cuda_available_now = torch.cuda.is_available() if hasattr(torch, 'cuda') else False
            cuda_was_available = self._hardware_cache.gpu_memory_gb > 0

            if cuda_available_now != cuda_was_available:
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°CUDAçŠ¶æ€å˜åŒ–: {cuda_was_available} -> {cuda_available_now}")
                return True

            # æ£€æŸ¥GPUæ•°é‡æ˜¯å¦å‘ç”Ÿå˜åŒ–
            if cuda_available_now:
                current_gpu_count = torch.cuda.device_count()
                cached_gpu_count = getattr(self._hardware_cache, 'gpu_count', 0)
                if current_gpu_count != cached_gpu_count:
                    logger.info(f"ğŸ”„ æ£€æµ‹åˆ°GPUæ•°é‡å˜åŒ–: {cached_gpu_count} -> {current_gpu_count}")
                    return True

            return False

        except Exception as e:
            logger.debug(f"ç¡¬ä»¶å˜åŒ–æ£€æµ‹å¤±è´¥ï¼Œå¼ºåˆ¶åˆ·æ–°: {e}")
            return True

    def _log_hardware_changes(self, old_hardware: HardwareProfile, new_hardware: HardwareProfile):
        """è®°å½•ç¡¬ä»¶é…ç½®å˜åŒ–"""
        try:
            changes = []

            # æ£€æŸ¥GPUå˜åŒ–
            if old_hardware.gpu_memory_gb != new_hardware.gpu_memory_gb:
                changes.append(f"GPUæ˜¾å­˜: {old_hardware.gpu_memory_gb:.1f}GB -> {new_hardware.gpu_memory_gb:.1f}GB")

            # æ£€æŸ¥å†…å­˜å˜åŒ–
            if abs(old_hardware.system_ram_gb - new_hardware.system_ram_gb) > 0.5:
                changes.append(f"ç³»ç»Ÿå†…å­˜: {old_hardware.system_ram_gb:.1f}GB -> {new_hardware.system_ram_gb:.1f}GB")

            # æ£€æŸ¥æ€§èƒ½ç­‰çº§å˜åŒ–
            if hasattr(old_hardware, 'performance_level') and hasattr(new_hardware, 'performance_level'):
                if old_hardware.performance_level != new_hardware.performance_level:
                    changes.append(f"æ€§èƒ½ç­‰çº§: {old_hardware.performance_level.value} -> {new_hardware.performance_level.value}")

            if changes:
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°ç¡¬ä»¶é…ç½®å˜åŒ–: {'; '.join(changes)}")
            else:
                logger.debug("ç¡¬ä»¶é…ç½®æ— é‡å¤§å˜åŒ–")

        except Exception as e:
            logger.debug(f"ç¡¬ä»¶å˜åŒ–è®°å½•å¤±è´¥: {e}")

    def force_refresh_hardware(self):
        """å¼ºåˆ¶åˆ·æ–°ç¡¬ä»¶é…ç½®ï¼ˆå…¬å…±æ¥å£ï¼‰"""
        logger.info("ğŸ”„ å¼ºåˆ¶åˆ·æ–°ç¡¬ä»¶é…ç½®")
        self._hardware_cache = None
        self._cache_timestamp = None
    
    def _auto_recommend(
        self, 
        model_name: str, 
        variants: List[ModelVariant], 
        hardware: HardwareProfile,
        deployment_target: Optional[DeploymentTarget],
        quality_requirement: str
    ) -> ModelRecommendation:
        """è‡ªåŠ¨æ¨èæœ€ä½³ç‰ˆæœ¬"""
        
        # ç¡®å®šéƒ¨ç½²ç›®æ ‡
        if deployment_target is None:
            deployment_target = self._infer_deployment_target(hardware)
        
        # è¯„ä¼°æ‰€æœ‰å˜ä½“
        scored_variants = []
        for variant in variants:
            score = self._calculate_variant_score(variant, hardware, deployment_target, quality_requirement)
            compatibility = self.detector.assess_compatibility(hardware, variant)
            
            scored_variants.append({
                "variant": variant,
                "score": score,
                "compatibility": compatibility
            })
        
        # æŒ‰è¯„åˆ†æ’åº
        scored_variants.sort(key=lambda x: x["score"], reverse=True)
        best_variant = scored_variants[0]
        
        # ç”Ÿæˆæ¨èç†ç”±
        reasoning = self._generate_reasoning(
            best_variant["variant"], 
            hardware, 
            deployment_target, 
            quality_requirement,
            best_variant["compatibility"]
        )
        
        # ç”Ÿæˆæ›¿ä»£é€‰é¡¹
        alternatives = [
            {
                "variant": sv["variant"],
                "score": sv["score"],
                "reason": self._get_alternative_reason(sv["variant"], best_variant["variant"])
            }
            for sv in scored_variants[1:3]  # å–å‰2ä¸ªæ›¿ä»£é€‰é¡¹
        ]
        
        # ç”Ÿæˆéƒ¨ç½²è¯´æ˜
        deployment_notes = self._generate_deployment_notes(best_variant["variant"], hardware)
        
        return ModelRecommendation(
            model_name=model_name,
            variant=best_variant["variant"],
            confidence_score=best_variant["score"],
            reasoning=reasoning,
            compatibility_assessment=best_variant["compatibility"],
            alternative_options=alternatives,
            deployment_notes=deployment_notes
        )
    
    def _calculate_variant_score(
        self,
        variant: ModelVariant,
        hardware: HardwareProfile,
        deployment_target: DeploymentTarget,
        quality_requirement: str
    ) -> float:
        """è®¡ç®—å˜ä½“è¯„åˆ†ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼Œä¸ç¡¬ä»¶æ£€æµ‹å™¨æ¨èé€»è¾‘ä¿æŒä¸€è‡´ï¼‰"""
        score = 0.0

        # è·å–ç¡¬ä»¶ä¿¡æ¯
        memory_gb = getattr(hardware, 'memory_gb', getattr(hardware, 'system_ram_gb', 0))
        gpu_available = getattr(hardware, 'gpu_available', getattr(hardware, 'has_gpu', False))
        gpu_type = getattr(hardware, 'gpu_type', 'Unknown')
        cpu_cores = getattr(hardware, 'cpu_cores', getattr(hardware, 'cpu_count', 0))

        # 1. å†…å­˜é€‚é…æ€§è¯„åˆ† (0-40åˆ†)
        memory_requirement = variant.size_gb * 1.5  # è€ƒè™‘è¿è¡Œæ—¶å†…å­˜å¼€é”€
        if memory_requirement <= memory_gb * 0.6:  # ä½¿ç”¨60%ä»¥ä¸‹å†…å­˜
            score += 40
        elif memory_requirement <= memory_gb * 0.8:  # ä½¿ç”¨80%ä»¥ä¸‹å†…å­˜
            score += 30
        elif memory_requirement <= memory_gb:  # åˆšå¥½å¤Ÿç”¨
            score += 20
        else:  # å†…å­˜ä¸è¶³
            score += 0

        # 2. è®¾å¤‡ç±»å‹é€‚é…æ€§è¯„åˆ† (0-30åˆ†)
        quantization = variant.quantization
        if memory_gb < 8:  # ä½å†…å­˜è®¾å¤‡
            if quantization in ['Q2_K', 'Q4_K_M']:
                score += 30
            elif quantization in ['Q5_K_M']:
                score += 15
            else:
                score += 0
        elif memory_gb < 16:  # ä¸­ç­‰å†…å­˜è®¾å¤‡
            if gpu_available and gpu_type in ['CUDA', 'NVIDIA']:
                if quantization in ['Q5_K_M', 'Q8_0']:
                    score += 30
                elif quantization in ['Q4_K_M']:
                    score += 25
                else:
                    score += 10
            else:
                if quantization in ['Q4_K_M', 'Q5_K_M']:
                    score += 30
                else:
                    score += 15
        elif memory_gb < 32:  # é«˜å†…å­˜è®¾å¤‡
            if gpu_available and gpu_type in ['CUDA', 'NVIDIA']:
                if quantization in ['Q8_0', 'FP16']:
                    score += 30
                else:
                    score += 20
            else:
                if quantization in ['Q5_K_M', 'Q8_0']:
                    score += 30
                else:
                    score += 20
        else:  # è¶…é«˜æ€§èƒ½è®¾å¤‡
            if gpu_available and gpu_type in ['CUDA', 'NVIDIA']:
                if quantization == 'FP16':
                    score += 30
                elif quantization == 'Q8_0':
                    score += 25
                else:
                    score += 15
            else:
                if quantization in ['Q8_0', 'FP16']:
                    score += 30
                else:
                    score += 20
        # 3. è´¨é‡è¦æ±‚åŒ¹é…è¯„åˆ† (0-20åˆ†)
        quality_threshold = self.quality_thresholds.get(quality_requirement, 0.85)
        if variant.quality_retention >= quality_threshold:
            score += 20
        elif variant.quality_retention >= quality_threshold - 0.1:
            score += 15
        else:
            score += 5

        # 4. éƒ¨ç½²ç›®æ ‡é€‚é…æ€§è¯„åˆ† (0-10åˆ†)
        if deployment_target:
            if deployment_target.value == 'production' and variant.quality_retention >= 0.9:
                score += 10
            elif deployment_target.value == 'development' and variant.quantization in ['Q4_K_M', 'Q5_K_M']:
                score += 10
            elif deployment_target.value == 'demo' and variant.size_gb <= 5:
                score += 10
            else:
                score += 5

        # å…¼å®¹æ€§è¯„åˆ† (25%)
        compatibility = self.detector.assess_compatibility(hardware, variant)
        score += compatibility["compatibility_score"] * 0.25

        # è´¨é‡è¦æ±‚è¯„åˆ† (15%)
        required_quality = self.selection_rules["quality_requirements"].get(quality_requirement, 0.90)
        if variant.quality_retention >= required_quality:
            quality_score = variant.quality_retention
        else:
            quality_score = variant.quality_retention * 0.7  # è½»å¾®æƒ©ç½š
        score += quality_score * 0.15

        # èµ„æºæ•ˆç‡è¯„åˆ† (10%)
        # ä¼˜å…ˆé€‰æ‹©èµ„æºå ç”¨åˆç†çš„ç‰ˆæœ¬
        system_memory = getattr(hardware, 'system_ram_gb', getattr(hardware, 'total_memory_gb', memory_gb))
        memory_efficiency = max(0, 1 - variant.memory_requirement_gb / max(system_memory * 0.6, 4.0))
        score += memory_efficiency * 0.1

        return score
    
    def _calculate_target_alignment_score(
        self, 
        variant: ModelVariant, 
        hardware: HardwareProfile,
        deployment_target: DeploymentTarget
    ) -> float:
        """è®¡ç®—ä¸éƒ¨ç½²ç›®æ ‡çš„å¯¹é½è¯„åˆ†"""
        if deployment_target == DeploymentTarget.HIGH_PERFORMANCE:
            # é«˜æ€§èƒ½ï¼šä¼˜å…ˆè€ƒè™‘è´¨é‡å’Œé€Ÿåº¦
            return variant.quality_retention * 0.6 + variant.inference_speed_factor * 0.4
        
        elif deployment_target == DeploymentTarget.BALANCED:
            # å¹³è¡¡ï¼šè´¨é‡ã€é€Ÿåº¦ã€å¤§å°å‡è¡¡
            size_score = max(0, 1 - variant.size_gb / 20.0)  # 20GBä¸ºåŸºå‡†
            return (variant.quality_retention * 0.4 + 
                   variant.inference_speed_factor * 0.3 + 
                   size_score * 0.3)
        
        elif deployment_target == DeploymentTarget.LIGHTWEIGHT:
            # è½»é‡åŒ–ï¼šä¼˜å…ˆè€ƒè™‘å¤§å°å’Œå†…å­˜
            size_score = max(0, 1 - variant.size_gb / 15.0)
            memory_score = max(0, 1 - variant.memory_requirement_gb / 10.0)
            return size_score * 0.5 + memory_score * 0.3 + variant.quality_retention * 0.2
        
        else:  # ULTRA_LIGHT
            # è¶…è½»é‡ï¼šæœ€å°åŒ–èµ„æºå ç”¨
            size_score = max(0, 1 - variant.size_gb / 10.0)
            memory_score = max(0, 1 - variant.memory_requirement_gb / 8.0)
            cpu_compat_score = 1.0 if variant.cpu_compatible else 0.0
            return size_score * 0.4 + memory_score * 0.4 + cpu_compat_score * 0.2
    
    def _infer_deployment_target(self, hardware: HardwareProfile) -> DeploymentTarget:
        """æ ¹æ®ç¡¬ä»¶æ¨æ–­éƒ¨ç½²ç›®æ ‡ï¼ˆä¸ç¡¬ä»¶æ£€æµ‹å™¨ä¿æŒä¸€è‡´ï¼‰"""
        memory_thresholds = self.selection_rules["memory_thresholds"]

        # è·å–GPUä¿¡æ¯
        gpu_memory = hardware.gpu_memory_gb if hasattr(hardware, 'gpu_memory_gb') else 0
        gpu_type = getattr(hardware, 'gpu_type', None)
        system_memory = hardware.system_ram_gb if hasattr(hardware, 'system_ram_gb') else hardware.total_memory_gb

        # ä¸ç¡¬ä»¶æ£€æµ‹å™¨ä¿æŒä¸€è‡´çš„æ¨æ–­é€»è¾‘
        if gpu_type and hasattr(gpu_type, 'value') and gpu_type.value == 'nvidia':
            # NVIDIAç‹¬æ˜¾è®¾å¤‡
            if gpu_memory >= 16:
                return DeploymentTarget.HIGH_PERFORMANCE
            elif gpu_memory >= 8:
                return DeploymentTarget.BALANCED
            else:
                return DeploymentTarget.LIGHTWEIGHT
        elif gpu_type and hasattr(gpu_type, 'value') and gpu_type.value == 'intel':
            # é›†æˆæ˜¾å¡è®¾å¤‡ï¼šæœ€é«˜åªèƒ½æ˜¯LIGHTWEIGHT
            if system_memory >= 16:
                return DeploymentTarget.LIGHTWEIGHT
            else:
                return DeploymentTarget.ULTRA_LIGHT
        else:
            # æ— GPUè®¾å¤‡ï¼šæ ¹æ®ç³»ç»Ÿå†…å­˜å†³å®š
            if system_memory >= 16:
                return DeploymentTarget.LIGHTWEIGHT
            elif system_memory >= 8:
                return DeploymentTarget.ULTRA_LIGHT
            else:
                return DeploymentTarget.ULTRA_LIGHT
    
    def _generate_reasoning(
        self, 
        variant: ModelVariant, 
        hardware: HardwareProfile,
        deployment_target: DeploymentTarget,
        quality_requirement: str,
        compatibility: Dict
    ) -> List[str]:
        """ç”Ÿæˆæ¨èç†ç”±"""
        reasons = []
        
        # ç¡¬ä»¶é€‚é…ç†ç”±
        if compatibility["is_compatible"]:
            reasons.append(f"âœ… ä¸å½“å‰ç¡¬ä»¶å®Œå…¨å…¼å®¹ (å…¼å®¹æ€§è¯„åˆ†: {compatibility['compatibility_score']:.1%})")
        else:
            reasons.append(f"âš ï¸ ç¡¬ä»¶å…¼å®¹æ€§æœ‰é™ï¼Œä½†å¯è¿è¡Œ (è¯„åˆ†: {compatibility['compatibility_score']:.1%})")
        
        # éƒ¨ç½²ç›®æ ‡ç†ç”±
        target_reasons = {
            DeploymentTarget.HIGH_PERFORMANCE: "è¿½æ±‚æœ€ä½³æ€§èƒ½å’Œè´¨é‡",
            DeploymentTarget.BALANCED: "åœ¨æ€§èƒ½ã€è´¨é‡å’Œèµ„æºå ç”¨é—´å–å¾—å¹³è¡¡",
            DeploymentTarget.LIGHTWEIGHT: "ä¼˜åŒ–èµ„æºå ç”¨ï¼Œé€‚åˆè½»é‡åŒ–éƒ¨ç½²",
            DeploymentTarget.ULTRA_LIGHT: "æœ€å°åŒ–èµ„æºéœ€æ±‚ï¼Œé€‚åˆä½é…ç½®è®¾å¤‡"
        }
        reasons.append(f"ğŸ¯ {target_reasons[deployment_target]}")
        
        # è´¨é‡ä¿è¯ç†ç”±
        required_quality = self.selection_rules["quality_requirements"].get(quality_requirement, 0.90)
        if variant.quality_retention >= required_quality:
            reasons.append(f"âœ… æ»¡è¶³{quality_requirement}è´¨é‡è¦æ±‚ (ä¿æŒ{variant.quality_retention:.1%}è´¨é‡)")
        else:
            reasons.append(f"âš ï¸ è´¨é‡ç•¥ä½äº{quality_requirement}è¦æ±‚ï¼Œä½†ä»å¯æ¥å—")
        
        # èµ„æºä¼˜åŒ–ç†ç”±
        if variant.size_gb < 10:
            reasons.append(f"ğŸ’¾ å­˜å‚¨å ç”¨è¾ƒå° ({variant.size_gb:.1f}GB)")
        if variant.cpu_compatible:
            reasons.append("ğŸ–¥ï¸ æ”¯æŒCPUæ¨ç†ï¼Œæ— éœ€GPU")
        
        return reasons
    
    def _get_alternative_reason(self, alternative: ModelVariant, recommended: ModelVariant) -> str:
        """è·å–æ›¿ä»£é€‰é¡¹çš„ç†ç”±"""
        if alternative.quality_retention > recommended.quality_retention:
            return f"æ›´é«˜è´¨é‡ ({alternative.quality_retention:.1%} vs {recommended.quality_retention:.1%})"
        elif alternative.size_gb < recommended.size_gb:
            return f"æ›´å°ä½“ç§¯ ({alternative.size_gb:.1f}GB vs {recommended.size_gb:.1f}GB)"
        elif alternative.inference_speed_factor > recommended.inference_speed_factor:
            return f"æ›´å¿«æ¨ç† ({alternative.inference_speed_factor:.1%} vs {recommended.inference_speed_factor:.1%})"
        else:
            return "ä¸åŒçš„æ€§èƒ½æƒè¡¡"
    
    def _generate_deployment_notes(self, variant: ModelVariant, hardware: HardwareProfile) -> List[str]:
        """ç”Ÿæˆéƒ¨ç½²è¯´æ˜"""
        notes = []
        
        # æ¨ç†æ¨¡å¼è¯´æ˜
        if hardware.has_gpu and hardware.gpu_memory_gb >= variant.gpu_memory_min_gb:
            notes.append("ğŸš€ æ¨èä½¿ç”¨GPUæ¨ç†ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        elif variant.cpu_compatible:
            notes.append("ğŸ–¥ï¸ å°†ä½¿ç”¨CPUæ¨ç†ï¼Œé€Ÿåº¦è¾ƒæ…¢ä½†å…¼å®¹æ€§å¥½")
        else:
            notes.append("âš ï¸ éœ€è¦GPUæ”¯æŒï¼Œè¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜")
        
        # å†…å­˜ä½¿ç”¨è¯´æ˜
        notes.append(f"ğŸ’¾ é¢„è®¡å†…å­˜ä½¿ç”¨: {variant.memory_requirement_gb:.1f}GB")
        
        # æ€§èƒ½é¢„æœŸè¯´æ˜
        performance = hardware.gpu_memory_gb if hardware.has_gpu else hardware.system_ram_gb
        if performance >= variant.memory_requirement_gb * 1.5:
            notes.append("âš¡ é¢„æœŸæ€§èƒ½: ä¼˜ç§€")
        elif performance >= variant.memory_requirement_gb:
            notes.append("âš¡ é¢„æœŸæ€§èƒ½: è‰¯å¥½")
        else:
            notes.append("âš¡ é¢„æœŸæ€§èƒ½: ä¸€èˆ¬ï¼Œå¯èƒ½è¾ƒæ…¢")
        
        # VisionAIç‰¹å®šè¯´æ˜
        notes.append("ğŸ“ å­—å¹•é‡æ„å‡†ç¡®ç‡: " + self._get_accuracy_description(variant.quality_retention))
        notes.append("ğŸ­ å‰§æœ¬åˆ†æè´¨é‡: " + self._get_quality_description(variant.quality_retention))
        
        return notes
    
    def _get_accuracy_description(self, quality_retention: float) -> str:
        """è·å–å‡†ç¡®ç‡æè¿°"""
        if quality_retention >= 0.95:
            return "ä¼˜ç§€ (>95%)"
        elif quality_retention >= 0.90:
            return "è‰¯å¥½ (90-95%)"
        elif quality_retention >= 0.85:
            return "å¯æ¥å— (85-90%)"
        else:
            return "ä¸€èˆ¬ (<85%)"
    
    def _get_quality_description(self, quality_retention: float) -> str:
        """è·å–è´¨é‡æè¿°"""
        if quality_retention >= 0.95:
            return "æ¥è¿‘åŸå§‹è´¨é‡"
        elif quality_retention >= 0.90:
            return "é«˜è´¨é‡è¾“å‡º"
        elif quality_retention >= 0.85:
            return "ä¸­ç­‰è´¨é‡è¾“å‡º"
        else:
            return "åŸºç¡€è´¨é‡è¾“å‡º"
    
    def _manual_select_options(self, model_name: str, variants: List[ModelVariant], hardware: HardwareProfile) -> ModelRecommendation:
        """æä¾›æ‰‹åŠ¨é€‰æ‹©é€‰é¡¹"""
        # ä¸ºæ‰‹åŠ¨é€‰æ‹©æ¨¡å¼ï¼Œè¿”å›æ‰€æœ‰é€‰é¡¹çš„è¯¦ç»†ä¿¡æ¯
        options = []
        for variant in variants:
            compatibility = self.detector.assess_compatibility(hardware, variant)
            options.append({
                "variant": variant,
                "compatibility": compatibility,
                "description": self._get_variant_description(variant)
            })
        
        # è¿”å›ç¬¬ä¸€ä¸ªä½œä¸ºé»˜è®¤é€‰æ‹©ï¼Œä½†æä¾›æ‰€æœ‰é€‰é¡¹
        return ModelRecommendation(
            model_name=model_name,
            variant=variants[0],
            confidence_score=0.0,  # æ‰‹åŠ¨é€‰æ‹©ä¸æä¾›ç½®ä¿¡åº¦
            reasoning=["ç”¨æˆ·æ‰‹åŠ¨é€‰æ‹©æ¨¡å¼"],
            compatibility_assessment={},
            alternative_options=options,
            deployment_notes=["è¯·æ ¹æ®éœ€æ±‚æ‰‹åŠ¨é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬"]
        )
    
    def _get_variant_description(self, variant: ModelVariant) -> str:
        """è·å–å˜ä½“æè¿°"""
        return (f"{variant.name} - {variant.size_gb:.1f}GB, "
                f"è´¨é‡ä¿æŒ{variant.quality_retention:.1%}, "
                f"é€Ÿåº¦{variant.inference_speed_factor:.1%}, "
                f"{'æ”¯æŒCPU' if variant.cpu_compatible else 'éœ€è¦GPU'}")
    
    def _hybrid_recommend(self, model_name: str, variants: List[ModelVariant], hardware: HardwareProfile, deployment_target: Optional[DeploymentTarget], quality_requirement: str) -> ModelRecommendation:
        """æ··åˆæ¨¡å¼æ¨è"""
        # å…ˆè‡ªåŠ¨æ¨è
        auto_recommendation = self._auto_recommend(model_name, variants, hardware, deployment_target, quality_requirement)
        
        # ç„¶åæä¾›æ‰‹åŠ¨é€‰æ‹©é€‰é¡¹
        manual_options = self._manual_select_options(model_name, variants, hardware)
        
        # åˆå¹¶ç»“æœ
        auto_recommendation.alternative_options.extend(manual_options.alternative_options)
        auto_recommendation.reasoning.append("ğŸ’¡ æä¾›è‡ªåŠ¨æ¨èå’Œæ‰‹åŠ¨é€‰æ‹©é€‰é¡¹")
        
        return auto_recommendation

def create_multi_tier_download_config() -> Dict:
    """åˆ›å»ºå¤šå±‚çº§ä¸‹è½½é…ç½®"""
    return {
        "qwen2.5-7b": {
            "fp16": {
                "name": "Qwen2.5-7B-Instruct-FP16",
                "size_gb": 14.4,
                "urls": ["https://modelscope.cn/models/qwen/Qwen2.5-7B-Instruct"],
                "target_dir": "models/models/qwen/fp16"
            },
            "q8": {
                "name": "Qwen2.5-7B-Instruct-Q8",
                "size_gb": 7.6,
                "urls": ["https://modelscope.cn/models/qwen/Qwen2.5-7B-Instruct-GGUF"],
                "target_dir": "models/models/qwen/q8",
                "filename": "qwen2.5-7b-instruct-q8_0.gguf"
            },
            "q5": {
                "name": "Qwen2.5-7B-Instruct-Q5",
                "size_gb": 5.1,
                "urls": ["https://modelscope.cn/models/qwen/Qwen2.5-7B-Instruct-GGUF"],
                "target_dir": "models/models/qwen/q5",
                "filename": "qwen2.5-7b-instruct-q5_k_m.gguf"
            },
            "q4": {
                "name": "Qwen2.5-7B-Instruct-Q4",
                "size_gb": 4.1,
                "urls": ["https://modelscope.cn/models/qwen/Qwen2.5-7B-Instruct-GGUF"],
                "target_dir": "models/models/qwen/q4",
                "filename": "qwen2.5-7b-instruct-q4_k_m.gguf"
            }
        },
        "mistral-7b": {
            "fp16": {
                "name": "Mistral-7B-Instruct-FP16",
                "size_gb": 13.5,
                "urls": ["https://hf-mirror.com/mistralai/Mistral-7B-Instruct-v0.1"],
                "target_dir": "models/mistral/fp16"
            },
            "q8": {
                "name": "Mistral-7B-Instruct-Q8",
                "size_gb": 7.2,
                "urls": ["https://hf-mirror.com/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"],
                "target_dir": "models/mistral/q8",
                "filename": "mistral-7b-instruct-v0.1.q8_0.gguf"
            },
            "q5": {
                "name": "Mistral-7B-Instruct-Q5",
                "size_gb": 4.8,
                "urls": ["https://hf-mirror.com/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"],
                "target_dir": "models/mistral/q5",
                "filename": "mistral-7b-instruct-v0.1.q5_k_m.gguf"
            },
            "q4": {
                "name": "Mistral-7B-Instruct-Q4",
                "size_gb": 4.1,
                "urls": ["https://hf-mirror.com/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"],
                "target_dir": "models/mistral/q4",
                "filename": "mistral-7b-instruct-v0.1.q4_k_m.gguf"
            }
        }
    }
