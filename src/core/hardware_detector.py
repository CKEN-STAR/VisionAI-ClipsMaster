#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Á°¨‰ª∂Ê£ÄÊµã‰∏éÂàÜÊûêÊ®°Âùó
Ëá™Âä®Ê£ÄÊµãÁ≥ªÁªüÁ°¨‰ª∂ÈÖçÁΩÆÔºåËØÑ‰º∞ÊÄßËÉΩÁ≠âÁ∫ßÔºå‰∏∫Ëá™ÈÄÇÂ∫îÊ®°ÂûãÈÖçÁΩÆÊèê‰æõÂü∫Á°ÄÊï∞ÊçÆ
"""

import os
import sys
import platform
import psutil
import logging
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Â∞ùËØïÂØºÂÖ•GPUÊ£ÄÊµãÂ∫ì
try:
    import GPUtil
    GPU_UTIL_AVAILABLE = True
except ImportError:
    GPU_UTIL_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)


class PerformanceLevel(Enum):
    """ËÆæÂ§áÊÄßËÉΩÁ≠âÁ∫ß"""
    LOW = "low"          # ‰ΩéÈÖçËÆæÂ§á (4GB RAM, Êó†Áã¨Êòæ)
    MEDIUM = "medium"    # ‰∏≠ÈÖçËÆæÂ§á (8GB RAM, ÈõÜÊàêÊòæÂç°)
    HIGH = "high"        # È´òÈÖçËÆæÂ§á (16GB+ RAM, Áã¨Á´ãÊòæÂç°)
    ULTRA = "ultra"      # Ë∂ÖÈ´òÈÖçËÆæÂ§á (32GB+ RAM, È´òÁ´ØÊòæÂç°)


class GPUType(Enum):
    """GPUÁ±ªÂûã"""
    NONE = "none"
    INTEGRATED = "integrated"
    NVIDIA = "nvidia"
    AMD = "amd"
    INTEL = "intel"


@dataclass
class HardwareInfo:
    """Á°¨‰ª∂‰ø°ÊÅØÊï∞ÊçÆÁ±ª"""
    # ÂÜÖÂ≠ò‰ø°ÊÅØ
    total_memory_gb: float
    available_memory_gb: float
    memory_usage_percent: float
    
    # CPU‰ø°ÊÅØ
    cpu_count: int
    cpu_freq_mhz: float
    cpu_architecture: str
    cpu_brand: str
    
    # GPU‰ø°ÊÅØ
    gpu_type: GPUType
    gpu_count: int
    gpu_memory_gb: float
    gpu_names: list
    
    # Á≥ªÁªü‰ø°ÊÅØ
    os_type: str
    os_version: str
    python_version: str
    
    # ÊÄßËÉΩËØÑÁ∫ß
    performance_level: PerformanceLevel
    memory_tier: str
    compute_tier: str
    
    # Êé®ËçêÈÖçÁΩÆ
    recommended_quantization: str
    max_model_memory_gb: float
    concurrent_models: int


class HardwareDetector:
    """Á°¨‰ª∂Ê£ÄÊµãÂô®"""
    
    def __init__(self):
        """ÂàùÂßãÂåñÁ°¨‰ª∂Ê£ÄÊµãÂô®"""
        self.logger = logging.getLogger(__name__)
        
    def detect_hardware(self) -> HardwareInfo:
        """Ê£ÄÊµãÁ°¨‰ª∂ÈÖçÁΩÆ"""
        try:
            self.logger.info("ÂºÄÂßãÊ£ÄÊµãÁ°¨‰ª∂ÈÖçÁΩÆ...")
            
            # Ê£ÄÊµãÂÜÖÂ≠ò
            memory_info = self._detect_memory()
            
            # Ê£ÄÊµãCPU
            cpu_info = self._detect_cpu()
            
            # Ê£ÄÊµãGPU
            gpu_info = self._detect_gpu()
            
            # Ê£ÄÊµãÁ≥ªÁªü‰ø°ÊÅØ
            system_info = self._detect_system()
            
            # ËØÑ‰º∞ÊÄßËÉΩÁ≠âÁ∫ß
            performance_level = self._evaluate_performance_level(
                memory_info, cpu_info, gpu_info
            )
            
            # ÁîüÊàêÊé®ËçêÈÖçÁΩÆ
            recommendations = self._generate_recommendations(
                memory_info, cpu_info, gpu_info, performance_level
            )
            
            # ÊûÑÂª∫Á°¨‰ª∂‰ø°ÊÅØÂØπË±°
            hardware_info = HardwareInfo(
                # ÂÜÖÂ≠ò‰ø°ÊÅØ
                total_memory_gb=memory_info["total_gb"],
                available_memory_gb=memory_info["available_gb"],
                memory_usage_percent=memory_info["usage_percent"],
                
                # CPU‰ø°ÊÅØ
                cpu_count=cpu_info["count"],
                cpu_freq_mhz=cpu_info["freq_mhz"],
                cpu_architecture=cpu_info["architecture"],
                cpu_brand=cpu_info["brand"],
                
                # GPU‰ø°ÊÅØ
                gpu_type=gpu_info["type"],
                gpu_count=gpu_info["count"],
                gpu_memory_gb=gpu_info["memory_gb"],
                gpu_names=gpu_info["names"],
                
                # Á≥ªÁªü‰ø°ÊÅØ
                os_type=system_info["os_type"],
                os_version=system_info["os_version"],
                python_version=system_info["python_version"],
                
                # ÊÄßËÉΩËØÑÁ∫ß
                performance_level=performance_level,
                memory_tier=recommendations["memory_tier"],
                compute_tier=recommendations["compute_tier"],
                
                # Êé®ËçêÈÖçÁΩÆ
                recommended_quantization=recommendations["quantization"],
                max_model_memory_gb=recommendations["max_model_memory"],
                concurrent_models=recommendations["concurrent_models"]
            )
            
            self.logger.info(f"Á°¨‰ª∂Ê£ÄÊµãÂÆåÊàê - ÊÄßËÉΩÁ≠âÁ∫ß: {performance_level.value}")
            return hardware_info
            
        except Exception as e:
            self.logger.error(f"Á°¨‰ª∂Ê£ÄÊµãÂ§±Ë¥•: {e}")
            # ËøîÂõûÈªòËÆ§ÁöÑ‰ΩéÈÖçÈÖçÁΩÆ
            return self._get_default_low_config()
    
    def _detect_memory(self) -> Dict[str, Any]:
        """Ê£ÄÊµãÂÜÖÂ≠ò‰ø°ÊÅØ"""
        try:
            memory = psutil.virtual_memory()
            return {
                "total_gb": round(memory.total / (1024**3), 2),
                "available_gb": round(memory.available / (1024**3), 2),
                "used_gb": round(memory.used / (1024**3), 2),
                "usage_percent": memory.percent
            }
        except Exception as e:
            self.logger.error(f"ÂÜÖÂ≠òÊ£ÄÊµãÂ§±Ë¥•: {e}")
            return {"total_gb": 4.0, "available_gb": 2.0, "used_gb": 2.0, "usage_percent": 50.0}
    
    def _detect_cpu(self) -> Dict[str, Any]:
        """Ê£ÄÊµãCPU‰ø°ÊÅØ"""
        try:
            cpu_freq = psutil.cpu_freq()
            cpu_count = psutil.cpu_count()
            
            # Ëé∑ÂèñCPUÊû∂ÊûÑÂíåÂìÅÁâå‰ø°ÊÅØ
            architecture = platform.machine()
            processor = platform.processor()
            
            return {
                "count": cpu_count,
                "freq_mhz": cpu_freq.current if cpu_freq else 2000.0,
                "architecture": architecture,
                "brand": processor if processor else "Unknown"
            }
        except Exception as e:
            self.logger.error(f"CPUÊ£ÄÊµãÂ§±Ë¥•: {e}")
            return {"count": 4, "freq_mhz": 2000.0, "architecture": "x86_64", "brand": "Unknown"}
    
    def _detect_gpu(self) -> Dict[str, Any]:
        """Ê£ÄÊµãGPU‰ø°ÊÅØ"""
        try:
            gpu_info = {
                "type": GPUType.NONE,
                "count": 0,
                "memory_gb": 0.0,
                "names": [],
                "detection_method": "none",
                "detailed_info": []
            }

            self.logger.info("üîç ÂºÄÂßãGPUÊ£ÄÊµã...")

            # ÊñπÊ≥ï1: ‰ΩøÁî®GPUtilÊ£ÄÊµãNVIDIA GPUÔºàÊúÄÂáÜÁ°ÆÁöÑÊòæÂ≠ò‰ø°ÊÅØÔºâ
            if GPU_UTIL_AVAILABLE:
                try:
                    self.logger.debug("Â∞ùËØï‰ΩøÁî®GPUtilÊ£ÄÊµãGPU...")
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu_info["type"] = GPUType.NVIDIA
                        gpu_info["count"] = len(gpus)
                        gpu_info["memory_gb"] = sum(gpu.memoryTotal / 1024 for gpu in gpus)
                        gpu_info["names"] = [gpu.name for gpu in gpus]
                        gpu_info["detection_method"] = "gputil"
                        gpu_info["detailed_info"] = [
                            {
                                "id": i,
                                "name": gpu.name,
                                "memory_total_mb": gpu.memoryTotal,
                                "memory_free_mb": gpu.memoryFree,
                                "memory_used_mb": gpu.memoryUsed,
                                "temperature": gpu.temperature,
                                "load": gpu.load
                            }
                            for i, gpu in enumerate(gpus)
                        ]
                        self.logger.info(f"‚úÖ GPUtilÊ£ÄÊµãÊàêÂäü: {len(gpus)}‰∏™NVIDIA GPU, ÊÄªÊòæÂ≠ò: {gpu_info['memory_gb']:.1f}GB")
                        return gpu_info
                except Exception as e:
                    self.logger.debug(f"GPUtilÊ£ÄÊµãÂ§±Ë¥•: {e}")

            # ÊñπÊ≥ï2: ‰ΩøÁî®PyTorch CUDAÊ£ÄÊµã
            if TORCH_AVAILABLE:
                try:
                    self.logger.debug("Â∞ùËØï‰ΩøÁî®PyTorch CUDAÊ£ÄÊµãGPU...")
                    if torch.cuda.is_available():
                        gpu_count = torch.cuda.device_count()
                        if gpu_count > 0:
                            gpu_info["type"] = GPUType.NVIDIA
                            gpu_info["count"] = gpu_count
                            gpu_info["names"] = []
                            total_memory = 0.0
                            detailed_info = []

                            for i in range(gpu_count):
                                try:
                                    name = torch.cuda.get_device_name(i)
                                    props = torch.cuda.get_device_properties(i)
                                    memory_gb = props.total_memory / (1024**3)

                                    gpu_info["names"].append(name)
                                    total_memory += memory_gb

                                    detailed_info.append({
                                        "id": i,
                                        "name": name,
                                        "memory_total_gb": memory_gb,
                                        "compute_capability": f"{props.major}.{props.minor}",
                                        "multiprocessor_count": props.multiprocessor_count
                                    })
                                except Exception as e:
                                    self.logger.warning(f"Ëé∑ÂèñGPU {i} ËØ¶ÁªÜ‰ø°ÊÅØÂ§±Ë¥•: {e}")
                                    # ‰ΩøÁî®ÈªòËÆ§‰º∞ÁÆóÂÄº
                                    gpu_info["names"].append(f"CUDA Device {i}")
                                    total_memory += 8.0  # ÈªòËÆ§8GB‰º∞ÁÆó
                                    detailed_info.append({
                                        "id": i,
                                        "name": f"CUDA Device {i}",
                                        "memory_total_gb": 8.0,
                                        "error": str(e)
                                    })

                            gpu_info["memory_gb"] = total_memory
                            gpu_info["detection_method"] = "pytorch_cuda"
                            gpu_info["detailed_info"] = detailed_info
                            self.logger.info(f"‚úÖ PyTorch CUDAÊ£ÄÊµãÊàêÂäü: {gpu_count}‰∏™GPU, ÊÄªÊòæÂ≠ò: {total_memory:.1f}GB")
                            return gpu_info
                except Exception as e:
                    self.logger.debug(f"PyTorch CUDAÊ£ÄÊµãÂ§±Ë¥•: {e}")

            # ÊñπÊ≥ï3: ‰ΩøÁî®nvidia-ml-pyÊ£ÄÊµã
            try:
                self.logger.debug("Â∞ùËØï‰ΩøÁî®nvidia-ml-pyÊ£ÄÊµãGPU...")
                import pynvml
                pynvml.nvmlInit()
                device_count = pynvml.nvmlDeviceGetCount()

                if device_count > 0:
                    gpu_info["type"] = GPUType.NVIDIA
                    gpu_info["count"] = device_count
                    gpu_info["names"] = []
                    total_memory = 0.0
                    detailed_info = []

                    for i in range(device_count):
                        handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                        name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                        memory_gb = memory_info.total / (1024**3)

                        gpu_info["names"].append(name)
                        total_memory += memory_gb

                        detailed_info.append({
                            "id": i,
                            "name": name,
                            "memory_total_gb": memory_gb,
                            "memory_free_gb": memory_info.free / (1024**3),
                            "memory_used_gb": memory_info.used / (1024**3)
                        })

                    gpu_info["memory_gb"] = total_memory
                    gpu_info["detection_method"] = "pynvml"
                    gpu_info["detailed_info"] = detailed_info
                    self.logger.info(f"‚úÖ pynvmlÊ£ÄÊµãÊàêÂäü: {device_count}‰∏™GPU, ÊÄªÊòæÂ≠ò: {total_memory:.1f}GB")
                    return gpu_info

            except ImportError:
                self.logger.debug("pynvml‰∏çÂèØÁî®")
            except Exception as e:
                self.logger.debug(f"pynvmlÊ£ÄÊµãÂ§±Ë¥•: {e}")

            # ÊñπÊ≥ï4: Ê£ÄÊµãÈõÜÊàêÊòæÂç°
            self.logger.debug("Ê£ÄÊµãÈõÜÊàêÊòæÂç°...")
            processor_info = platform.processor().lower()
            if "intel" in processor_info:
                gpu_info["type"] = GPUType.INTEL
                gpu_info["count"] = 1
                gpu_info["names"] = ["Intel Integrated Graphics"]
                gpu_info["memory_gb"] = 2.0  # ÈõÜÊàêÊòæÂç°‰º∞ÁÆó2GBÂÖ±‰∫´ÂÜÖÂ≠ò
                gpu_info["detection_method"] = "integrated_intel"
                self.logger.info("‚úÖ Ê£ÄÊµãÂà∞IntelÈõÜÊàêÊòæÂç°")
            elif "amd" in processor_info:
                gpu_info["type"] = GPUType.AMD
                gpu_info["count"] = 1
                gpu_info["names"] = ["AMD Integrated Graphics"]
                gpu_info["memory_gb"] = 2.0  # ÈõÜÊàêÊòæÂç°‰º∞ÁÆó2GBÂÖ±‰∫´ÂÜÖÂ≠ò
                gpu_info["detection_method"] = "integrated_amd"
                self.logger.info("‚úÖ Ê£ÄÊµãÂà∞AMDÈõÜÊàêÊòæÂç°")
            else:
                self.logger.info("‚ùå Êú™Ê£ÄÊµãÂà∞GPUËÆæÂ§á")

            return gpu_info

        except Exception as e:
            self.logger.error(f"GPUÊ£ÄÊµãÂ§±Ë¥•: {e}")
            return {
                "type": GPUType.NONE,
                "count": 0,
                "memory_gb": 0.0,
                "names": [],
                "detection_method": "failed",
                "error": str(e)
            }
    
    def _detect_system(self) -> Dict[str, Any]:
        """Ê£ÄÊµãÁ≥ªÁªü‰ø°ÊÅØ"""
        try:
            return {
                "os_type": platform.system(),
                "os_version": platform.version(),
                "python_version": sys.version.split()[0]
            }
        except Exception as e:
            self.logger.error(f"Á≥ªÁªü‰ø°ÊÅØÊ£ÄÊµãÂ§±Ë¥•: {e}")
            return {"os_type": "Unknown", "os_version": "Unknown", "python_version": "3.11"}

    def _evaluate_performance_level(self, memory_info: Dict, cpu_info: Dict, gpu_info: Dict) -> PerformanceLevel:
        """ËØÑ‰º∞ËÆæÂ§áÊÄßËÉΩÁ≠âÁ∫ßÔºàÈáçÊñ∞Ê†°ÂáÜÈòàÂÄºÔºâ"""
        try:
            total_memory = memory_info["total_gb"]
            cpu_count = cpu_info["count"]
            cpu_freq = cpu_info["freq_mhz"]
            gpu_type = gpu_info["type"]
            gpu_memory = gpu_info["memory_gb"]

            # ËÆ°ÁÆóÊÄßËÉΩÂàÜÊï∞
            memory_score = self._calculate_memory_score(total_memory)
            cpu_score = self._calculate_cpu_score(cpu_count, cpu_freq)
            gpu_score = self._calculate_gpu_score(gpu_type, gpu_memory)

            # ÁªºÂêàËØÑÂàÜ
            total_score = memory_score + cpu_score + gpu_score

            # ËÆ∞ÂΩïËØ¶ÁªÜËØÑÂàÜ‰ø°ÊÅØ
            self.logger.info(f"ÊÄßËÉΩËØÑÂàÜËØ¶ÊÉÖ: ÂÜÖÂ≠ò={memory_score}, CPU={cpu_score}, GPU={gpu_score}, ÊÄªÂàÜ={total_score}")

            # ÈáçÊñ∞Ê†°ÂáÜÁöÑÊÄßËÉΩÁ≠âÁ∫ßÈòàÂÄºÔºàÊèêÈ´òÈó®ÊßõÔºåÈÅøÂÖçÈõÜÊàêÊòæÂç°Ë¢´ËØÑ‰∏∫ËøáÈ´òÁ≠âÁ∫ßÔºâ
            if total_score >= 85:  # ÊèêÈ´òULTRAÈó®Êßõ
                performance_level = PerformanceLevel.ULTRA
            elif total_score >= 65:  # ÊèêÈ´òHIGHÈó®Êßõ
                performance_level = PerformanceLevel.HIGH
            elif total_score >= 45:  # ÊèêÈ´òMEDIUMÈó®Êßõ
                performance_level = PerformanceLevel.MEDIUM
            else:
                performance_level = PerformanceLevel.LOW

            # ÁâπÊÆäËßÑÂàôÔºöÈõÜÊàêÊòæÂç°ÊúÄÈ´òÂè™ËÉΩÊòØMEDIUMÁ≠âÁ∫ß
            if gpu_type == GPUType.INTEL and performance_level == PerformanceLevel.HIGH:
                self.logger.info("ÈõÜÊàêÊòæÂç°ÊÄßËÉΩÁ≠âÁ∫ßÈôêÂà∂ÔºöHIGH -> MEDIUM")
                performance_level = PerformanceLevel.MEDIUM
            elif gpu_type == GPUType.INTEL and performance_level == PerformanceLevel.ULTRA:
                self.logger.info("ÈõÜÊàêÊòæÂç°ÊÄßËÉΩÁ≠âÁ∫ßÈôêÂà∂ÔºöULTRA -> MEDIUM")
                performance_level = PerformanceLevel.MEDIUM

            return performance_level

        except Exception as e:
            self.logger.error(f"ÊÄßËÉΩÁ≠âÁ∫ßËØÑ‰º∞Â§±Ë¥•: {e}")
            return PerformanceLevel.LOW

    def _calculate_memory_score(self, total_memory_gb: float) -> int:
        """ËÆ°ÁÆóÂÜÖÂ≠òÂàÜÊï∞"""
        if total_memory_gb >= 32:
            return 30
        elif total_memory_gb >= 16:
            return 25
        elif total_memory_gb >= 8:
            return 20
        elif total_memory_gb >= 4:
            return 15
        else:
            return 10

    def _calculate_cpu_score(self, cpu_count: int, cpu_freq_mhz: float) -> int:
        """ËÆ°ÁÆóCPUÂàÜÊï∞"""
        # CPUÊ†∏ÂøÉÊï∞ÂàÜÊï∞
        core_score = min(cpu_count * 2, 20)  # ÊúÄÂ§ö20ÂàÜ

        # CPUÈ¢ëÁéáÂàÜÊï∞
        freq_score = 0
        if cpu_freq_mhz >= 3000:
            freq_score = 15
        elif cpu_freq_mhz >= 2500:
            freq_score = 12
        elif cpu_freq_mhz >= 2000:
            freq_score = 10
        else:
            freq_score = 8

        return core_score + freq_score

    def _calculate_gpu_score(self, gpu_type: GPUType, gpu_memory_gb: float) -> int:
        """ËÆ°ÁÆóGPUÂàÜÊï∞ÔºàÈáçÊñ∞Ê†°ÂáÜÔºåÈôç‰ΩéÈõÜÊàêÊòæÂç°ÊùÉÈáçÔºâ"""
        if gpu_type == GPUType.NVIDIA:
            # NVIDIAÁã¨ÊòæËØÑÂàÜÊõ¥Âä†ÁªÜËá¥ÔºåÁ°Æ‰øùÈ´òÁ´ØÊòæÂç°ËÉΩÂæóÂà∞È´òÂàÜ
            if gpu_memory_gb >= 24:
                return 35  # È´òÁ´ØÊòæÂç°ÔºàRTX 4090, RTX 3090Á≠âÔºâ
            elif gpu_memory_gb >= 16:
                return 30  # ‰∏≠È´òÁ´ØÊòæÂç°ÔºàRTX 4080, RTX 3080Á≠âÔºâ
            elif gpu_memory_gb >= 12:
                return 25  # ‰∏≠Á´ØÊòæÂç°ÔºàRTX 4070Ti, RTX 3070Á≠âÔºâ
            elif gpu_memory_gb >= 8:
                return 20  # ÂÖ•Èó®Áã¨ÊòæÔºàRTX 4060, GTX 1660Á≠âÔºâ
            elif gpu_memory_gb >= 4:
                return 15  # ‰ΩéÁ´ØÁã¨Êòæ
            else:
                return 10  # ÊûÅ‰ΩéÁ´ØÁã¨Êòæ
        elif gpu_type == GPUType.AMD:
            # AMDÁã¨ÊòæËØÑÂàÜ
            if gpu_memory_gb >= 16:
                return 25  # È´òÁ´ØAMDÊòæÂç°
            elif gpu_memory_gb >= 8:
                return 20  # ‰∏≠Á´ØAMDÊòæÂç°
            elif gpu_memory_gb >= 4:
                return 15  # ÂÖ•Èó®AMDÊòæÂç°
            else:
                return 10  # ‰ΩéÁ´ØAMDÊòæÂç°
        elif gpu_type == GPUType.INTEL:
            # Â§ßÂπÖÈôç‰ΩéÈõÜÊàêÊòæÂç°ÂàÜÊï∞ÔºåÈÅøÂÖçÊÄßËÉΩÁ≠âÁ∫ßËØÑ‰º∞ËøáÈ´ò
            if gpu_memory_gb >= 4:
                return 5   # È´òÁ´ØÈõÜÊàêÊòæÂç°ÔºàËæÉÊñ∞ÁöÑIris XeÁ≠âÔºâ
            elif gpu_memory_gb >= 2:
                return 3   # Ê†áÂáÜÈõÜÊàêÊòæÂç°
            else:
                return 1   # ‰ΩéÁ´ØÈõÜÊàêÊòæÂç°
        else:
            return 0   # Êó†GPU

    def _generate_recommendations(self, memory_info: Dict, cpu_info: Dict,
                                gpu_info: Dict, performance_level: PerformanceLevel) -> Dict[str, Any]:
        """ÁîüÊàêÊé®ËçêÈÖçÁΩÆÔºà‰ºòÂåñÈáèÂåñÁ≠ñÁï•ÔºåÊõ¥Âä†‰øùÂÆàÁ®≥ÂÆöÔºâ"""
        try:
            total_memory = memory_info["total_gb"]
            available_memory = memory_info["available_gb"]
            gpu_memory = gpu_info.get("memory_gb", 0.0)
            gpu_type = gpu_info.get("type", GPUType.NONE)

            self.logger.info(f"ÁîüÊàêÊé®ËçêÈÖçÁΩÆ - ÊÄßËÉΩÁ≠âÁ∫ß: {performance_level.value}, GPU: {gpu_type.value}, ÊòæÂ≠ò: {gpu_memory:.1f}GB")

            # ‰ºòÂåñÂêéÁöÑÈáèÂåñÊé®ËçêÁ≠ñÁï•ÔºöÊõ¥Âä†‰øùÂÆàÔºåÁ°Æ‰øùÁ®≥ÂÆöÊÄß
            if performance_level == PerformanceLevel.ULTRA:
                # Ë∂ÖÈ´òÊÄßËÉΩÔºöÂè™ÊúâÁúüÊ≠£ÁöÑÈ´òÁ´ØÁã¨ÊòæÊâçÊé®ËçêÊúÄÈ´òÁ≤æÂ∫¶
                if gpu_type == GPUType.NVIDIA and gpu_memory >= 16:
                    quantization = "Q8_0"  # Âè™Êúâ16GB+Áã¨ÊòæÊâçÊé®ËçêQ8_0
                elif gpu_type == GPUType.NVIDIA and gpu_memory >= 12:
                    quantization = "Q5_K"  # 12GB+Áã¨ÊòæÊé®ËçêQ5_K
                else:
                    quantization = "Q4_K_M"  # ÂÖ∂‰ªñÊÉÖÂÜµ‰øùÂÆàÊé®Ëçê

                return {
                    "memory_tier": "ultra",
                    "compute_tier": "ultra",
                    "quantization": quantization,
                    "max_model_memory": min(total_memory * 0.6, 16.0),
                    "concurrent_models": 2,
                    "gpu_acceleration": gpu_type == GPUType.NVIDIA,
                    "recommended_batch_size": 8 if gpu_type == GPUType.NVIDIA else 4
                }
            elif performance_level == PerformanceLevel.HIGH:
                # È´òÊÄßËÉΩÔºöÊõ¥‰øùÂÆàÁöÑÊé®ËçêÁ≠ñÁï•
                if gpu_type == GPUType.NVIDIA and gpu_memory >= 12:
                    quantization = "Q5_K"  # Âè™Êúâ12GB+Áã¨ÊòæÊâçÊé®ËçêQ5_K
                elif gpu_type == GPUType.NVIDIA and gpu_memory >= 8:
                    quantization = "Q4_K_M"  # 8GBÁã¨ÊòæÊé®ËçêQ4_K_M
                else:
                    quantization = "Q4_K"  # ÂÖ∂‰ªñÊÉÖÂÜµÊõ¥‰øùÂÆà

                return {
                    "memory_tier": "high",
                    "compute_tier": "high",
                    "quantization": quantization,
                    "max_model_memory": min(total_memory * 0.5, 10.0),
                    "concurrent_models": 1 if gpu_type != GPUType.NVIDIA else 2,
                    "gpu_acceleration": gpu_type == GPUType.NVIDIA,
                    "recommended_batch_size": 4 if gpu_type == GPUType.NVIDIA else 2
                }
            elif performance_level == PerformanceLevel.MEDIUM:
                # ‰∏≠Á≠âÊÄßËÉΩÔºöÈíàÂØπÈõÜÊàêÊòæÂç°‰ºòÂåñ
                if gpu_type == GPUType.NVIDIA and gpu_memory >= 6:
                    quantization = "Q4_K_M"  # 6GB+Áã¨Êòæ
                elif gpu_type == GPUType.NVIDIA and gpu_memory >= 4:
                    quantization = "Q4_K"    # 4GBÁã¨Êòæ
                elif gpu_type == GPUType.INTEL:
                    # ÈõÜÊàêÊòæÂç°ÁâπÊÆäÂ§ÑÁêÜÔºöÊ†πÊçÆÁ≥ªÁªüÂÜÖÂ≠òÂÜ≥ÂÆö
                    if total_memory >= 16:
                        quantization = "Q4_K"    # 16GB+ÂÜÖÂ≠òÁöÑÈõÜÊàêÊòæÂç°
                    else:
                        quantization = "Q2_K"    # ‰ΩéÂÜÖÂ≠òÈõÜÊàêÊòæÂç°
                else:
                    quantization = "Q2_K"    # Êó†GPUÊàñÂÖ∂‰ªñÊÉÖÂÜµ

                return {
                    "memory_tier": "medium",
                    "compute_tier": "medium",
                    "quantization": quantization,
                    "max_model_memory": min(total_memory * 0.4, 6.0),
                    "concurrent_models": 1,
                    "gpu_acceleration": gpu_type == GPUType.NVIDIA,
                    "recommended_batch_size": 2 if gpu_type == GPUType.NVIDIA else 1
                }
            else:  # LOW
                # ‰ΩéÊÄßËÉΩÔºöÊúÄ‰øùÂÆàÈÖçÁΩÆ
                return {
                    "memory_tier": "low",
                    "compute_tier": "low",
                    "quantization": "Q2_K",  # Áªü‰∏Ä‰ΩøÁî®ÊúÄËΩªÈáèÈÖçÁΩÆ
                    "max_model_memory": min(total_memory * 0.8, 3.5),
                    "concurrent_models": 1,
                    "gpu_acceleration": False,
                    "recommended_batch_size": 1
                }

        except Exception as e:
            self.logger.error(f"Êé®ËçêÈÖçÁΩÆÁîüÊàêÂ§±Ë¥•: {e}")
            return self._get_default_recommendations()

    def _get_default_low_config(self) -> HardwareInfo:
        """Ëé∑ÂèñÈªòËÆ§‰ΩéÈÖçÈÖçÁΩÆ"""
        return HardwareInfo(
            total_memory_gb=4.0,
            available_memory_gb=2.0,
            memory_usage_percent=50.0,
            cpu_count=4,
            cpu_freq_mhz=2000.0,
            cpu_architecture="x86_64",
            cpu_brand="Unknown",
            gpu_type=GPUType.NONE,
            gpu_count=0,
            gpu_memory_gb=0.0,
            gpu_names=[],
            os_type="Windows",
            os_version="Unknown",
            python_version="3.11",
            performance_level=PerformanceLevel.LOW,
            memory_tier="low",
            compute_tier="low",
            recommended_quantization="Q2_K",
            max_model_memory_gb=3.2,
            concurrent_models=1
        )

    def _get_default_recommendations(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÈªòËÆ§Êé®ËçêÈÖçÁΩÆ"""
        return {
            "memory_tier": "low",
            "compute_tier": "low",
            "quantization": "Q2_K",
            "max_model_memory": 3.2,
            "concurrent_models": 1
        }


if __name__ == "__main__":
    # ÊµãËØïÁ°¨‰ª∂Ê£ÄÊµã
    detector = HardwareDetector()
    hardware_info = detector.detect_hardware()
    print(f"Ê£ÄÊµãÂà∞ÁöÑÁ°¨‰ª∂ÈÖçÁΩÆ: {hardware_info}")
