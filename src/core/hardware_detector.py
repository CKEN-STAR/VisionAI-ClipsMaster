#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¡¬ä»¶æ£€æµ‹ä¸åˆ†ææ¨¡å—
è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶é…ç½®ï¼Œè¯„ä¼°æ€§èƒ½ç­‰çº§ï¼Œä¸ºè‡ªé€‚åº”æ¨¡å‹é…ç½®æä¾›åŸºç¡€æ•°æ®
"""

import os
import sys
import platform
import psutil
import logging
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# å°è¯•å¯¼å…¥GPUæ£€æµ‹åº“
try:
    import GPUtil
    GPU_UTIL_AVAILABLE = True
except ImportError:
    GPU_UTIL_AVAILABLE = False

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

logger = logging.getLogger(__name__)


class PerformanceLevel(Enum):
    """è®¾å¤‡æ€§èƒ½ç­‰çº§"""
    LOW = "low"          # ä½é…è®¾å¤‡ (4GB RAM, æ— ç‹¬æ˜¾)
    MEDIUM = "medium"    # ä¸­é…è®¾å¤‡ (8GB RAM, é›†æˆæ˜¾å¡)
    HIGH = "high"        # é«˜é…è®¾å¤‡ (16GB+ RAM, ç‹¬ç«‹æ˜¾å¡)
    ULTRA = "ultra"      # è¶…é«˜é…è®¾å¤‡ (32GB+ RAM, é«˜ç«¯æ˜¾å¡)


class GPUType(Enum):
    """GPUç±»å‹"""
    NONE = "none"
    INTEGRATED = "integrated"
    NVIDIA = "nvidia"
    AMD = "amd"
    INTEL = "intel"


@dataclass
class HardwareInfo:
    """ç¡¬ä»¶ä¿¡æ¯æ•°æ®ç±»"""
    # å†…å­˜ä¿¡æ¯
    total_memory_gb: float
    available_memory_gb: float
    memory_usage_percent: float
    
    # CPUä¿¡æ¯
    cpu_count: int
    cpu_freq_mhz: float
    cpu_architecture: str
    cpu_brand: str
    
    # GPUä¿¡æ¯
    gpu_type: GPUType
    gpu_count: int
    gpu_memory_gb: float
    gpu_names: list
    
    # ç³»ç»Ÿä¿¡æ¯
    os_type: str
    os_version: str
    python_version: str
    
    # æ€§èƒ½è¯„çº§
    performance_level: PerformanceLevel
    memory_tier: str
    compute_tier: str
    
    # æ¨èé…ç½®
    recommended_quantization: str
    max_model_memory_gb: float
    concurrent_models: int


class HardwareDetector:
    """ç¡¬ä»¶æ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¡¬ä»¶æ£€æµ‹å™¨"""
        self.logger = logging.getLogger(__name__)
        
    def detect_hardware(self) -> HardwareInfo:
        """æ£€æµ‹ç¡¬ä»¶é…ç½®"""
        try:
            self.logger.info("å¼€å§‹æ£€æµ‹ç¡¬ä»¶é…ç½®...")
            
            # æ£€æµ‹å†…å­˜
            memory_info = self._detect_memory()
            
            # æ£€æµ‹CPU
            cpu_info = self._detect_cpu()
            
            # æ£€æµ‹GPU
            gpu_info = self._detect_gpu()
            
            # æ£€æµ‹ç³»ç»Ÿä¿¡æ¯
            system_info = self._detect_system()
            
            # è¯„ä¼°æ€§èƒ½ç­‰çº§
            performance_level = self._evaluate_performance_level(
                memory_info, cpu_info, gpu_info
            )
            
            # ç”Ÿæˆæ¨èé…ç½®
            recommendations = self._generate_recommendations(
                memory_info, cpu_info, gpu_info, performance_level
            )
            
            # æ„å»ºç¡¬ä»¶ä¿¡æ¯å¯¹è±¡
            hardware_info = HardwareInfo(
                # å†…å­˜ä¿¡æ¯
                total_memory_gb=memory_info["total_gb"],
                available_memory_gb=memory_info["available_gb"],
                memory_usage_percent=memory_info["usage_percent"],
                
                # CPUä¿¡æ¯
                cpu_count=cpu_info["count"],
                cpu_freq_mhz=cpu_info["freq_mhz"],
                cpu_architecture=cpu_info["architecture"],
                cpu_brand=cpu_info["brand"],
                
                # GPUä¿¡æ¯
                gpu_type=gpu_info["type"],
                gpu_count=gpu_info["count"],
                gpu_memory_gb=gpu_info["memory_gb"],
                gpu_names=gpu_info["names"],
                
                # ç³»ç»Ÿä¿¡æ¯
                os_type=system_info["os_type"],
                os_version=system_info["os_version"],
                python_version=system_info["python_version"],
                
                # æ€§èƒ½è¯„çº§
                performance_level=performance_level,
                memory_tier=recommendations["memory_tier"],
                compute_tier=recommendations["compute_tier"],
                
                # æ¨èé…ç½®
                recommended_quantization=recommendations["quantization"],
                max_model_memory_gb=recommendations["max_model_memory"],
                concurrent_models=recommendations["concurrent_models"]
            )
            
            self.logger.info(f"ç¡¬ä»¶æ£€æµ‹å®Œæˆ - æ€§èƒ½ç­‰çº§: {performance_level.value}")
            return hardware_info
            
        except Exception as e:
            self.logger.error(f"ç¡¬ä»¶æ£€æµ‹å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤çš„ä½é…é…ç½®
            return self._get_default_low_config()
    
    def _detect_memory(self) -> Dict[str, Any]:
        """æ£€æµ‹å†…å­˜ä¿¡æ¯"""
        try:
            memory = psutil.virtual_memory()
            return {
                "total_gb": round(memory.total / (1024**3), 2),
                "available_gb": round(memory.available / (1024**3), 2),
                "used_gb": round(memory.used / (1024**3), 2),
                "usage_percent": memory.percent
            }
        except Exception as e:
            self.logger.error(f"å†…å­˜æ£€æµ‹å¤±è´¥: {e}")
            return {"total_gb": 4.0, "available_gb": 2.0, "used_gb": 2.0, "usage_percent": 50.0}
    
    def _detect_cpu(self) -> Dict[str, Any]:
        """æ£€æµ‹CPUä¿¡æ¯"""
        try:
            cpu_freq = psutil.cpu_freq()
            cpu_count = psutil.cpu_count()
            
            # è·å–CPUæ¶æ„å’Œå“ç‰Œä¿¡æ¯
            architecture = platform.machine()
            processor = platform.processor()
            
            return {
                "count": cpu_count,
                "freq_mhz": cpu_freq.current if cpu_freq else 2000.0,
                "architecture": architecture,
                "brand": processor if processor else "Unknown"
            }
        except Exception as e:
            self.logger.error(f"CPUæ£€æµ‹å¤±è´¥: {e}")
            return {"count": 4, "freq_mhz": 2000.0, "architecture": "x86_64", "brand": "Unknown"}
    
    def _detect_gpu(self) -> Dict[str, Any]:
        """æ£€æµ‹GPUä¿¡æ¯"""
        try:
            gpu_info = {
                "type": GPUType.NONE,
                "count": 0,
                "memory_gb": 0.0,
                "names": [],
                "detection_method": "none",
                "detailed_info": []
            }

            self.logger.info("ğŸ” å¼€å§‹GPUæ£€æµ‹...")

            # æ–¹æ³•1: ä½¿ç”¨GPUtilæ£€æµ‹NVIDIA GPUï¼ˆæœ€å‡†ç¡®çš„æ˜¾å­˜ä¿¡æ¯ï¼‰
            if GPU_UTIL_AVAILABLE:
                try:
                    self.logger.debug("å°è¯•ä½¿ç”¨GPUtilæ£€æµ‹GPU...")
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu_info["type"] = GPUType.NVIDIA
                        gpu_info["count"] = len(gpus)
                        gpu_info["memory_gb"] = sum(gpu.memoryTotal / 1024 for gpu in gpus)
                        gpu_info["names"] = [gpu.name for gpu in gpus]
                        gpu_info["detection_method"] = "gputil"
                        gpu_info["detailed_info"] = [
                            {
                                "id": i,
                                "name": gpu.name,
                                "memory_total_mb": gpu.memoryTotal,
                                "memory_free_mb": gpu.memoryFree,
                                "memory_used_mb": gpu.memoryUsed,
                                "temperature": gpu.temperature,
                                "load": gpu.load
                            }
                            for i, gpu in enumerate(gpus)
                        ]
                        self.logger.info(f"âœ… GPUtilæ£€æµ‹æˆåŠŸ: {len(gpus)}ä¸ªNVIDIA GPU, æ€»æ˜¾å­˜: {gpu_info['memory_gb']:.1f}GB")
                        return gpu_info
                except Exception as e:
                    self.logger.debug(f"GPUtilæ£€æµ‹å¤±è´¥: {e}")

            # æ–¹æ³•2: ä½¿ç”¨PyTorch CUDAæ£€æµ‹
            if TORCH_AVAILABLE:
                try:
                    self.logger.debug("å°è¯•ä½¿ç”¨PyTorch CUDAæ£€æµ‹GPU...")
                    if torch.cuda.is_available():
                        gpu_count = torch.cuda.device_count()
                        if gpu_count > 0:
                            gpu_info["type"] = GPUType.NVIDIA
                            gpu_info["count"] = gpu_count
                            gpu_info["names"] = []
                            total_memory = 0.0
                            detailed_info = []

                            for i in range(gpu_count):
                                try:
                                    name = torch.cuda.get_device_name(i)
                                    props = torch.cuda.get_device_properties(i)
                                    memory_gb = props.total_memory / (1024**3)

                                    gpu_info["names"].append(name)
                                    total_memory += memory_gb

                                    detailed_info.append({
                                        "id": i,
                                        "name": name,
                                        "memory_total_gb": memory_gb,
                                        "compute_capability": f"{props.major}.{props.minor}",
                                        "multiprocessor_count": props.multiprocessor_count
                                    })
                                except Exception as e:
                                    self.logger.warning(f"è·å–GPU {i} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                                    # ä½¿ç”¨é»˜è®¤ä¼°ç®—å€¼
                                    gpu_info["names"].append(f"CUDA Device {i}")
                                    total_memory += 8.0  # é»˜è®¤8GBä¼°ç®—
                                    detailed_info.append({
                                        "id": i,
                                        "name": f"CUDA Device {i}",
                                        "memory_total_gb": 8.0,
                                        "error": str(e)
                                    })

                            gpu_info["memory_gb"] = total_memory
                            gpu_info["detection_method"] = "pytorch_cuda"
                            gpu_info["detailed_info"] = detailed_info
                            self.logger.info(f"âœ… PyTorch CUDAæ£€æµ‹æˆåŠŸ: {gpu_count}ä¸ªGPU, æ€»æ˜¾å­˜: {total_memory:.1f}GB")
                            return gpu_info
                except Exception as e:
                    self.logger.debug(f"PyTorch CUDAæ£€æµ‹å¤±è´¥: {e}")

            # æ–¹æ³•3: ä½¿ç”¨nvidia-ml-pyæ£€æµ‹
            try:
                self.logger.debug("å°è¯•ä½¿ç”¨nvidia-ml-pyæ£€æµ‹GPU...")
                import pynvml
                pynvml.nvmlInit()
                device_count = pynvml.nvmlDeviceGetCount()

                if device_count > 0:
                    gpu_info["type"] = GPUType.NVIDIA
                    gpu_info["count"] = device_count
                    gpu_info["names"] = []
                    total_memory = 0.0
                    detailed_info = []

                    for i in range(device_count):
                        handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                        name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                        memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                        memory_gb = memory_info.total / (1024**3)

                        gpu_info["names"].append(name)
                        total_memory += memory_gb

                        detailed_info.append({
                            "id": i,
                            "name": name,
                            "memory_total_gb": memory_gb,
                            "memory_free_gb": memory_info.free / (1024**3),
                            "memory_used_gb": memory_info.used / (1024**3)
                        })

                    gpu_info["memory_gb"] = total_memory
                    gpu_info["detection_method"] = "pynvml"
                    gpu_info["detailed_info"] = detailed_info
                    self.logger.info(f"âœ… pynvmlæ£€æµ‹æˆåŠŸ: {device_count}ä¸ªGPU, æ€»æ˜¾å­˜: {total_memory:.1f}GB")
                    return gpu_info

            except ImportError:
                self.logger.debug("pynvmlä¸å¯ç”¨")
            except Exception as e:
                self.logger.debug(f"pynvmlæ£€æµ‹å¤±è´¥: {e}")

            # æ–¹æ³•4: æ£€æµ‹é›†æˆæ˜¾å¡
            self.logger.debug("æ£€æµ‹é›†æˆæ˜¾å¡...")
            processor_info = platform.processor().lower()
            if "intel" in processor_info:
                gpu_info["type"] = GPUType.INTEL
                gpu_info["count"] = 1
                gpu_info["names"] = ["Intel Integrated Graphics"]
                gpu_info["memory_gb"] = 2.0  # é›†æˆæ˜¾å¡ä¼°ç®—2GBå…±äº«å†…å­˜
                gpu_info["detection_method"] = "integrated_intel"
                self.logger.info("âœ… æ£€æµ‹åˆ°Intelé›†æˆæ˜¾å¡")
            elif "amd" in processor_info:
                gpu_info["type"] = GPUType.AMD
                gpu_info["count"] = 1
                gpu_info["names"] = ["AMD Integrated Graphics"]
                gpu_info["memory_gb"] = 2.0  # é›†æˆæ˜¾å¡ä¼°ç®—2GBå…±äº«å†…å­˜
                gpu_info["detection_method"] = "integrated_amd"
                self.logger.info("âœ… æ£€æµ‹åˆ°AMDé›†æˆæ˜¾å¡")
            else:
                self.logger.info("âŒ æœªæ£€æµ‹åˆ°GPUè®¾å¤‡")

            return gpu_info

        except Exception as e:
            self.logger.error(f"GPUæ£€æµ‹å¤±è´¥: {e}")
            return {
                "type": GPUType.NONE,
                "count": 0,
                "memory_gb": 0.0,
                "names": [],
                "detection_method": "failed",
                "error": str(e)
            }
    
    def _detect_system(self) -> Dict[str, Any]:
        """æ£€æµ‹ç³»ç»Ÿä¿¡æ¯"""
        try:
            return {
                "os_type": platform.system(),
                "os_version": platform.version(),
                "python_version": sys.version.split()[0]
            }
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿä¿¡æ¯æ£€æµ‹å¤±è´¥: {e}")
            return {"os_type": "Unknown", "os_version": "Unknown", "python_version": "3.11"}

    def _evaluate_performance_level(self, memory_info: Dict, cpu_info: Dict, gpu_info: Dict) -> PerformanceLevel:
        """è¯„ä¼°è®¾å¤‡æ€§èƒ½ç­‰çº§ï¼ˆé‡æ–°æ ¡å‡†é˜ˆå€¼ï¼‰"""
        try:
            total_memory = memory_info["total_gb"]
            cpu_count = cpu_info["count"]
            cpu_freq = cpu_info["freq_mhz"]
            gpu_type = gpu_info["type"]
            gpu_memory = gpu_info["memory_gb"]

            # è®¡ç®—æ€§èƒ½åˆ†æ•°
            memory_score = self._calculate_memory_score(total_memory)
            cpu_score = self._calculate_cpu_score(cpu_count, cpu_freq)
            gpu_score = self._calculate_gpu_score(gpu_type, gpu_memory)

            # ç»¼åˆè¯„åˆ†
            total_score = memory_score + cpu_score + gpu_score

            # è®°å½•è¯¦ç»†è¯„åˆ†ä¿¡æ¯
            self.logger.info(f"æ€§èƒ½è¯„åˆ†è¯¦æƒ…: å†…å­˜={memory_score}, CPU={cpu_score}, GPU={gpu_score}, æ€»åˆ†={total_score}")

            # é‡æ–°æ ¡å‡†çš„æ€§èƒ½ç­‰çº§é˜ˆå€¼ï¼ˆæé«˜é—¨æ§›ï¼Œé¿å…é›†æˆæ˜¾å¡è¢«è¯„ä¸ºè¿‡é«˜ç­‰çº§ï¼‰
            if total_score >= 85:  # æé«˜ULTRAé—¨æ§›
                performance_level = PerformanceLevel.ULTRA
            elif total_score >= 65:  # æé«˜HIGHé—¨æ§›
                performance_level = PerformanceLevel.HIGH
            elif total_score >= 45:  # æé«˜MEDIUMé—¨æ§›
                performance_level = PerformanceLevel.MEDIUM
            else:
                performance_level = PerformanceLevel.LOW

            # ç‰¹æ®Šè§„åˆ™ï¼šé›†æˆæ˜¾å¡æœ€é«˜åªèƒ½æ˜¯MEDIUMç­‰çº§
            if gpu_type == GPUType.INTEL and performance_level == PerformanceLevel.HIGH:
                self.logger.info("é›†æˆæ˜¾å¡æ€§èƒ½ç­‰çº§é™åˆ¶ï¼šHIGH -> MEDIUM")
                performance_level = PerformanceLevel.MEDIUM
            elif gpu_type == GPUType.INTEL and performance_level == PerformanceLevel.ULTRA:
                self.logger.info("é›†æˆæ˜¾å¡æ€§èƒ½ç­‰çº§é™åˆ¶ï¼šULTRA -> MEDIUM")
                performance_level = PerformanceLevel.MEDIUM

            return performance_level

        except Exception as e:
            self.logger.error(f"æ€§èƒ½ç­‰çº§è¯„ä¼°å¤±è´¥: {e}")
            return PerformanceLevel.LOW

    def _calculate_memory_score(self, total_memory_gb: float) -> int:
        """è®¡ç®—å†…å­˜åˆ†æ•°"""
        if total_memory_gb >= 32:
            return 30
        elif total_memory_gb >= 16:
            return 25
        elif total_memory_gb >= 8:
            return 20
        elif total_memory_gb >= 4:
            return 15
        else:
            return 10

    def _calculate_cpu_score(self, cpu_count: int, cpu_freq_mhz: float) -> int:
        """è®¡ç®—CPUåˆ†æ•°"""
        # CPUæ ¸å¿ƒæ•°åˆ†æ•°
        core_score = min(cpu_count * 2, 20)  # æœ€å¤š20åˆ†

        # CPUé¢‘ç‡åˆ†æ•°
        freq_score = 0
        if cpu_freq_mhz >= 3000:
            freq_score = 15
        elif cpu_freq_mhz >= 2500:
            freq_score = 12
        elif cpu_freq_mhz >= 2000:
            freq_score = 10
        else:
            freq_score = 8

        return core_score + freq_score

    def _calculate_gpu_score(self, gpu_type: GPUType, gpu_memory_gb: float) -> int:
        """è®¡ç®—GPUåˆ†æ•°ï¼ˆé‡æ–°æ ¡å‡†ï¼Œé™ä½é›†æˆæ˜¾å¡æƒé‡ï¼‰"""
        if gpu_type == GPUType.NVIDIA:
            # NVIDIAç‹¬æ˜¾è¯„åˆ†æ›´åŠ ç»†è‡´ï¼Œç¡®ä¿é«˜ç«¯æ˜¾å¡èƒ½å¾—åˆ°é«˜åˆ†
            if gpu_memory_gb >= 24:
                return 35  # é«˜ç«¯æ˜¾å¡ï¼ˆRTX 4090, RTX 3090ç­‰ï¼‰
            elif gpu_memory_gb >= 16:
                return 30  # ä¸­é«˜ç«¯æ˜¾å¡ï¼ˆRTX 4080, RTX 3080ç­‰ï¼‰
            elif gpu_memory_gb >= 12:
                return 25  # ä¸­ç«¯æ˜¾å¡ï¼ˆRTX 4070Ti, RTX 3070ç­‰ï¼‰
            elif gpu_memory_gb >= 8:
                return 20  # å…¥é—¨ç‹¬æ˜¾ï¼ˆRTX 4060, GTX 1660ç­‰ï¼‰
            elif gpu_memory_gb >= 4:
                return 15  # ä½ç«¯ç‹¬æ˜¾
            else:
                return 10  # æä½ç«¯ç‹¬æ˜¾
        elif gpu_type == GPUType.AMD:
            # AMDç‹¬æ˜¾è¯„åˆ†
            if gpu_memory_gb >= 16:
                return 25  # é«˜ç«¯AMDæ˜¾å¡
            elif gpu_memory_gb >= 8:
                return 20  # ä¸­ç«¯AMDæ˜¾å¡
            elif gpu_memory_gb >= 4:
                return 15  # å…¥é—¨AMDæ˜¾å¡
            else:
                return 10  # ä½ç«¯AMDæ˜¾å¡
        elif gpu_type == GPUType.INTEL:
            # å¤§å¹…é™ä½é›†æˆæ˜¾å¡åˆ†æ•°ï¼Œé¿å…æ€§èƒ½ç­‰çº§è¯„ä¼°è¿‡é«˜
            if gpu_memory_gb >= 4:
                return 5   # é«˜ç«¯é›†æˆæ˜¾å¡ï¼ˆè¾ƒæ–°çš„Iris Xeç­‰ï¼‰
            elif gpu_memory_gb >= 2:
                return 3   # æ ‡å‡†é›†æˆæ˜¾å¡
            else:
                return 1   # ä½ç«¯é›†æˆæ˜¾å¡
        else:
            return 0   # æ— GPU

    def _generate_recommendations(self, memory_info: Dict, cpu_info: Dict,
                                gpu_info: Dict, performance_level: PerformanceLevel) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨èé…ç½®ï¼ˆä¼˜åŒ–é‡åŒ–ç­–ç•¥ï¼Œæ›´åŠ ä¿å®ˆç¨³å®šï¼‰"""
        try:
            total_memory = memory_info["total_gb"]
            available_memory = memory_info["available_gb"]
            gpu_memory = gpu_info.get("memory_gb", 0.0)
            gpu_type = gpu_info.get("type", GPUType.NONE)

            self.logger.info(f"ç”Ÿæˆæ¨èé…ç½® - æ€§èƒ½ç­‰çº§: {performance_level.value}, GPU: {gpu_type.value}, æ˜¾å­˜: {gpu_memory:.1f}GB")

            # ä¼˜åŒ–åçš„é‡åŒ–æ¨èç­–ç•¥ï¼šæ›´åŠ ä¿å®ˆï¼Œç¡®ä¿ç¨³å®šæ€§
            if performance_level == PerformanceLevel.ULTRA:
                # è¶…é«˜æ€§èƒ½ï¼šåªæœ‰çœŸæ­£çš„é«˜ç«¯ç‹¬æ˜¾æ‰æ¨èæœ€é«˜ç²¾åº¦
                if gpu_type == GPUType.NVIDIA and gpu_memory >= 16:
                    quantization = "Q8_0"  # åªæœ‰16GB+ç‹¬æ˜¾æ‰æ¨èQ8_0
                elif gpu_type == GPUType.NVIDIA and gpu_memory >= 12:
                    quantization = "Q5_K"  # 12GB+ç‹¬æ˜¾æ¨èQ5_K
                else:
                    quantization = "Q4_K_M"  # å…¶ä»–æƒ…å†µä¿å®ˆæ¨è

                return {
                    "memory_tier": "ultra",
                    "compute_tier": "ultra",
                    "quantization": quantization,
                    "max_model_memory": min(total_memory * 0.6, 16.0),
                    "concurrent_models": 2,
                    "gpu_acceleration": gpu_type == GPUType.NVIDIA,
                    "recommended_batch_size": 8 if gpu_type == GPUType.NVIDIA else 4
                }
            elif performance_level == PerformanceLevel.HIGH:
                # é«˜æ€§èƒ½ï¼šæ›´ä¿å®ˆçš„æ¨èç­–ç•¥
                if gpu_type == GPUType.NVIDIA and gpu_memory >= 12:
                    quantization = "Q5_K"  # åªæœ‰12GB+ç‹¬æ˜¾æ‰æ¨èQ5_K
                elif gpu_type == GPUType.NVIDIA and gpu_memory >= 8:
                    quantization = "Q4_K_M"  # 8GBç‹¬æ˜¾æ¨èQ4_K_M
                else:
                    quantization = "Q4_K"  # å…¶ä»–æƒ…å†µæ›´ä¿å®ˆ

                return {
                    "memory_tier": "high",
                    "compute_tier": "high",
                    "quantization": quantization,
                    "max_model_memory": min(total_memory * 0.5, 10.0),
                    "concurrent_models": 1 if gpu_type != GPUType.NVIDIA else 2,
                    "gpu_acceleration": gpu_type == GPUType.NVIDIA,
                    "recommended_batch_size": 4 if gpu_type == GPUType.NVIDIA else 2
                }
            elif performance_level == PerformanceLevel.MEDIUM:
                # ä¸­ç­‰æ€§èƒ½ï¼šé’ˆå¯¹é›†æˆæ˜¾å¡ä¼˜åŒ–
                if gpu_type == GPUType.NVIDIA and gpu_memory >= 6:
                    quantization = "Q4_K_M"  # 6GB+ç‹¬æ˜¾
                elif gpu_type == GPUType.NVIDIA and gpu_memory >= 4:
                    quantization = "Q4_K"    # 4GBç‹¬æ˜¾
                elif gpu_type == GPUType.INTEL:
                    # é›†æˆæ˜¾å¡ç‰¹æ®Šå¤„ç†ï¼šæ ¹æ®ç³»ç»Ÿå†…å­˜å†³å®š
                    if total_memory >= 16:
                        quantization = "Q4_K"    # 16GB+å†…å­˜çš„é›†æˆæ˜¾å¡
                    else:
                        quantization = "Q2_K"    # ä½å†…å­˜é›†æˆæ˜¾å¡
                else:
                    quantization = "Q2_K"    # æ— GPUæˆ–å…¶ä»–æƒ…å†µ

                return {
                    "memory_tier": "medium",
                    "compute_tier": "medium",
                    "quantization": quantization,
                    "max_model_memory": min(total_memory * 0.4, 6.0),
                    "concurrent_models": 1,
                    "gpu_acceleration": gpu_type == GPUType.NVIDIA,
                    "recommended_batch_size": 2 if gpu_type == GPUType.NVIDIA else 1
                }
            else:  # LOW
                # ä½æ€§èƒ½ï¼šæœ€ä¿å®ˆé…ç½®
                return {
                    "memory_tier": "low",
                    "compute_tier": "low",
                    "quantization": "Q2_K",  # ç»Ÿä¸€ä½¿ç”¨æœ€è½»é‡é…ç½®
                    "max_model_memory": min(total_memory * 0.8, 3.5),
                    "concurrent_models": 1,
                    "gpu_acceleration": False,
                    "recommended_batch_size": 1
                }

        except Exception as e:
            self.logger.error(f"æ¨èé…ç½®ç”Ÿæˆå¤±è´¥: {e}")
            return self._get_default_recommendations()

    def _get_default_low_config(self) -> HardwareInfo:
        """è·å–é»˜è®¤ä½é…é…ç½®"""
        return HardwareInfo(
            total_memory_gb=4.0,
            available_memory_gb=2.0,
            memory_usage_percent=50.0,
            cpu_count=4,
            cpu_freq_mhz=2000.0,
            cpu_architecture="x86_64",
            cpu_brand="Unknown",
            gpu_type=GPUType.NONE,
            gpu_count=0,
            gpu_memory_gb=0.0,
            gpu_names=[],
            os_type="Windows",
            os_version="Unknown",
            python_version="3.11",
            performance_level=PerformanceLevel.LOW,
            memory_tier="low",
            compute_tier="low",
            recommended_quantization="Q2_K",
            max_model_memory_gb=3.2,
            concurrent_models=1
        )

    def _get_default_recommendations(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ¨èé…ç½®"""
        return {
            "memory_tier": "low",
            "compute_tier": "low",
            "quantization": "Q2_K",
            "max_model_memory": 3.2,
            "concurrent_models": 1
        }


if __name__ == "__main__":
    # æµ‹è¯•ç¡¬ä»¶æ£€æµ‹
    detector = HardwareDetector()
    hardware_info = detector.detect_hardware()
    print(f"æ£€æµ‹åˆ°çš„ç¡¬ä»¶é…ç½®: {hardware_info}")
