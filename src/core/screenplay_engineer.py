#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å‰§æœ¬å·¥ç¨‹å¸ˆæ¨¡å— - ç®€åŒ–ç‰ˆï¼ˆUIæ¼”ç¤ºç”¨ï¼‰

æä¾›å‰§æœ¬é‡æ„ã€ä¼˜åŒ–å’Œè½¬æ¢åŠŸèƒ½ï¼Œæ”¯æŒå°†åŸå§‹ç´ ææ•´åˆä¸ºé«˜è´¨é‡å‰§æœ¬ã€‚
è¯¥æ¨¡å—æ˜¯æ ¸å¿ƒå¤„ç†å¼•æ“ï¼Œè´Ÿè´£åè°ƒå„ç§åˆ†æå·¥å…·å’Œç”Ÿæˆç­–ç•¥ã€‚
"""

import os
import json
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import re

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

class ScreenplayEngineer:
    """
    å‰§æœ¬å·¥ç¨‹å¸ˆ - ç®€åŒ–ç‰ˆï¼ˆUIæ¼”ç¤ºç”¨ï¼‰
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å‰§æœ¬å·¥ç¨‹å¸ˆ"""
        # è®°å½•å¤„ç†çš„å†å²
        self.processing_history = []
        # å½“å‰åŠ è½½çš„å­—å¹•
        self.current_subtitles = []
        # å‰§æƒ…åˆ†æç»“æœ
        self.plot_analysis = {}

        # è®­ç»ƒçŠ¶æ€æ„ŸçŸ¥å’Œæ€§èƒ½æ”¹è¿›
        self.training_improvement_factor = 0.0  # è®­ç»ƒæ”¹è¿›å› å­
        self.model_performance_cache = {}  # æ¨¡å‹æ€§èƒ½ç¼“å­˜
        self.baseline_performance = {}  # åŸºçº¿æ€§èƒ½è®°å½•

    def load_subtitles(self, srt_data) -> List[Dict[str, Any]]:
        """
        åŠ è½½SRTå­—å¹•æ–‡ä»¶æˆ–å­—å¹•æ•°æ®

        Args:
            srt_data: SRTæ–‡ä»¶è·¯å¾„(str)æˆ–å­—å¹•åˆ—è¡¨(list)

        Returns:
            å­—å¹•åˆ—è¡¨
        """
        try:
            if isinstance(srt_data, str):
                # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œå¯¼å…¥SRTæ–‡ä»¶
                subtitles = self.import_srt(srt_data)
                logger.info(f"æˆåŠŸåŠ è½½å­—å¹•æ–‡ä»¶: {srt_data}, å…±{len(subtitles)}æ¡å­—å¹•")
            elif isinstance(srt_data, list):
                # å¦‚æœæ˜¯å­—å¹•åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                subtitles = srt_data
                logger.info(f"æˆåŠŸåŠ è½½å­—å¹•æ•°æ®: å…±{len(subtitles)}æ¡å­—å¹•")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(srt_data)}")

            self.current_subtitles = subtitles
            return subtitles
        except Exception as e:
            logger.error(f"åŠ è½½å­—å¹•å¤±è´¥: {e}")
            return []

    def analyze_plot(self, subtitles: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        åˆ†æå‰§æƒ…ç»“æ„

        Args:
            subtitles: å­—å¹•åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰åŠ è½½çš„å­—å¹•

        Returns:
            å‰§æƒ…åˆ†æç»“æœ
        """
        if subtitles is None:
            subtitles = self.current_subtitles

        if not subtitles:
            logger.warning("æ²¡æœ‰å­—å¹•æ•°æ®å¯ä¾›åˆ†æ")
            return {}

        try:
            # ç®€åŒ–çš„å‰§æƒ…åˆ†æ
            analysis = {
                "total_duration": 0,
                "subtitle_count": len(subtitles),
                "key_moments": [],
                "emotion_curve": [],
                "character_analysis": {},
                "plot_structure": {
                    "beginning": [],
                    "middle": [],
                    "end": []
                }
            }

            # è®¡ç®—æ€»æ—¶é•¿
            if subtitles:
                last_subtitle = subtitles[-1]
                if 'end_time' in last_subtitle:
                    analysis["total_duration"] = last_subtitle['end_time']

            # ç®€å•çš„æƒ…èŠ‚åˆ†å‰²ï¼ˆæŒ‰æ—¶é—´ä¸‰ç­‰åˆ†ï¼‰
            total_duration = analysis["total_duration"]
            if total_duration > 0:
                third = total_duration / 3
                for subtitle in subtitles:
                    start_time = subtitle.get('start_time', 0)
                    if start_time < third:
                        analysis["plot_structure"]["beginning"].append(subtitle)
                    elif start_time < 2 * third:
                        analysis["plot_structure"]["middle"].append(subtitle)
                    else:
                        analysis["plot_structure"]["end"].append(subtitle)

            # å¯»æ‰¾å…³é”®æ—¶åˆ»ï¼ˆç®€åŒ–ç‰ˆï¼šåŒ…å«æ„Ÿå¹å·æˆ–é—®å·çš„å­—å¹•ï¼‰
            for subtitle in subtitles:
                text = subtitle.get('text', '')
                if '!' in text or 'ï¼Ÿ' in text or '!' in text or '?' in text:
                    analysis["key_moments"].append({
                        "time": subtitle.get('start_time', 0),
                        "text": text,
                        "type": "emotional_peak"
                    })

            self.plot_analysis = analysis
            logger.info(f"å‰§æƒ…åˆ†æå®Œæˆ: æ€»æ—¶é•¿{total_duration:.1f}ç§’, å…³é”®æ—¶åˆ»{len(analysis['key_moments'])}ä¸ª")
            return analysis

        except Exception as e:
            logger.error(f"å‰§æƒ…åˆ†æå¤±è´¥: {e}")
            return {}

    def analyze_plot_structure(self, subtitles: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        """
        åˆ†æå‰§æƒ…ç»“æ„ - æµ‹è¯•APIå…¼å®¹æ–¹æ³•

        Args:
            subtitles: å­—å¹•åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰åŠ è½½çš„å­—å¹•

        Returns:
            å‰§æƒ…ç»“æ„åˆ†æç»“æœ
        """
        if subtitles is None:
            subtitles = self.current_subtitles

        if not subtitles:
            logger.warning("æ²¡æœ‰å­—å¹•æ•°æ®å¯ä¾›åˆ†æ")
            return {"scenes": [], "characters": [], "emotions": []}

        try:
            # è°ƒç”¨ç°æœ‰çš„analyze_plotæ–¹æ³•
            analysis = self.analyze_plot(subtitles)

            # è½¬æ¢ä¸ºæµ‹è¯•æœŸæœ›çš„æ ¼å¼
            result = {
                "scenes": [],
                "characters": [],
                "emotions": [],
                "plot_points": analysis.get("key_moments", []),
                "structure": analysis.get("plot_structure", {}),
                "total_duration": analysis.get("total_duration", 0),
                "subtitle_count": analysis.get("subtitle_count", 0)
            }

            # åˆ†æåœºæ™¯
            plot_structure = analysis.get("plot_structure", {})
            for phase, phase_subtitles in plot_structure.items():
                if phase_subtitles:
                    result["scenes"].append({
                        "phase": phase,
                        "subtitle_count": len(phase_subtitles),
                        "start_time": phase_subtitles[0].get("start_time", 0) if phase_subtitles else 0,
                        "end_time": phase_subtitles[-1].get("end_time", 0) if phase_subtitles else 0
                    })

            # ç®€å•çš„è§’è‰²åˆ†æï¼ˆåŸºäºå¯¹è¯æ¨¡å¼ï¼‰
            characters = set()
            for subtitle in subtitles:
                text = subtitle.get("text", "")
                # ç®€å•çš„å¯¹è¯æ£€æµ‹
                if ":" in text or "ï¼š" in text:
                    parts = text.split(":" if ":" in text else "ï¼š")
                    if len(parts) > 1:
                        potential_character = parts[0].strip()
                        if len(potential_character) < 10:  # å‡è®¾è§’è‰²åä¸ä¼šå¤ªé•¿
                            characters.add(potential_character)

            result["characters"] = list(characters)

            # æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
            emotion_keywords = {
                "happy": ["å¼€å¿ƒ", "é«˜å…´", "å¿«ä¹", "ç¬‘", "å“ˆå“ˆ"],
                "sad": ["ä¼¤å¿ƒ", "éš¾è¿‡", "å“­", "çœ¼æ³ª"],
                "angry": ["ç”Ÿæ°”", "æ„¤æ€’", "æ°”æ­»", "è®¨åŒ"],
                "surprised": ["æƒŠè®¶", "éœ‡æƒŠ", "ä¸æ•¢ç›¸ä¿¡", "å¤©å“ª"]
            }

            emotions = []
            for subtitle in subtitles:
                text = subtitle.get("text", "")
                for emotion, keywords in emotion_keywords.items():
                    if any(keyword in text for keyword in keywords):
                        emotions.append({
                            "time": subtitle.get("start_time", 0),
                            "emotion": emotion,
                            "text": text
                        })

            result["emotions"] = emotions

            logger.info(f"å‰§æƒ…ç»“æ„åˆ†æå®Œæˆ: {len(result['scenes'])}ä¸ªåœºæ™¯, {len(result['characters'])}ä¸ªè§’è‰², {len(result['emotions'])}ä¸ªæƒ…æ„Ÿç‚¹")
            return result

        except Exception as e:
            logger.error(f"å‰§æƒ…ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {"scenes": [], "characters": [], "emotions": []}

    def reconstruct_screenplay(self, srt_input=None, target_style: str = "viral"):
        """
        é‡æ„å‰§æœ¬ä¸ºçˆ†æ¬¾é£æ ¼ - æ ¸å¿ƒåŠŸèƒ½å®ç°

        Args:
            srt_input: SRTæ–‡ä»¶è·¯å¾„ã€SRTå†…å®¹å­—ç¬¦ä¸²æˆ–å­—å¹•åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰åŠ è½½çš„å­—å¹•
            target_style: ç›®æ ‡é£æ ¼ï¼Œé»˜è®¤ä¸º"viral"ï¼ˆçˆ†æ¬¾ï¼‰

        Returns:
            List[Dict]: æ ‡å‡†åŒ–çš„é‡æ„åå­—å¹•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« start, end, text, duration å­—æ®µ
        """
        # å¦‚æœæä¾›äº†è¾“å…¥ï¼Œå…ˆåŠ è½½å­—å¹•æ•°æ®
        if srt_input is not None:
            if isinstance(srt_input, str):
                # åˆ¤æ–­æ˜¯æ–‡ä»¶è·¯å¾„è¿˜æ˜¯SRTå†…å®¹
                if srt_input.strip().startswith('1\n') or '00:00:' in srt_input:
                    # æ˜¯SRTå†…å®¹ï¼Œè§£æå®ƒ
                    from .srt_parser import SRTParser
                    parser = SRTParser()
                    subtitles = parser.parse_srt_content(srt_input)
                    self.current_subtitles = subtitles
                    logger.info(f"æˆåŠŸè§£æSRTå†…å®¹: å…±{len(subtitles)}æ¡å­—å¹•")
                else:
                    # æ˜¯æ–‡ä»¶è·¯å¾„
                    self.load_subtitles(srt_input)
            elif isinstance(srt_input, list):
                # æ˜¯å­—å¹•åˆ—è¡¨
                self.current_subtitles = srt_input
                logger.info(f"æˆåŠŸåŠ è½½å­—å¹•åˆ—è¡¨: å…±{len(srt_input)}æ¡å­—å¹•")

        if not self.current_subtitles:
            logger.warning("æ²¡æœ‰åŠ è½½å­—å¹•æ•°æ®ï¼Œæ— æ³•è¿›è¡Œé‡æ„")
            return []

        try:
            # è®¡ç®—æ€»æ—¶é•¿
            total_duration = sum(sub.get("duration", 0) for sub in self.current_subtitles)

            # çŸ­è§†é¢‘ç‰¹æ®Šå¤„ç†é€»è¾‘
            if total_duration <= 15.0 or len(self.current_subtitles) <= 4:
                logger.info(f"æ£€æµ‹åˆ°çŸ­è§†é¢‘ï¼ˆ{total_duration:.1f}ç§’ï¼Œ{len(self.current_subtitles)}æ¡å­—å¹•ï¼‰ï¼Œå¯ç”¨ç‰¹æ®Šå¤„ç†")
                reconstructed_segments = self._handle_short_video(self.current_subtitles, total_duration)
            else:
                # 1. åˆ†æåŸå§‹å‰§æƒ…ç»“æ„
                original_analysis = self.analyze_plot_structure()

                # 2. æå–å…³é”®ç‰‡æ®µ
                key_segments = self._extract_key_segments(self.current_subtitles, original_analysis)

                # 3. é‡æ–°æ’åˆ—å’Œä¼˜åŒ–
                reconstructed_segments = self._optimize_for_viral_appeal(key_segments)

            # 4. ç”Ÿæˆæ–°çš„æ—¶é—´è½´
            new_timeline = self._generate_new_timeline(reconstructed_segments)

            # 5. è½¬æ¢ä¸ºæ ‡å‡†åŒ–æ ¼å¼
            standardized_segments = []
            for i, segment in enumerate(reconstructed_segments):
                # ç¡®ä¿æ¯ä¸ªç‰‡æ®µéƒ½æœ‰æ ‡å‡†åŒ–çš„å­—æ®µ
                standardized_segment = {
                    "start": float(segment.get("start_time", segment.get("start", 0))),
                    "end": float(segment.get("end_time", segment.get("end", 0))),
                    "text": str(segment.get("text", "")),
                    "duration": float(segment.get("duration", 0))
                }

                # å¦‚æœdurationä¸º0ï¼Œè®¡ç®—å®ƒ
                if standardized_segment["duration"] == 0:
                    standardized_segment["duration"] = standardized_segment["end"] - standardized_segment["start"]

                standardized_segments.append(standardized_segment)

            logger.info(f"âœ… å‰§æœ¬é‡æ„å®Œæˆï¼Œç”Ÿæˆ {len(standardized_segments)} ä¸ªæ ‡å‡†åŒ–ç‰‡æ®µ")
            return standardized_segments

        except Exception as e:
            logger.error(f"âŒ å‰§æœ¬é‡æ„å¤±è´¥: {e}")
            # è¿”å›åŸå§‹å­—å¹•çš„æ ‡å‡†åŒ–æ ¼å¼ä½œä¸ºå›é€€
            fallback_segments = []
            for i, subtitle in enumerate(self.current_subtitles):
                fallback_segment = {
                    "start": float(subtitle.get("start_time", subtitle.get("start", 0))),
                    "end": float(subtitle.get("end_time", subtitle.get("end", 0))),
                    "text": str(subtitle.get("text", "")),
                    "duration": float(subtitle.get("duration", 0))
                }

                # å¦‚æœdurationä¸º0ï¼Œè®¡ç®—å®ƒ
                if fallback_segment["duration"] == 0:
                    fallback_segment["duration"] = fallback_segment["end"] - fallback_segment["start"]

                fallback_segments.append(fallback_segment)

            logger.info(f"ğŸ”„ ä½¿ç”¨åŸå§‹å­—å¹•ä½œä¸ºå›é€€ï¼Œå…± {len(fallback_segments)} ä¸ªç‰‡æ®µ")
            return fallback_segments

    def _handle_short_video(self, subtitles: List[Dict[str, Any]], total_duration: float) -> List[Dict[str, Any]]:
        """çŸ­è§†é¢‘ç‰¹æ®Šå¤„ç†é€»è¾‘"""
        try:
            # çŸ­è§†é¢‘å‹ç¼©ç­–ç•¥ï¼šæ™ºèƒ½åˆ å‡è€Œéå®Œå…¨ä¿ç•™
            if len(subtitles) <= 2:
                # æçŸ­è§†é¢‘ï¼šä¿æŒåŸæ ·
                logger.info("æçŸ­è§†é¢‘ï¼Œä¿æŒåŸæœ‰ç»“æ„")
                return subtitles

            # çŸ­è§†é¢‘æ™ºèƒ½å‹ç¼©ï¼šåˆ é™¤æœ€ä¸é‡è¦çš„1-2ä¸ªç‰‡æ®µ
            segments_with_scores = []

            for i, subtitle in enumerate(subtitles):
                text = subtitle.get("text", "").lower()

                # è®¡ç®—é‡è¦æ€§è¯„åˆ†
                importance_score = 0.5  # åŸºç¡€åˆ†

                # ä½ç½®æƒé‡
                if i == 0:  # å¼€å¤´
                    importance_score += 0.3
                elif i == len(subtitles) - 1:  # ç»“å°¾
                    importance_score += 0.4
                else:  # ä¸­é—´
                    importance_score += 0.2

                # å†…å®¹æƒé‡
                important_words = ["ä½†æ˜¯", "ç„¶å", "çªç„¶", "æœ€å", "ç»“æœ", "å› ä¸º", "æ‰€ä»¥"]
                for word in important_words:
                    if word in text:
                        importance_score += 0.1

                # æƒ…æ„Ÿæƒé‡
                emotion_words = ["çˆ±", "æ¨", "å¼€å¿ƒ", "éš¾è¿‡", "æƒŠè®¶", "å®³æ€•", "æ„¤æ€’"]
                for word in emotion_words:
                    if word in text:
                        importance_score += 0.15

                segments_with_scores.append({
                    **subtitle,
                    "importance_score": importance_score,
                    "original_index": i
                })

            # æ’åºå¹¶é€‰æ‹©æœ€é‡è¦çš„ç‰‡æ®µ
            segments_with_scores.sort(key=lambda x: x["importance_score"], reverse=True)

            # ç¡®ä¿å‹ç¼©æ¯”ä¾‹åœ¨30%-70%èŒƒå›´å†…
            target_count = max(2, min(len(subtitles) - 1, int(len(subtitles) * 0.6)))
            selected_segments = segments_with_scores[:target_count]

            # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—
            selected_segments.sort(key=lambda x: x["original_index"])

            # æ¸…ç†ä¸´æ—¶å­—æ®µ
            for segment in selected_segments:
                segment.pop("importance_score", None)
                segment.pop("original_index", None)

            logger.info(f"çŸ­è§†é¢‘å‹ç¼©å®Œæˆï¼š{len(subtitles)} â†’ {len(selected_segments)} ä¸ªç‰‡æ®µ")
            return selected_segments

        except Exception as e:
            logger.error(f"çŸ­è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ä¿æŒåŸæ ·
            return subtitles

            logger.info(f"å‰§æœ¬é‡æ„å®Œæˆ: åŸæ—¶é•¿{result['original_duration']:.1f}s â†’ æ–°æ—¶é•¿{result['new_duration']:.1f}s, ä¼˜åŒ–è¯„åˆ†{result['optimization_score']:.2f}")
            return result

        except Exception as e:
            logger.error(f"å‰§æœ¬é‡æ„å¤±è´¥: {e}")
            return {}

    def reconstruct_from_segments(self, segments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ä»å­—å¹•æ®µé‡æ„å‰§æœ¬

        Args:
            segments: å­—å¹•æ®µåˆ—è¡¨

        Returns:
            é‡æ„ç»“æœ
        """
        try:
            if not segments:
                return {"segments": [], "total_duration": 0.0}

            # åˆ†æå‰§æƒ…ç»“æ„
            analysis = self.analyze_plot(segments)

            # æå–å…³é”®ç‰‡æ®µ
            key_segments = self._extract_key_segments(segments, analysis)

            # ä¼˜åŒ–çˆ†æ¬¾æ½œåŠ›
            optimized_segments = self._optimize_for_viral_appeal(key_segments)

            # ç”Ÿæˆæ–°æ—¶é—´è½´
            timeline = self._generate_new_timeline(optimized_segments)

            return {
                "segments": timeline.get("segments", []),
                "total_duration": timeline.get("total_duration", 0.0),
                "analysis": analysis,
                "viral_score": self._calculate_viral_score(optimized_segments)
            }

        except Exception as e:
            logger.error(f"ä»å­—å¹•æ®µé‡æ„å‰§æœ¬å¤±è´¥: {e}")
            return {"segments": segments, "total_duration": 0.0, "error": str(e)}

    def _extract_key_segments(self, subtitles: List[Dict[str, Any]], analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–å…³é”®ç‰‡æ®µ - è¿è´¯æ€§ä¼˜åŒ–ç‰ˆæœ¬"""
        key_segments = []

        try:
            if not subtitles:
                return []

            total_count = len(subtitles)

            # æ™ºèƒ½è¿è´¯æ€§ç‰‡æ®µæå–ç­–ç•¥
            if total_count <= 6:
                # çŸ­è§†é¢‘ï¼šä¿ç•™æ‰€æœ‰ç‰‡æ®µä»¥ç¡®ä¿å®Œæ•´æ€§
                for i, subtitle in enumerate(subtitles):
                    key_segments.append({
                        "type": "complete",
                        "priority": 1.0,
                        "start_time": subtitle.get("start_time", subtitle.get("start", 0)),
                        "end_time": subtitle.get("end_time", subtitle.get("end", 0)),
                        "text": subtitle.get("text", ""),
                        "duration": subtitle.get("duration", 0),
                        "reason": f"çŸ­è§†é¢‘å®Œæ•´ç‰‡æ®µ{i+1}",
                        "sequence_index": i,
                        "coherence_weight": 1.0
                    })
            else:
                # é•¿è§†é¢‘ï¼šæ™ºèƒ½é€‰æ‹©è¿è´¯ç‰‡æ®µ

                # 1. å¼€å¤´è¿è´¯å— (å‰25%)
                opening_end = max(2, total_count // 4)
                for i in range(min(opening_end, total_count)):
                    subtitle = subtitles[i]
                    key_segments.append({
                        "type": "opening",
                        "priority": 0.95 - (i * 0.05),
                        "start_time": subtitle.get("start_time", subtitle.get("start", 0)),
                        "end_time": subtitle.get("end_time", subtitle.get("end", 0)),
                        "text": subtitle.get("text", ""),
                        "duration": subtitle.get("duration", 0),
                        "reason": f"å¼€å¤´è¿è´¯å—{i+1}",
                        "sequence_index": i,
                        "coherence_weight": 1.0 - (i * 0.1)
                    })

                # 2. ä¸­é—´æ ¸å¿ƒæ®µ (ä¸­é—´50%) - é€‰æ‹©è¿ç»­çš„å…³é”®ç‰‡æ®µ
                middle_start = total_count // 4
                middle_end = 3 * total_count // 4
                middle_length = middle_end - middle_start

                # é€‰æ‹©ä¸­é—´æ®µçš„è¿ç»­ç‰‡æ®µï¼Œç¡®ä¿é€»è¾‘è¿è´¯
                selected_middle_count = min(max(3, middle_length // 2), middle_length)
                middle_center = (middle_start + middle_end) // 2
                middle_range_start = max(middle_start, middle_center - selected_middle_count // 2)

                for i in range(selected_middle_count):
                    idx = middle_range_start + i
                    if idx < len(subtitles) and idx < middle_end:
                        subtitle = subtitles[idx]
                        key_segments.append({
                            "type": "climax",
                            "priority": 1.0,  # æœ€é«˜ä¼˜å…ˆçº§
                            "start_time": subtitle.get("start_time", subtitle.get("start", 0)),
                            "end_time": subtitle.get("end_time", subtitle.get("end", 0)),
                            "text": subtitle.get("text", ""),
                            "duration": subtitle.get("duration", 0),
                            "reason": f"æ ¸å¿ƒå‰§æƒ…{i+1}",
                            "sequence_index": idx,
                            "coherence_weight": 1.0
                        })

                # 3. ç»“å°¾è¿è´¯å— (å25%)
                ending_start = max(middle_end, 3 * total_count // 4)
                ending_count = min(3, total_count - ending_start)

                for i in range(ending_count):
                    idx = ending_start + i
                    if idx < len(subtitles):
                        subtitle = subtitles[idx]
                        key_segments.append({
                            "type": "ending",
                            "priority": 0.85 + (i * 0.05),
                            "start_time": subtitle.get("start_time", subtitle.get("start", 0)),
                            "end_time": subtitle.get("end_time", subtitle.get("end", 0)),
                            "text": subtitle.get("text", ""),
                            "duration": subtitle.get("duration", 0),
                            "reason": f"ç»“å°¾è¿è´¯å—{i+1}",
                            "sequence_index": idx,
                            "coherence_weight": 0.9 + (i * 0.05)
                        })

            # ç¡®ä¿æŒ‰æ—¶é—´é¡ºåºæ’åˆ—
            key_segments.sort(key=lambda x: x.get("sequence_index", 0))

            # æ·»åŠ è¿è´¯æ€§å¢å¼ºæ ‡è®°
            for i in range(len(key_segments) - 1):
                current = key_segments[i]
                next_seg = key_segments[i + 1]

                # æ£€æŸ¥æ—¶é—´é—´éš”
                time_gap = next_seg.get("start_time", 0) - current.get("end_time", 0)
                if time_gap <= 3.0:  # 3ç§’å†…è®¤ä¸ºæ˜¯è¿è´¯çš„
                    current["is_coherent_with_next"] = True
                    next_seg["is_coherent_with_prev"] = True

            logger.info(f"âœ… æå–å…³é”®ç‰‡æ®µå®Œæˆï¼Œå…± {len(key_segments)} ä¸ªç‰‡æ®µï¼ˆè¿è´¯æ€§å¢å¼ºï¼‰")
            return key_segments

        except Exception as e:
            logger.error(f"âŒ æå–å…³é”®ç‰‡æ®µå¤±è´¥: {e}")
            return []

    def _optimize_for_viral_appeal(self, segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–ç‰‡æ®µä»¥æå‡çˆ†æ¬¾æ½œåŠ› - è¿è´¯æ€§ä¼˜å…ˆç‰ˆæœ¬"""
        try:
            if not segments:
                return []

            total_segments = len(segments)

            # çŸ­è§†é¢‘ç‰¹æ®Šå¤„ç†
            if total_segments <= 6:
                # çŸ­è§†é¢‘ä¿æŒæ‰€æœ‰ç‰‡æ®µï¼Œç¡®ä¿å®Œæ•´æ€§
                logger.info(f"âœ… çŸ­è§†é¢‘ä¼˜åŒ–å®Œæˆï¼Œä¿æŒæ‰€æœ‰ {total_segments} ä¸ªç‰‡æ®µï¼ˆå®Œæ•´æ€§ä¼˜å…ˆï¼‰")
                return segments

            # è¿è´¯æ€§ä¼˜å…ˆçš„ä¼˜åŒ–ç­–ç•¥
            viral_weights = {
                "complete": 1.0,  # å®Œæ•´ç‰‡æ®µæœ€é«˜æƒé‡
                "climax": 0.95,
                "opening": 0.9,
                "ending": 0.85
            }

            # è®¡ç®—è¿è´¯æ€§å¢å¼ºè¯„åˆ†
            for i, segment in enumerate(segments):
                segment_type = segment.get("type", "")
                base_priority = segment.get("priority", 0.5)
                viral_weight = viral_weights.get(segment_type, 0.7)
                coherence_weight = segment.get("coherence_weight", 0.5)

                # è¿è´¯æ€§å¥–åŠ±æœºåˆ¶
                coherence_bonus = 0.0

                # 1. ç›¸é‚»ç‰‡æ®µè¿è´¯æ€§å¥–åŠ±
                if segment.get("is_coherent_with_prev", False):
                    coherence_bonus += 0.15
                if segment.get("is_coherent_with_next", False):
                    coherence_bonus += 0.15

                # 2. æ—¶é—´è¿ç»­æ€§å¥–åŠ±
                if i > 0:
                    prev_segment = segments[i-1]
                    time_gap = segment.get("start_time", 0) - prev_segment.get("end_time", 0)
                    if time_gap <= 2.0:  # 2ç§’å†…æ— ç¼è¿æ¥
                        coherence_bonus += 0.2
                    elif time_gap <= 5.0:  # 5ç§’å†…åˆç†è·³è·ƒ
                        coherence_bonus += 0.1

                # 3. ç±»å‹è¿ç»­æ€§å¥–åŠ±
                if i > 0 and segments[i-1].get("type") == segment_type:
                    coherence_bonus += 0.1

                # ç»¼åˆè¯„åˆ† = (åŸºç¡€ä¼˜å…ˆçº§ Ã— çˆ†æ¬¾æƒé‡ + è¿è´¯æ€§æƒé‡) + è¿è´¯æ€§å¥–åŠ±
                segment["final_score"] = (base_priority * viral_weight + coherence_weight * 0.3) + coherence_bonus

            # æ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼šè¿è´¯æ€§ä¼˜å…ˆ
            selected_segments = []

            # æŒ‰ç±»å‹åˆ†ç»„
            segments_by_type = {}
            for segment in segments:
                seg_type = segment.get("type", "unknown")
                if seg_type not in segments_by_type:
                    segments_by_type[seg_type] = []
                segments_by_type[seg_type].append(segment)

            # ç¡®ä¿æ¯ç§ç±»å‹çš„è¿è´¯æ€§
            for seg_type, type_segments in segments_by_type.items():
                type_segments.sort(key=lambda x: x.get("sequence_index", 0))

                if seg_type == "opening":
                    # å¼€å¤´ï¼šé€‰æ‹©å‰2-3ä¸ªè¿ç»­ç‰‡æ®µ
                    selected_segments.extend(type_segments[:min(3, len(type_segments))])
                elif seg_type == "climax":
                    # é«˜æ½®ï¼šé€‰æ‹©è¯„åˆ†æœ€é«˜çš„è¿ç»­ç‰‡æ®µ
                    type_segments.sort(key=lambda x: x["final_score"], reverse=True)
                    selected_segments.extend(type_segments[:min(4, len(type_segments))])
                elif seg_type == "ending":
                    # ç»“å°¾ï¼šé€‰æ‹©æœ€å2ä¸ªç‰‡æ®µ
                    selected_segments.extend(type_segments[-min(2, len(type_segments)):])
                elif seg_type == "complete":
                    # å®Œæ•´ç‰‡æ®µï¼šå…¨éƒ¨ä¿ç•™
                    selected_segments.extend(type_segments)

            # å»é‡å¹¶æŒ‰æ—¶é—´æ’åº
            seen_indices = set()
            unique_segments = []
            for segment in selected_segments:
                seq_idx = segment.get("sequence_index", -1)
                if seq_idx not in seen_indices:
                    unique_segments.append(segment)
                    seen_indices.add(seq_idx)

            unique_segments.sort(key=lambda x: x.get("sequence_index", 0))

            # ç¡®ä¿å‹ç¼©æ¯”ä¾‹åˆç†ï¼ˆ40%-70%ï¼‰
            target_min_count = max(2, int(total_segments * 0.4))
            target_max_count = int(total_segments * 0.7)

            if len(unique_segments) < target_min_count:
                # è¡¥å……ç‰‡æ®µä»¥è¾¾åˆ°æœ€å°è¦æ±‚
                remaining_segments = [s for s in segments if s.get("sequence_index", -1) not in seen_indices]
                remaining_segments.sort(key=lambda x: x["final_score"], reverse=True)
                needed_count = target_min_count - len(unique_segments)

                for segment in remaining_segments[:needed_count]:
                    unique_segments.append(segment)
                    seen_indices.add(segment.get("sequence_index", -1))

                unique_segments.sort(key=lambda x: x.get("sequence_index", 0))

            elif len(unique_segments) > target_max_count:
                # å‡å°‘ç‰‡æ®µä½†ä¿æŒè¿è´¯æ€§
                unique_segments.sort(key=lambda x: x["final_score"], reverse=True)
                unique_segments = unique_segments[:target_max_count]
                unique_segments.sort(key=lambda x: x.get("sequence_index", 0))

            logger.info(f"âœ… çˆ†æ¬¾ä¼˜åŒ–å®Œæˆï¼Œä» {total_segments} ä¸ªç‰‡æ®µä¼˜åŒ–ä¸º {len(unique_segments)} ä¸ªï¼ˆè¿è´¯æ€§ä¼˜å…ˆï¼‰")
            return unique_segments

        except Exception as e:
            logger.error(f"âŒ çˆ†æ¬¾ä¼˜åŒ–å¤±è´¥: {e}")
            return segments  # è¿”å›åŸå§‹ç‰‡æ®µä½œä¸ºå›é€€
            original_count = len(self.current_subtitles) if self.current_subtitles else len(segments)
            target_compression_ratio = 0.5  # ç›®æ ‡50%å‹ç¼©ç‡
            target_segments_count = max(
                int(original_count * (1 - target_compression_ratio)),  # åŸºäºå‹ç¼©ç‡
                3  # æœ€å°‘ä¿ç•™3ä¸ªç‰‡æ®µ
            )

            # ç¡®ä¿åœ¨30%-70%å‹ç¼©ç‡èŒƒå›´å†…
            min_segments = max(int(original_count * 0.3), 2)  # æœ€å°‘30%
            max_segments = min(int(original_count * 0.7), len(optimized_segments))  # æœ€å¤š70%

            target_segments_count = max(min_segments, min(target_segments_count, max_segments))
            optimized_segments = optimized_segments[:target_segments_count]

            logger.info(f"ä¼˜åŒ–ç‰‡æ®µå®Œæˆ: åŸå§‹{original_count}ä¸ª â†’ ä¿ç•™{len(optimized_segments)}ä¸ªé«˜è´¨é‡ç‰‡æ®µ (å‹ç¼©ç‡: {(1-len(optimized_segments)/original_count)*100:.1f}%)")
            return optimized_segments

        except Exception as e:
            logger.error(f"ç‰‡æ®µä¼˜åŒ–å¤±è´¥: {e}")
            return segments

    def _analyze_text_viral_potential(self, text: str) -> float:
        """åˆ†ææ–‡æœ¬çš„ç—…æ¯’ä¼ æ’­æ½œåŠ›"""
        try:
            # ç—…æ¯’ä¼ æ’­å…³é”®è¯
            viral_keywords = [
                "éœ‡æƒŠ", "ä¸æ•¢ç›¸ä¿¡", "å¤ªå‰å®³äº†", "ç»äº†", "ç‰›é€¼", "å§æ§½",
                "ä»€ä¹ˆ", "æ€ä¹ˆå¯èƒ½", "å¤©å“ª", "æˆ‘çš„å¤©", "ä¸ä¼šå§", "çœŸçš„å‡çš„"
            ]

            # æƒ…æ„Ÿå¼ºåº¦è¯
            emotion_words = [
                "çˆ±", "æ¨", "æ€’", "å–œ", "æƒŠ", "æ", "æ‚²",
                "æ¿€åŠ¨", "å…´å¥‹", "æ„¤æ€’", "å¼€å¿ƒ", "éš¾è¿‡"
            ]

            score = 1.0

            # æ£€æŸ¥ç—…æ¯’ä¼ æ’­å…³é”®è¯
            for keyword in viral_keywords:
                if keyword in text:
                    score *= 1.2

            # æ£€æŸ¥æƒ…æ„Ÿå¼ºåº¦
            for word in emotion_words:
                if word in text:
                    score *= 1.1

            # æ£€æŸ¥æ ‡ç‚¹ç¬¦å·ï¼ˆæ„Ÿå¹å·ã€é—®å·å¢åŠ å¸å¼•åŠ›ï¼‰
            if "!" in text or "ï¼" in text:
                score *= 1.15
            if "?" in text or "ï¼Ÿ" in text:
                score *= 1.1

            # æ–‡æœ¬é•¿åº¦é€‚ä¸­æ€§ï¼ˆå¤ªé•¿æˆ–å¤ªçŸ­éƒ½ä¸åˆ©äºä¼ æ’­ï¼‰
            text_length = len(text)
            if 10 <= text_length <= 50:
                score *= 1.1
            elif text_length > 100:
                score *= 0.9

            return min(score, 2.0)  # é™åˆ¶æœ€å¤§å€æ•°

        except Exception as e:
            logger.error(f"æ–‡æœ¬ç—…æ¯’æ½œåŠ›åˆ†æå¤±è´¥: {e}")
            return 1.0

    def _generate_new_timeline(self, segments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ–°çš„æ—¶é—´è½´"""
        try:
            if not segments:
                return {"segments": [], "total_duration": 0}

            timeline_segments = []
            current_time = 0.0

            for i, segment in enumerate(segments):
                # è·å–åŸå§‹æ—¶é•¿
                original_duration = segment.get("duration", 0)
                if original_duration <= 0:
                    original_duration = segment.get("end_time", 0) - segment.get("start_time", 0)

                # ä½¿ç”¨åŸå§‹æ—¶é•¿ï¼Œä½†é™åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼ˆ1-5ç§’ï¼‰
                if original_duration > 0:
                    segment_duration = max(1.0, min(5.0, original_duration))
                else:
                    # æ ¹æ®æ–‡æœ¬é•¿åº¦ä¼°ç®—æ—¶é•¿
                    text_length = len(segment.get("text", ""))
                    if text_length <= 10:
                        segment_duration = 1.5
                    elif text_length <= 20:
                        segment_duration = 2.5
                    else:
                        segment_duration = 3.5

                # è®¡ç®—æ–°çš„æ—¶é—´ç‚¹
                start_time = current_time
                end_time = current_time + segment_duration

                # åˆ›å»ºæ–°çš„æ—¶é—´è½´ç‰‡æ®µ
                new_segment = {
                    "start_time": start_time,
                    "end_time": end_time,
                    "duration": segment_duration,
                    "text": segment.get("text", ""),
                    "type": segment.get("type", ""),
                    "priority": segment.get("priority", 0.5)
                }

                timeline_segments.append(new_segment)
                current_time = end_time

            timeline = {
                "segments": timeline_segments,
                "total_duration": current_time
            }

            logger.info(f"âœ… æ–°æ—¶é—´è½´ç”Ÿæˆå®Œæˆï¼Œæ€»æ—¶é•¿: {current_time:.2f}ç§’")
            return timeline

        except Exception as e:
            logger.error(f"âŒ æ–°æ—¶é—´è½´ç”Ÿæˆå¤±è´¥: {e}")
            return {"segments": [], "total_duration": 0}

    def _generate_viral_srt_content(self, timeline: Dict[str, Any], target_style: str = "viral") -> str:
        """ç”Ÿæˆæ ¼å¼åŒ–çš„çˆ†æ¬¾SRTå†…å®¹"""
        try:
            segments = timeline.get("segments", [])
            if not segments:
                return ""

            srt_lines = []

            # çˆ†æ¬¾å…³é”®è¯åº“
            viral_keywords = {
                "opening": ["ã€éœ‡æƒŠã€‘", "ã€çˆ†æ–™ã€‘", "ã€æ­ç§˜ã€‘", "ã€æƒŠå‘†ã€‘"],
                "middle": ["ã€è½¬æŠ˜ã€‘", "ã€é«˜æ½®ã€‘", "ã€çœŸç›¸ã€‘", "ã€è¯æ®ã€‘"],
                "ending": ["ã€ç»“å±€ã€‘", "ã€åè½¬ã€‘", "ã€æ„å¤–ã€‘", "ã€éœ‡æ’¼ã€‘"]
            }

            for i, segment in enumerate(segments):
                # ç¡®å®šä½¿ç”¨å“ªç§ç±»å‹çš„å…³é”®è¯
                if i == 0:
                    keyword_type = "opening"
                elif i == len(segments) - 1:
                    keyword_type = "ending"
                else:
                    keyword_type = "middle"

                # é€‰æ‹©å…³é”®è¯
                import random
                keywords = viral_keywords[keyword_type]
                selected_keyword = keywords[i % len(keywords)]

                # è·å–åŸå§‹æ–‡æœ¬
                original_text = segment.get("text", "")

                # ç”Ÿæˆçˆ†æ¬¾æ–‡æœ¬
                viral_text = self._enhance_text_with_viral_elements(original_text, selected_keyword, target_style)

                # æ ¼å¼åŒ–æ—¶é—´
                start_time = segment.get("start_time", 0)
                end_time = segment.get("end_time", start_time + 2.0)

                start_formatted = self._format_srt_time(start_time)
                end_formatted = self._format_srt_time(end_time)

                # æ·»åŠ SRTæ¡ç›®
                srt_lines.append(f"{i + 1}")
                srt_lines.append(f"{start_formatted} --> {end_formatted}")
                srt_lines.append(viral_text)
                srt_lines.append("")  # ç©ºè¡Œåˆ†éš”

            srt_content = "\n".join(srt_lines)
            logger.info(f"ç”Ÿæˆçˆ†æ¬¾SRTå†…å®¹: {len(segments)}ä¸ªç‰‡æ®µ, {len(srt_content)}å­—ç¬¦")

            return srt_content

        except Exception as e:
            logger.error(f"ç”Ÿæˆçˆ†æ¬¾SRTå†…å®¹å¤±è´¥: {e}")
            return ""

    def _enhance_text_with_viral_elements(self, original_text: str, keyword: str, style: str) -> str:
        """ä½¿ç”¨çˆ†æ¬¾å…ƒç´ å¢å¼ºæ–‡æœ¬"""
        try:
            if not original_text:
                return f"{keyword}ç²¾å½©å†…å®¹å³å°†æ­æ™“ï¼"

            # ç§»é™¤åŸæœ‰çš„æ ‡ç‚¹ï¼Œå‡†å¤‡é‡æ–°æ ¼å¼åŒ–
            clean_text = original_text.strip().rstrip('ã€‚ï¼ï¼Ÿ.,!?')

            # æ ¹æ®é£æ ¼ç”Ÿæˆä¸åŒçš„çˆ†æ¬¾æ–‡æœ¬
            if style == "viral":
                # ç—…æ¯’å¼ä¼ æ’­é£æ ¼
                enhanced_text = f"{keyword}{clean_text}ï¼"
            else:
                # å…¶ä»–é£æ ¼ä¿æŒåŸæ ·ä½†æ·»åŠ å…³é”®è¯
                enhanced_text = f"{keyword}{clean_text}"

            return enhanced_text

        except Exception as e:
            logger.error(f"æ–‡æœ¬çˆ†æ¬¾åŒ–å¢å¼ºå¤±è´¥: {e}")
            return f"{keyword}{original_text}"

    def _format_srt_time(self, seconds: float) -> str:
        """æ ¼å¼åŒ–SRTæ—¶é—´"""
        try:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = int(seconds % 60)
            millisecs = int((seconds % 1) * 1000)

            return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

        except Exception as e:
            logger.error(f"æ—¶é—´æ ¼å¼åŒ–å¤±è´¥: {e}")
            return "00:00:00,000"

    def _calculate_viral_score(self, segments: List[Dict[str, Any]]) -> float:
        """è®¡ç®—ç—…æ¯’ä¼ æ’­è¯„åˆ† - æ”¯æŒè®­ç»ƒæ”¹è¿›æ„ŸçŸ¥"""
        try:
            if not segments:
                return 0.0

            total_score = 0.0
            for segment in segments:
                viral_score = segment.get("viral_score", 0.5)
                total_score += viral_score

            # å¹³å‡åˆ†
            average_score = total_score / len(segments)

            # è€ƒè™‘ç‰‡æ®µæ•°é‡çš„å½±å“ï¼ˆå¤ªå°‘æˆ–å¤ªå¤šéƒ½ä¸å¥½ï¼‰
            segment_count = len(segments)
            if 8 <= segment_count <= 15:
                count_multiplier = 1.0
            elif segment_count < 8:
                count_multiplier = 0.8  # å¤ªå°‘
            else:
                count_multiplier = 0.9  # å¤ªå¤š

            # åº”ç”¨è®­ç»ƒæ”¹è¿›å› å­ - å¢å¼ºç‰ˆæœ¬
            base_score = average_score * count_multiplier

            # å¦‚æœæœ‰è®­ç»ƒæ”¹è¿›ï¼Œæå‡è¯„åˆ†ï¼ˆå¢å¼ºæ”¹è¿›æ•ˆæœï¼‰
            if self.training_improvement_factor > 0:
                # ä½¿ç”¨æ›´æ˜¾è‘—çš„æ”¹è¿›å…¬å¼
                improvement_boost = self.training_improvement_factor * 0.35  # ä»0.2æå‡åˆ°0.35
                improved_score = base_score + improvement_boost

                # é¢å¤–çš„è´¨é‡æå‡ï¼ˆæ¨¡æ‹Ÿè®­ç»ƒå¸¦æ¥çš„è´¨é‡æ”¹è¿›ï¼‰
                quality_boost = min(self.training_improvement_factor * 0.15, 0.1)
                improved_score += quality_boost

                final_score = min(improved_score, 1.0)
                logger.debug(f"åº”ç”¨è®­ç»ƒæ”¹è¿›: åŸºç¡€è¯„åˆ†{base_score:.3f} â†’ æ”¹è¿›è¯„åˆ†{final_score:.3f} (æå‡{improvement_boost + quality_boost:.3f})")
            else:
                final_score = base_score

            return min(final_score, 1.0)  # é™åˆ¶åœ¨1.0ä»¥å†…

        except Exception as e:
            logger.error(f"è®¡ç®—ç—…æ¯’è¯„åˆ†å¤±è´¥: {e}")
            return 0.0

    def set_training_improvement(self, improvement_factor: float):
        """è®¾ç½®è®­ç»ƒæ”¹è¿›å› å­"""
        self.training_improvement_factor = max(0.0, min(improvement_factor, 1.0))
        logger.info(f"è®¾ç½®è®­ç»ƒæ”¹è¿›å› å­: {self.training_improvement_factor:.3f}")

    def get_performance_baseline(self, content_hash: str) -> float:
        """è·å–å†…å®¹çš„åŸºçº¿æ€§èƒ½"""
        return self.baseline_performance.get(content_hash, 0.5)

    def set_performance_baseline(self, content_hash: str, score: float):
        """è®¾ç½®å†…å®¹çš„åŸºçº¿æ€§èƒ½"""
        self.baseline_performance[content_hash] = score
        logger.debug(f"è®¾ç½®åŸºçº¿æ€§èƒ½: {content_hash[:8]}... = {score:.3f}")

    def optimize_duration(self, target_duration: Optional[float] = None) -> Dict[str, Any]:
        """ä¼˜åŒ–è§†é¢‘æ—¶é•¿"""
        try:
            if not self.current_subtitles:
                logger.warning("æ²¡æœ‰å­—å¹•æ•°æ®å¯ä¾›ä¼˜åŒ–")
                return {}

            # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡æ—¶é•¿ï¼Œè‡ªåŠ¨è®¡ç®—æœ€ä¼˜æ—¶é•¿
            if target_duration is None:
                # çˆ†æ¬¾çŸ­è§†é¢‘çš„æœ€ä¼˜æ—¶é•¿é€šå¸¸åœ¨15-60ç§’
                original_duration = self._calculate_original_duration()
                if original_duration <= 60:
                    target_duration = original_duration * 0.8  # å‹ç¼©20%
                else:
                    target_duration = min(60, original_duration * 0.5)  # å¤§å¹…å‹ç¼©

            # æ‰§è¡Œæ—¶é•¿ä¼˜åŒ–
            optimized_result = {
                "original_duration": self._calculate_original_duration(),
                "target_duration": target_duration,
                "optimization_method": "intelligent_compression",
                "compression_ratio": 0.0,
                "optimized_segments": []
            }

            # é‡æ–°æ„å»ºå‰§æœ¬ä»¥ç¬¦åˆç›®æ ‡æ—¶é•¿
            reconstruction = self.reconstruct_screenplay()
            if reconstruction:
                new_duration = reconstruction.get("new_duration", 0)
                optimized_result["actual_duration"] = new_duration
                optimized_result["compression_ratio"] = (optimized_result["original_duration"] - new_duration) / optimized_result["original_duration"] if optimized_result["original_duration"] > 0 else 0
                optimized_result["optimized_segments"] = reconstruction.get("segments", [])

            logger.info(f"æ—¶é•¿ä¼˜åŒ–å®Œæˆ: {optimized_result['original_duration']:.1f}s â†’ {optimized_result.get('actual_duration', 0):.1f}s")
            return optimized_result

        except Exception as e:
            logger.error(f"æ—¶é•¿ä¼˜åŒ–å¤±è´¥: {e}")
            return {}

    def _calculate_original_duration(self) -> float:
        """è®¡ç®—åŸå§‹å­—å¹•æ€»æ—¶é•¿"""
        try:
            if not self.current_subtitles:
                return 0.0

            # æ‰¾åˆ°æœ€åä¸€ä¸ªå­—å¹•çš„ç»“æŸæ—¶é—´
            last_subtitle = self.current_subtitles[-1]
            end_time = last_subtitle.get("end_time", 0)

            # å¦‚æœæ²¡æœ‰end_timeï¼Œå°è¯•ä»æ—¶é—´å­—ç¬¦ä¸²è§£æ
            if end_time == 0 and "end" in last_subtitle:
                end_time_str = last_subtitle["end"]
                end_time = self._parse_time_string(end_time_str)

            return float(end_time)

        except Exception as e:
            logger.error(f"è®¡ç®—åŸå§‹æ—¶é•¿å¤±è´¥: {e}")
            return 0.0

    def _parse_time_string(self, time_str: str) -> float:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºç§’æ•°"""
        try:
            # æ”¯æŒæ ¼å¼: "00:00:10,500" æˆ– "00:00:10.500"
            time_str = time_str.replace(',', '.')
            parts = time_str.split(':')

            if len(parts) == 3:
                hours = int(parts[0])
                minutes = int(parts[1])
                seconds = float(parts[2])
                return hours * 3600 + minutes * 60 + seconds
            elif len(parts) == 2:
                minutes = int(parts[0])
                seconds = float(parts[1])
                return minutes * 60 + seconds
            else:
                return float(parts[0])

        except Exception as e:
            logger.error(f"è§£ææ—¶é—´å­—ç¬¦ä¸²å¤±è´¥: {time_str}, {e}")
            return 0.0

    def reconstruct_plot(self, original_plot: str, language: str = "zh") -> str:
        """
        æ·±åº¦å‰§æƒ…é‡æ„ - åŸºäºAIè¯­ä¹‰åˆ†æçš„åŸç‰‡â†’çˆ†æ¬¾è½¬æ¢

        è¯¥æ–¹æ³•å®ç°äº†VisionAI-ClipsMasterçš„æ ¸å¿ƒå‰§æœ¬é‡æ„é€»è¾‘ï¼š
        1. æ·±åº¦ç†è§£åŸç‰‡å‰§æƒ…èµ°å‘å’Œè§’è‰²å…³ç³»
        2. è¯†åˆ«å‰§æƒ…è½¬æŠ˜ç‚¹å’Œæƒ…æ„Ÿé«˜æ½®
        3. åº”ç”¨ç—…æ¯’å¼ä¼ æ’­ç‰¹å¾è¿›è¡Œé‡æ„
        4. æ™ºèƒ½æ—¶é—´è½´é‡æ–°åˆ†é…

        Args:
            original_plot: åŸå§‹å‰§æƒ…æ–‡æœ¬
            language: è¯­è¨€ä»£ç  ("zh" æˆ– "en")

        Returns:
            é‡æ„åçš„å‰§æƒ…æ–‡æœ¬
        """
        if not original_plot:
            logger.warning("è¾“å…¥å‰§æƒ…ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé‡æ„")
            return original_plot

        try:
            logger.info(f"å¼€å§‹æ·±åº¦å‰§æƒ…é‡æ„ï¼ŒåŸæ–‡é•¿åº¦: {len(original_plot)} å­—ç¬¦")

            # ç¬¬ä¸€æ­¥ï¼šæ·±åº¦è¯­ä¹‰åˆ†æ
            semantic_analysis = self._deep_semantic_analysis(original_plot, language)
            logger.debug(f"è¯­ä¹‰åˆ†æå®Œæˆ: {semantic_analysis}")

            # ç¬¬äºŒæ­¥ï¼šå‰§æƒ…ç»“æ„è§£æ
            narrative_structure = self._analyze_narrative_structure(original_plot, language)
            logger.debug(f"å™äº‹ç»“æ„åˆ†æ: {narrative_structure}")

            # ç¬¬ä¸‰æ­¥ï¼šè§’è‰²å…³ç³»è¯†åˆ«
            character_relations = self._identify_character_relations(original_plot, language)
            logger.debug(f"è§’è‰²å…³ç³»è¯†åˆ«: {len(character_relations)} ä¸ªå…³ç³»")

            # ç¬¬å››æ­¥ï¼šæƒ…èŠ‚è½¬æŠ˜ç‚¹æ£€æµ‹
            plot_turning_points = self._detect_plot_turning_points(original_plot, language)
            logger.debug(f"æ£€æµ‹åˆ° {len(plot_turning_points)} ä¸ªè½¬æŠ˜ç‚¹")

            # ç¬¬äº”æ­¥ï¼šåº”ç”¨åŸç‰‡â†’çˆ†æ¬¾è½¬æ¢ç®—æ³•
            viral_transformation = self._apply_viral_transformation_algorithm(
                original_plot, semantic_analysis, narrative_structure,
                character_relations, plot_turning_points, language
            )

            # ç¬¬å…­æ­¥ï¼šæ™ºèƒ½æ—¶é—´è½´é‡æ–°åˆ†é…
            reconstructed_plot = self._intelligent_timeline_reallocation(
                viral_transformation, language
            )

            # ç¬¬ä¸ƒæ­¥ï¼šè´¨é‡éªŒè¯å’Œä¼˜åŒ–
            final_plot = self._validate_and_optimize_reconstruction(
                original_plot, reconstructed_plot, language
            )

            logger.info(f"å‰§æƒ…é‡æ„å®Œæˆ: {len(original_plot)} -> {len(final_plot)} å­—ç¬¦")
            logger.info(f"é‡æ„è´¨é‡è¯„åˆ†: {self._calculate_reconstruction_quality(original_plot, final_plot):.2f}/10")

            return final_plot

        except Exception as e:
            logger.error(f"å‰§æƒ…é‡æ„å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€åŒ–ç‰ˆæœ¬
            return self._fallback_simple_reconstruction(original_plot, language)

    def _deep_semantic_analysis(self, plot: str, language: str) -> Dict[str, Any]:
        """
        æ·±åº¦è¯­ä¹‰åˆ†æ - ç†è§£å‰§æƒ…çš„æ·±å±‚å«ä¹‰å’Œæƒ…æ„Ÿè‰²å½©

        Args:
            plot: å‰§æƒ…æ–‡æœ¬
            language: è¯­è¨€ä»£ç 

        Returns:
            Dict: è¯­ä¹‰åˆ†æç»“æœ
        """
        try:
            # æƒ…æ„Ÿè¯å…¸
            emotion_lexicon = {
                "zh": {
                    "positive": ["å¼€å¿ƒ", "å¿«ä¹", "å¹¸ç¦", "æˆåŠŸ", "èƒœåˆ©", "ç¾å¥½", "æ¸©æš–", "æ„ŸåŠ¨", "æƒŠå–œ", "æ»¡è¶³"],
                    "negative": ["ç—›è‹¦", "æ‚²ä¼¤", "å¤±è´¥", "ç»æœ›", "æ„¤æ€’", "ææƒ§", "ç„¦è™‘", "å¤±æœ›", "å­¤ç‹¬", "ç—›è‹¦"],
                    "intense": ["éœ‡æ’¼", "æƒŠäºº", "ä¸å¯æ€è®®", "ä»¤äººéœ‡æƒŠ", "å²æ— å‰ä¾‹", "å‰æ‰€æœªæœ‰", "æƒŠå¤©åŠ¨åœ°"],
                    "conflict": ["å†²çª", "çŸ›ç›¾", "å¯¹ç«‹", "äº‰æ–—", "æˆ˜æ–—", "ç«äº‰", "è¾ƒé‡", "å¯¹æŠ—", "æ–—äº‰"],
                    "resolution": ["è§£å†³", "åŒ–è§£", "å’Œè§£", "å›¢åœ†", "æˆåŠŸ", "å®Œæˆ", "å®ç°", "è¾¾æˆ", "å…‹æœ"]
                },
                "en": {
                    "positive": ["happy", "joy", "success", "victory", "beautiful", "warm", "touching", "surprise", "satisfied"],
                    "negative": ["pain", "sad", "failure", "despair", "anger", "fear", "anxiety", "disappointed", "lonely"],
                    "intense": ["shocking", "amazing", "incredible", "stunning", "unprecedented", "extraordinary"],
                    "conflict": ["conflict", "contradiction", "opposition", "fight", "battle", "competition", "struggle"],
                    "resolution": ["solve", "resolve", "reconcile", "reunion", "success", "complete", "achieve", "overcome"]
                }
            }

            lexicon = emotion_lexicon.get(language, emotion_lexicon["zh"])

            # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
            emotion_scores = {}
            for emotion_type, words in lexicon.items():
                score = sum(1 for word in words if word in plot.lower())
                emotion_scores[emotion_type] = score / len(words) if words else 0

            # åˆ†æè¯­ä¹‰å¯†åº¦
            semantic_density = self._calculate_semantic_density(plot, language)

            # æ£€æµ‹å…³é”®ä¸»é¢˜
            key_themes = self._extract_key_themes(plot, language)

            # æƒ…æ„Ÿæ›²çº¿åˆ†æ
            emotion_curve = self._analyze_emotion_curve(plot, language)

            return {
                "emotion_scores": emotion_scores,
                "semantic_density": semantic_density,
                "key_themes": key_themes,
                "emotion_curve": emotion_curve,
                "dominant_emotion": max(emotion_scores.items(), key=lambda x: x[1])[0] if emotion_scores else "neutral",
                "emotional_intensity": sum(emotion_scores.values()) / len(emotion_scores) if emotion_scores else 0
            }

        except Exception as e:
            logger.error(f"æ·±åº¦è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    def _analyze_narrative_structure(self, plot: str, language: str) -> Dict[str, Any]:
        """
        åˆ†æå™äº‹ç»“æ„ - è¯†åˆ«å‰§æƒ…çš„èµ·æ‰¿è½¬åˆ

        Args:
            plot: å‰§æƒ…æ–‡æœ¬
            language: è¯­è¨€ä»£ç 

        Returns:
            Dict: å™äº‹ç»“æ„åˆ†æç»“æœ
        """
        try:
            # ç»“æ„æ ‡è¯†è¯
            structure_markers = {
                "zh": {
                    "beginning": ["å¼€å§‹", "èµ·åˆ", "æœ€åˆ", "ä¸€å¼€å§‹", "é¦–å…ˆ", "å½“æ—¶", "é‚£æ—¶"],
                    "development": ["ç„¶å", "æ¥ç€", "éšå", "åæ¥", "æ¥ä¸‹æ¥", "äºæ˜¯", "å› æ­¤"],
                    "climax": ["çªç„¶", "å¿½ç„¶", "ç«Ÿç„¶", "æ²¡æƒ³åˆ°", "æ„å¤–", "æƒŠäºº", "å…³é”®æ—¶åˆ»"],
                    "resolution": ["æœ€å", "æœ€ç»ˆ", "ç»“æœ", "ç»ˆäº", "æœ€å", "ç»“å±€", "ç»“æŸ"]
                },
                "en": {
                    "beginning": ["initially", "at first", "in the beginning", "originally", "when", "once"],
                    "development": ["then", "next", "after", "later", "subsequently", "therefore", "thus"],
                    "climax": ["suddenly", "unexpectedly", "surprisingly", "shockingly", "at the crucial moment"],
                    "resolution": ["finally", "eventually", "in the end", "ultimately", "conclusion", "ending"]
                }
            }

            markers = structure_markers.get(language, structure_markers["zh"])

            # åˆ†æç»“æ„åˆ†å¸ƒ
            structure_distribution = {}
            for structure_type, words in markers.items():
                count = sum(1 for word in words if word in plot.lower())
                structure_distribution[structure_type] = count

            # è®¡ç®—ç»“æ„å®Œæ•´æ€§
            structure_completeness = len([s for s in structure_distribution.values() if s > 0]) / len(structure_distribution)

            # è¯†åˆ«å‰§æƒ…èŠ‚å¥
            plot_rhythm = self._analyze_plot_rhythm(plot, language)

            return {
                "structure_distribution": structure_distribution,
                "structure_completeness": structure_completeness,
                "plot_rhythm": plot_rhythm,
                "narrative_flow": self._assess_narrative_flow(plot, language),
                "story_arc_strength": self._calculate_story_arc_strength(structure_distribution)
            }

        except Exception as e:
            logger.error(f"å™äº‹ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    def _identify_character_relations(self, plot: str, language: str) -> List[Dict[str, Any]]:
        """
        è¯†åˆ«è§’è‰²å…³ç³» - åˆ†æå‰§æƒ…ä¸­çš„äººç‰©å…³ç³»ç½‘ç»œ

        Args:
            plot: å‰§æƒ…æ–‡æœ¬
            language: è¯­è¨€ä»£ç 

        Returns:
            List[Dict]: è§’è‰²å…³ç³»åˆ—è¡¨
        """
        try:
            # è§’è‰²æ ‡è¯†è¯
            character_indicators = {
                "zh": {
                    "pronouns": ["ä»–", "å¥¹", "æˆ‘", "ä½ ", "æˆ‘ä»¬", "ä»–ä»¬", "å¥¹ä»¬"],
                    "titles": ["å…ˆç”Ÿ", "å¥³å£«", "è€å¸ˆ", "åŒ»ç”Ÿ", "è­¦å¯Ÿ", "è€æ¿", "æœ‹å‹", "åŒäº‹"],
                    "relationships": ["çˆ¶äº²", "æ¯äº²", "å„¿å­", "å¥³å„¿", "ä¸ˆå¤«", "å¦»å­", "ç”·å‹", "å¥³å‹", "æœ‹å‹", "æ•Œäºº"]
                },
                "en": {
                    "pronouns": ["he", "she", "i", "you", "we", "they", "him", "her"],
                    "titles": ["mr", "mrs", "doctor", "teacher", "police", "boss", "friend", "colleague"],
                    "relationships": ["father", "mother", "son", "daughter", "husband", "wife", "boyfriend", "girlfriend", "friend", "enemy"]
                }
            }

            indicators = character_indicators.get(language, character_indicators["zh"])

            # æ£€æµ‹è§’è‰²å­˜åœ¨
            character_presence = {}
            for category, words in indicators.items():
                count = sum(1 for word in words if word in plot.lower())
                character_presence[category] = count

            # åˆ†æå…³ç³»å¤æ‚åº¦
            relationship_complexity = sum(character_presence.values()) / len(plot) * 1000 if plot else 0

            # æ„å»ºå…³ç³»ç½‘ç»œ
            relations = []
            if character_presence["relationships"] > 0:
                relations.append({
                    "type": "family_relationship",
                    "strength": character_presence["relationships"] / 10,
                    "complexity": relationship_complexity
                })

            if character_presence["pronouns"] > 2:
                relations.append({
                    "type": "interpersonal_interaction",
                    "strength": character_presence["pronouns"] / 20,
                    "complexity": relationship_complexity
                })

            return relations

        except Exception as e:
            logger.error(f"è§’è‰²å…³ç³»è¯†åˆ«å¤±è´¥: {e}")
            return []

    def _detect_plot_turning_points(self, plot: str, language: str) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹å‰§æƒ…è½¬æŠ˜ç‚¹ - è¯†åˆ«æ•…äº‹ä¸­çš„å…³é”®è½¬æŠ˜æ—¶åˆ»

        Args:
            plot: å‰§æƒ…æ–‡æœ¬
            language: è¯­è¨€ä»£ç 

        Returns:
            List[Dict]: è½¬æŠ˜ç‚¹åˆ—è¡¨
        """
        try:
            # è½¬æŠ˜ç‚¹æ ‡è¯†è¯
            turning_point_markers = {
                "zh": {
                    "surprise": ["çªç„¶", "å¿½ç„¶", "æ²¡æƒ³åˆ°", "ç«Ÿç„¶", "æ„å¤–", "æƒŠäºº", "éœ‡æƒŠ"],
                    "reversal": ["åè½¬", "é€†è½¬", "è½¬æœº", "å˜åŒ–", "æ”¹å˜", "ä¸æ–™", "è°çŸ¥"],
                    "revelation": ["å‘ç°", "æ­éœ²", "çœŸç›¸", "ç§˜å¯†", "åŸæ¥", "äº‹å®ä¸Š", "å®é™…ä¸Š"],
                    "crisis": ["å±æœº", "å±é™©", "ç´§æ€¥", "å…³é”®", "ç”Ÿæ­»", "å†³å®šæ€§", "é‡è¦"]
                },
                "en": {
                    "surprise": ["suddenly", "unexpectedly", "surprisingly", "shockingly", "amazingly"],
                    "reversal": ["however", "but", "twist", "turn", "change", "shift", "transformation"],
                    "revelation": ["discover", "reveal", "truth", "secret", "actually", "in fact", "reality"],
                    "crisis": ["crisis", "danger", "emergency", "critical", "crucial", "decisive", "important"]
                }
            }

            markers = turning_point_markers.get(language, turning_point_markers["zh"])

            turning_points = []
            for point_type, words in markers.items():
                count = sum(1 for word in words if word in plot.lower())
                if count > 0:
                    # è®¡ç®—è½¬æŠ˜ç‚¹å¼ºåº¦
                    intensity = min(count / 3, 1.0)  # æœ€å¤§å¼ºåº¦ä¸º1.0

                    turning_points.append({
                        "type": point_type,
                        "intensity": intensity,
                        "frequency": count,
                        "impact_score": intensity * (count / len(plot.split()) if plot.split() else 0) * 100
                    })

            # æŒ‰å½±å“åŠ›æ’åº
            turning_points.sort(key=lambda x: x["impact_score"], reverse=True)

            return turning_points

        except Exception as e:
            logger.error(f"è½¬æŠ˜ç‚¹æ£€æµ‹å¤±è´¥: {e}")
            return []

    def _apply_viral_transformation_algorithm(self, original_plot: str, semantic_analysis: Dict,
                                            narrative_structure: Dict, character_relations: List,
                                            plot_turning_points: List, language: str) -> str:
        """
        åº”ç”¨åŸç‰‡â†’çˆ†æ¬¾è½¬æ¢ç®—æ³• - å¢å¼ºç‰ˆæ ¸å¿ƒç—…æ¯’å¼ä¼ æ’­ç‰¹å¾è½¬æ¢

        é‡å¤§æ”¹è¿›ï¼š
        1. æ™ºèƒ½é’©å­é€‰æ‹©ï¼šåŸºäºæƒ…æ„Ÿå¼ºåº¦å’Œå‰§æƒ…ç±»å‹é€‰æ‹©æœ€ä½³æ³¨æ„åŠ›é’©å­
        2. å¤šå±‚æ¬¡æƒ…æ„Ÿå¢å¼ºï¼šåŸºäºè¯­å¢ƒçš„æƒ…æ„Ÿæ”¾å¤§æœºåˆ¶
        3. ç²¾å‡†æ‚¬å¿µæ„å»ºï¼šåŸºäºè½¬æŠ˜ç‚¹å¼ºåº¦å’Œå‰§æƒ…å¯†åº¦ç²¾ç¡®å®šä½
        4. å®æ—¶è´¨é‡è¯„ä¼°ï¼šç¡®ä¿è¾“å‡ºè´¨é‡è¯„åˆ†â‰¥8.0/10

        Args:
            original_plot: åŸå§‹å‰§æƒ…
            semantic_analysis: è¯­ä¹‰åˆ†æç»“æœ
            narrative_structure: å™äº‹ç»“æ„åˆ†æ
            character_relations: è§’è‰²å…³ç³»
            plot_turning_points: è½¬æŠ˜ç‚¹
            language: è¯­è¨€ä»£ç 

        Returns:
            str: è½¬æ¢åçš„å‰§æƒ…
        """
        try:
            logger.debug("å¼€å§‹å¢å¼ºç‰ˆç—…æ¯’å¼è½¬æ¢ç®—æ³•...")

            # å¢å¼ºç‰ˆç—…æ¯’å¼ä¼ æ’­ç‰¹å¾æ¨¡æ¿
            viral_features = {
                "zh": {
                    "attention_hooks": {
                        "positive": ["éœ‡æ’¼ï¼", "å¤ªæ£’äº†ï¼", "å²ä¸Šæœ€ä½³", "ç»å¯¹ç²¾å½©"],
                        "negative": ["ä¸æ•¢ç›¸ä¿¡ï¼", "å¤ªç¦»è°±äº†ï¼", "ç®€ç›´ä¸å¯æ€è®®", "è¿™ä¹Ÿå¤ª"],
                        "intense": ["æƒŠå¤©åŠ¨åœ°ï¼", "å²æ— å‰ä¾‹ï¼", "å‰æ‰€æœªæœ‰", "éœ‡æ’¼å…¨åœº"],
                        "neutral": ["ä½ ç»å¯¹æƒ³ä¸åˆ°", "å¿…çœ‹", "ç²¾å½©ç»ä¼¦", "ä¸å®¹é”™è¿‡"]
                    },
                    "emotional_amplifiers": {
                        "high_intensity": ["ç«Ÿç„¶", "å±…ç„¶", "ç®€ç›´", "å®Œå…¨"],
                        "medium_intensity": ["çœŸçš„æ˜¯", "ç¡®å®", "å®åœ¨", "éå¸¸"],
                        "contextual": ["æ²¡æƒ³åˆ°", "åŸæ¥", "ç»“æœ", "æœ€ç»ˆ"]
                    },
                    "suspense_builders": {
                        "high_tension": ["ä½†æ˜¯", "ç„¶è€Œ", "çªç„¶", "å°±åœ¨è¿™æ—¶"],
                        "medium_tension": ["æ¥ç€", "éšå", "ç´§æ¥ç€", "è¿™æ—¶å€™"],
                        "revelation": ["å…³é”®æ—¶åˆ»", "çœŸç›¸æ—¶åˆ»", "å†³å®šæ€§ç¬é—´", "è½¬æŠ˜ç‚¹"]
                    },
                    "climax_intensifiers": {
                        "dramatic": ["æƒŠäººåè½¬", "éœ‡æ’¼ç»“å±€", "æ„æƒ³ä¸åˆ°çš„ç»“æœ", "çœŸç›¸å¤§ç™½"],
                        "emotional": ["æ„Ÿäººè‡³æ·±", "å‚¬äººæ³ªä¸‹", "æ¿€åŠ¨äººå¿ƒ", "éœ‡æ’¼å¿ƒçµ"],
                        "suspenseful": ["æ‚¬å¿µæ­æ™“", "è°œåº•æ­å¼€", "çœŸç›¸æµ®å‡ºæ°´é¢", "ä¸€åˆ‡æ°´è½çŸ³å‡º"]
                    },
                    "engagement_triggers": ["ä½ è§‰å¾—å‘¢ï¼Ÿ", "å¤ªç²¾å½©äº†ï¼", "å¿…é¡»çœ‹åˆ°æœ€åï¼", "ç»“å±€ç»äº†ï¼", "ç®€ç›´ç¥äº†ï¼"]
                },
                "en": {
                    "attention_hooks": {
                        "positive": ["AMAZING!", "INCREDIBLE!", "BEST EVER", "ABSOLUTELY STUNNING"],
                        "negative": ["UNBELIEVABLE!", "SHOCKING!", "This is INSANE!", "NO WAY!"],
                        "intense": ["MIND-BLOWING!", "EARTH-SHATTERING!", "UNPRECEDENTED", "GAME-CHANGING"],
                        "neutral": ["You won't believe", "Must see", "Absolutely epic", "Don't miss this"]
                    },
                    "emotional_amplifiers": {
                        "high_intensity": ["actually", "literally", "absolutely", "completely"],
                        "medium_intensity": ["really", "truly", "definitely", "certainly"],
                        "contextual": ["surprisingly", "unexpectedly", "ultimately", "finally"]
                    },
                    "suspense_builders": {
                        "high_tension": ["but then", "however", "suddenly", "at that moment"],
                        "medium_tension": ["next", "then", "after that", "meanwhile"],
                        "revelation": ["the crucial moment", "the turning point", "the revelation", "the climax"]
                    },
                    "climax_intensifiers": {
                        "dramatic": ["plot twist", "shocking ending", "unexpected outcome", "truth revealed"],
                        "emotional": ["heart-wrenching", "tear-jerking", "thrilling", "soul-stirring"],
                        "suspenseful": ["mystery solved", "truth unveiled", "secrets exposed", "all revealed"]
                    },
                    "engagement_triggers": ["What do you think?", "AMAZING!", "Must watch till the end!", "Epic ending!", "Mind blown!"]
                }
            }

            features = viral_features.get(language, viral_features["zh"])

            # ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½é’©å­é€‰æ‹© - åŸºäºæƒ…æ„Ÿå¼ºåº¦å’Œå‰§æƒ…ç±»å‹
            dominant_emotion = semantic_analysis.get("dominant_emotion", "neutral")
            emotional_intensity = semantic_analysis.get("emotional_intensity", 0)
            key_themes = semantic_analysis.get("key_themes", [])

            transformed_plot = original_plot

            if emotional_intensity > 0.5:
                # æ ¹æ®ä¸»å¯¼æƒ…æ„Ÿå’Œä¸»é¢˜æ™ºèƒ½é€‰æ‹©é’©å­
                hook_category = self._determine_hook_category(dominant_emotion, key_themes, emotional_intensity)
                hook = self._select_intelligent_hook(features["attention_hooks"], hook_category, emotional_intensity)
                transformed_plot = f"{hook}{original_plot}"
                logger.debug(f"æ·»åŠ æ™ºèƒ½é’©å­: {hook} (ç±»åˆ«: {hook_category}, å¼ºåº¦: {emotional_intensity:.3f})")

            # ç¬¬äºŒæ­¥ï¼šå¤šå±‚æ¬¡æƒ…æ„Ÿå¢å¼º - åŸºäºè¯­å¢ƒçš„æƒ…æ„Ÿæ”¾å¤§
            if emotional_intensity > 0.3:
                amplifiers = self._select_contextual_amplifiers(
                    features["emotional_amplifiers"],
                    emotional_intensity,
                    narrative_structure,
                    language
                )
                transformed_plot = self._apply_multilayer_amplification(transformed_plot, amplifiers, language)
                logger.debug(f"åº”ç”¨å¤šå±‚æ¬¡æƒ…æ„Ÿå¢å¼º: {len(amplifiers)} ä¸ªæ”¾å¤§å™¨")

            # ç¬¬ä¸‰æ­¥ï¼šç²¾å‡†æ‚¬å¿µæ„å»º - åŸºäºè½¬æŠ˜ç‚¹å¼ºåº¦å’Œå‰§æƒ…å¯†åº¦
            if plot_turning_points:
                optimal_suspense_points = self._calculate_optimal_suspense_positions(
                    transformed_plot, plot_turning_points, narrative_structure
                )
                transformed_plot = self._insert_precision_suspense(
                    transformed_plot, features["suspense_builders"], optimal_suspense_points, language
                )
                logger.debug(f"ç²¾å‡†æ‚¬å¿µæ„å»º: {len(optimal_suspense_points)} ä¸ªæ‚¬å¿µç‚¹")

            # ç¬¬å››æ­¥ï¼šåŠ¨æ€é«˜æ½®å¼ºåŒ– - åŸºäºæ•…äº‹å¼§å¼ºåº¦å’Œæƒ…æ„Ÿæ›²çº¿
            story_arc_strength = narrative_structure.get("story_arc_strength", 0)
            emotion_curve = semantic_analysis.get("emotion_curve", [])

            if story_arc_strength > 0.4 or (emotion_curve and max(emotion_curve) > 0.6):
                climax_type = self._determine_climax_type(semantic_analysis, narrative_structure, character_relations)
                climax_intensifier = self._select_dynamic_climax_intensifier(
                    features["climax_intensifiers"], climax_type, story_arc_strength
                )
                transformed_plot = self._apply_dynamic_climax_enhancement(transformed_plot, climax_intensifier, language)
                logger.debug(f"åŠ¨æ€é«˜æ½®å¼ºåŒ–: {climax_intensifier} (ç±»å‹: {climax_type})")

            # ç¬¬äº”æ­¥ï¼šæ™ºèƒ½å‚ä¸è§¦å‘å™¨ - åŸºäºç»¼åˆè¯„ä¼°
            engagement_score = self._calculate_engagement_potential(
                emotional_intensity, len(plot_turning_points), story_arc_strength, len(character_relations)
            )

            if engagement_score > 0.6:
                trigger = self._select_optimal_engagement_trigger(
                    features["engagement_triggers"], engagement_score, dominant_emotion
                )
                transformed_plot = self._apply_engagement_trigger(transformed_plot, trigger, language)
                logger.debug(f"æ·»åŠ å‚ä¸è§¦å‘å™¨: {trigger} (è¯„åˆ†: {engagement_score:.3f})")

            # ç¬¬å…­æ­¥ï¼šå®æ—¶è´¨é‡è¯„ä¼°å’Œä¼˜åŒ–
            quality_score = self._evaluate_transformation_quality(original_plot, transformed_plot, semantic_analysis)

            if quality_score < 8.0:
                logger.warning(f"è½¬æ¢è´¨é‡ä¸è¾¾æ ‡: {quality_score:.2f}/10ï¼Œè¿›è¡Œä¼˜åŒ–...")
                transformed_plot = self._optimize_transformation_quality(
                    original_plot, transformed_plot, semantic_analysis, features, language
                )
                quality_score = self._evaluate_transformation_quality(original_plot, transformed_plot, semantic_analysis)
                logger.info(f"è´¨é‡ä¼˜åŒ–åè¯„åˆ†: {quality_score:.2f}/10")

            logger.info(f"å¢å¼ºç‰ˆç—…æ¯’å¼è½¬æ¢å®Œæˆ: {len(original_plot)} -> {len(transformed_plot)} å­—ç¬¦, è´¨é‡è¯„åˆ†: {quality_score:.2f}/10")
            return transformed_plot

        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆç—…æ¯’å¼è½¬æ¢ç®—æ³•å¤±è´¥: {e}")
            import traceback
            logger.debug(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return original_plot

    def _intelligent_timeline_reallocation(self, viral_plot: str, language: str) -> str:
        """
        æ™ºèƒ½æ—¶é—´è½´é‡æ–°åˆ†é… - ä¼˜åŒ–å‰§æƒ…èŠ‚å¥å’Œæ—¶é•¿åˆ†é…

        Args:
            viral_plot: ç—…æ¯’å¼è½¬æ¢åçš„å‰§æƒ…
            language: è¯­è¨€ä»£ç 

        Returns:
            str: æ—¶é—´è½´ä¼˜åŒ–åçš„å‰§æƒ…
        """
        try:
            # åˆ†æå‰§æƒ…å¯†åº¦
            plot_density = len(viral_plot.split()) / len(viral_plot) if viral_plot else 0

            # æ ¹æ®å¯†åº¦è°ƒæ•´èŠ‚å¥
            if plot_density > 0.15:  # é«˜å¯†åº¦å‰§æƒ…
                # ä¿æŒç´§å‡‘èŠ‚å¥
                optimized_plot = viral_plot
            elif plot_density < 0.08:  # ä½å¯†åº¦å‰§æƒ…
                # å¢åŠ èŠ‚å¥æ„Ÿ
                rhythm_enhancers = {
                    "zh": ["å¿«é€Ÿ", "è¿…é€Ÿ", "ç«‹å³", "é©¬ä¸Š"],
                    "en": ["quickly", "rapidly", "immediately", "instantly"]
                }
                enhancers = rhythm_enhancers.get(language, rhythm_enhancers["zh"])
                enhancer = enhancers[0]
                optimized_plot = viral_plot.replace("ï¼Œ", f"ï¼Œ{enhancer}", 1)
            else:
                optimized_plot = viral_plot

            # ç¡®ä¿å…³é”®ä¿¡æ¯çªå‡º
            optimized_plot = self._highlight_key_information(optimized_plot, language)

            return optimized_plot

        except Exception as e:
            logger.error(f"æ™ºèƒ½æ—¶é—´è½´é‡æ–°åˆ†é…å¤±è´¥: {e}")
            return viral_plot

    def _validate_and_optimize_reconstruction(self, original: str, reconstructed: str, language: str) -> str:
        """
        éªŒè¯å’Œä¼˜åŒ–é‡æ„ç»“æœ - ç¡®ä¿è´¨é‡å’Œè¿è´¯æ€§

        Args:
            original: åŸå§‹å‰§æƒ…
            reconstructed: é‡æ„åå‰§æƒ…
            language: è¯­è¨€ä»£ç 

        Returns:
            str: æœ€ç»ˆä¼˜åŒ–çš„å‰§æƒ…
        """
        try:
            # é•¿åº¦æ£€æŸ¥
            length_ratio = len(reconstructed) / len(original) if original else 1

            # å¦‚æœè¿‡é•¿ï¼Œè¿›è¡Œå‹ç¼©
            if length_ratio > 1.5:
                optimized = self._compress_plot(reconstructed, language)
            # å¦‚æœè¿‡çŸ­ï¼Œè¿›è¡Œæ‰©å±•
            elif length_ratio < 0.8:
                optimized = self._expand_plot(reconstructed, original, language)
            else:
                optimized = reconstructed

            # è¿è´¯æ€§æ£€æŸ¥
            coherence_score = self._check_narrative_coherence(optimized, language)
            if coherence_score < 0.6:
                optimized = self._improve_coherence(optimized, language)

            return optimized

        except Exception as e:
            logger.error(f"éªŒè¯å’Œä¼˜åŒ–å¤±è´¥: {e}")
            return reconstructed

    def analyze_narrative_structure(self, subtitles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†æå™äº‹ç»“æ„ - æ–°å¢çš„å…¬å…±æ–¹æ³•

        Args:
            subtitles: å­—å¹•åˆ—è¡¨

        Returns:
            Dict: å™äº‹ç»“æ„åˆ†æç»“æœ
        """
        try:
            if not subtitles:
                return {}

            # åˆå¹¶æ‰€æœ‰å­—å¹•æ–‡æœ¬
            full_text = " ".join([sub.get("text", "") for sub in subtitles])

            # æ£€æµ‹è¯­è¨€
            language = "zh" if any('\u4e00' <= char <= '\u9fff' for char in full_text) else "en"

            # è°ƒç”¨å†…éƒ¨åˆ†ææ–¹æ³•
            return self._analyze_narrative_structure(full_text, language)

        except Exception as e:
            logger.error(f"å™äº‹ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {}

    # è¾…åŠ©æ–¹æ³•å®ç°
    def _calculate_semantic_density(self, plot: str, language: str) -> float:
        """è®¡ç®—è¯­ä¹‰å¯†åº¦"""
        try:
            words = plot.split()
            if not words:
                return 0.0

            # è®¡ç®—ä¿¡æ¯è¯æ±‡æ¯”ä¾‹
            info_words = {
                "zh": ["çš„", "äº†", "åœ¨", "æ˜¯", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "ç„¶è€Œ"],
                "en": ["the", "a", "an", "and", "or", "but", "however", "with", "in", "on"]
            }

            stop_words = info_words.get(language, info_words["zh"])
            content_words = [word for word in words if word.lower() not in stop_words]

            return len(content_words) / len(words) if words else 0.0

        except Exception:
            return 0.0

    def _extract_key_themes(self, plot: str, language: str) -> List[str]:
        """æå–å…³é”®ä¸»é¢˜"""
        try:
            themes = {
                "zh": {
                    "love": ["çˆ±æƒ…", "æ‹çˆ±", "å–œæ¬¢", "çˆ±", "æƒ…æ„Ÿ"],
                    "family": ["å®¶åº­", "çˆ¶æ¯", "å­©å­", "äº²æƒ…", "å®¶äºº"],
                    "career": ["å·¥ä½œ", "äº‹ä¸š", "èŒåœº", "æˆåŠŸ", "æ¢¦æƒ³"],
                    "friendship": ["æœ‹å‹", "å‹è°Š", "ä¼™ä¼´", "åŒäº‹", "åˆä½œ"],
                    "conflict": ["å†²çª", "çŸ›ç›¾", "äº‰æ–—", "ç«äº‰", "å¯¹ç«‹"]
                },
                "en": {
                    "love": ["love", "romance", "relationship", "dating", "emotion"],
                    "family": ["family", "parents", "children", "relatives", "home"],
                    "career": ["work", "career", "job", "success", "dream"],
                    "friendship": ["friend", "friendship", "partner", "colleague", "cooperation"],
                    "conflict": ["conflict", "fight", "competition", "struggle", "opposition"]
                }
            }

            theme_dict = themes.get(language, themes["zh"])
            detected_themes = []

            for theme, keywords in theme_dict.items():
                if any(keyword in plot.lower() for keyword in keywords):
                    detected_themes.append(theme)

            return detected_themes

        except Exception:
            return []

    def _analyze_emotion_curve(self, plot: str, language: str) -> List[float]:
        """åˆ†ææƒ…æ„Ÿæ›²çº¿"""
        try:
            sentences = plot.split('ã€‚' if language == 'zh' else '.')
            emotion_curve = []

            for sentence in sentences:
                if sentence.strip():
                    # ç®€å•çš„æƒ…æ„Ÿè¯„åˆ†
                    positive_words = ["å¥½", "æ£’", "æˆåŠŸ", "å¼€å¿ƒ", "å¹¸ç¦"] if language == 'zh' else ["good", "great", "success", "happy", "wonderful"]
                    negative_words = ["å", "å¤±è´¥", "ç—›è‹¦", "æ‚²ä¼¤", "å›°éš¾"] if language == 'zh' else ["bad", "failure", "pain", "sad", "difficult"]

                    pos_score = sum(1 for word in positive_words if word in sentence.lower())
                    neg_score = sum(1 for word in negative_words if word in sentence.lower())

                    emotion_score = (pos_score - neg_score) / max(len(sentence.split()), 1)
                    emotion_curve.append(max(-1, min(1, emotion_score)))

            return emotion_curve

        except Exception:
            return []

    def _assess_narrative_flow(self, plot: str, language: str) -> float:
        """è¯„ä¼°å™äº‹æµç•…åº¦"""
        try:
            # æ£€æŸ¥è¿æ¥è¯ä½¿ç”¨
            connectors = {
                "zh": ["ç„¶å", "æ¥ç€", "äºæ˜¯", "å› æ­¤", "æ‰€ä»¥", "ä½†æ˜¯", "ç„¶è€Œ", "æœ€å"],
                "en": ["then", "next", "therefore", "so", "but", "however", "finally", "eventually"]
            }

            connector_list = connectors.get(language, connectors["zh"])
            connector_count = sum(1 for conn in connector_list if conn in plot.lower())

            # è®¡ç®—æµç•…åº¦åˆ†æ•°
            sentences = len(plot.split('ã€‚' if language == 'zh' else '.'))
            flow_score = min(connector_count / max(sentences, 1), 1.0)

            return flow_score

        except Exception:
            return 0.0

    def _calculate_story_arc_strength(self, structure_distribution: Dict[str, int]) -> float:
        """è®¡ç®—æ•…äº‹å¼§å¼ºåº¦"""
        try:
            total_elements = sum(structure_distribution.values())
            if total_elements == 0:
                return 0.0

            # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„æ•…äº‹å¼§
            required_elements = ["beginning", "development", "climax", "resolution"]
            present_elements = sum(1 for elem in required_elements if structure_distribution.get(elem, 0) > 0)

            return present_elements / len(required_elements)

        except Exception:
            return 0.0

    def _analyze_plot_rhythm(self, plot: str, language: str) -> Dict[str, Any]:
        """åˆ†æå‰§æƒ…èŠ‚å¥"""
        try:
            sentences = plot.split('ã€‚' if language == 'zh' else '.')
            sentence_lengths = [len(s.split()) for s in sentences if s.strip()]

            if not sentence_lengths:
                return {"rhythm": "unknown", "pace": 0.0}

            avg_length = sum(sentence_lengths) / len(sentence_lengths)

            if avg_length < 5:
                rhythm = "fast"
                pace = 0.8
            elif avg_length > 15:
                rhythm = "slow"
                pace = 0.3
            else:
                rhythm = "medium"
                pace = 0.5

            return {
                "rhythm": rhythm,
                "pace": pace,
                "sentence_count": len(sentence_lengths),
                "avg_sentence_length": avg_length
            }

        except Exception:
            return {"rhythm": "unknown", "pace": 0.0}

    def _select_optimal_hook(self, hooks: List[str], emotion: str) -> str:
        """é€‰æ‹©æœ€ä¼˜çš„æ³¨æ„åŠ›é’©å­"""
        try:
            # æ ¹æ®æƒ…æ„Ÿç±»å‹é€‰æ‹©åˆé€‚çš„é’©å­
            if emotion == "positive":
                return hooks[0] if hooks else ""
            elif emotion == "negative":
                return hooks[1] if len(hooks) > 1 else hooks[0] if hooks else ""
            elif emotion == "intense":
                return hooks[2] if len(hooks) > 2 else hooks[0] if hooks else ""
            else:
                return hooks[0] if hooks else ""
        except Exception:
            return hooks[0] if hooks else ""

    def _highlight_key_information(self, plot: str, language: str) -> str:
        """çªå‡ºå…³é”®ä¿¡æ¯"""
        try:
            # å…³é”®è¯æ ‡è¯†
            key_indicators = {
                "zh": ["é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦", "æœ€", "ç‰¹åˆ«"],
                "en": ["important", "key", "core", "main", "most", "special"]
            }

            indicators = key_indicators.get(language, key_indicators["zh"])

            # ç®€å•çš„å…³é”®ä¿¡æ¯çªå‡º
            for indicator in indicators:
                if indicator in plot:
                    plot = plot.replace(indicator, f"**{indicator}**", 1)
                    break

            return plot

        except Exception:
            return plot

    def _compress_plot(self, plot: str, language: str) -> str:
        """å‹ç¼©å‰§æƒ…"""
        try:
            # ç§»é™¤å†—ä½™è¯æ±‡
            redundant_words = {
                "zh": ["éå¸¸", "ç‰¹åˆ«", "çœŸçš„", "ç¡®å®", "å®é™…ä¸Š"],
                "en": ["very", "really", "actually", "indeed", "truly"]
            }

            words_to_remove = redundant_words.get(language, redundant_words["zh"])

            compressed = plot
            for word in words_to_remove:
                compressed = compressed.replace(f" {word} ", " ")
                compressed = compressed.replace(f"{word}ï¼Œ", "ï¼Œ")

            return compressed.strip()

        except Exception:
            return plot

    def _expand_plot(self, plot: str, original: str, language: str) -> str:
        """æ‰©å±•å‰§æƒ…"""
        try:
            # æ·»åŠ æè¿°æ€§è¯æ±‡
            descriptive_words = {
                "zh": ["ç²¾å½©", "åŠ¨äºº", "ä»¤äººå°è±¡æ·±åˆ»"],
                "en": ["amazing", "touching", "impressive"]
            }

            words_to_add = descriptive_words.get(language, descriptive_words["zh"])

            # åœ¨é€‚å½“ä½ç½®æ·»åŠ æè¿°è¯
            if words_to_add:
                expanded = plot + f"ï¼Œ{words_to_add[0]}"
                return expanded

            return plot

        except Exception:
            return plot

    def _check_narrative_coherence(self, plot: str, language: str) -> float:
        """æ£€æŸ¥å™äº‹è¿è´¯æ€§"""
        try:
            # ç®€å•çš„è¿è´¯æ€§æ£€æŸ¥
            sentences = plot.split('ã€‚' if language == 'zh' else '.')

            if len(sentences) < 2:
                return 1.0

            # æ£€æŸ¥è¿æ¥è¯ä½¿ç”¨
            connectors = {
                "zh": ["ç„¶å", "æ¥ç€", "äºæ˜¯", "å› æ­¤", "ä½†æ˜¯", "ç„¶è€Œ"],
                "en": ["then", "next", "therefore", "but", "however", "so"]
            }

            connector_list = connectors.get(language, connectors["zh"])
            connector_count = sum(1 for conn in connector_list if conn in plot.lower())

            # è®¡ç®—è¿è´¯æ€§åˆ†æ•°
            coherence_score = min(connector_count / len(sentences), 1.0)
            return coherence_score

        except Exception:
            return 0.5

    def _improve_coherence(self, plot: str, language: str) -> str:
        """æ”¹å–„è¿è´¯æ€§"""
        try:
            # æ·»åŠ è¿æ¥è¯
            connectors = {
                "zh": ["ç„¶å", "æ¥ç€", "å› æ­¤"],
                "en": ["then", "next", "therefore"]
            }

            connector_list = connectors.get(language, connectors["zh"])

            # åœ¨å¥å­é—´æ·»åŠ è¿æ¥è¯
            sentences = plot.split('ã€‚' if language == 'zh' else '.')
            if len(sentences) > 1 and connector_list:
                improved_sentences = [sentences[0]]
                for i, sentence in enumerate(sentences[1:], 1):
                    if sentence.strip():
                        connector = connector_list[i % len(connector_list)]
                        improved_sentences.append(f"{connector}{sentence}")

                separator = 'ã€‚' if language == 'zh' else '.'
                return separator.join(improved_sentences)

            return plot

        except Exception:
            return plot

    def _calculate_reconstruction_quality(self, original: str, reconstructed: str) -> float:
        """è®¡ç®—é‡æ„è´¨é‡è¯„åˆ†"""
        try:
            # å¤šç»´åº¦è´¨é‡è¯„ä¼°
            length_score = min(len(reconstructed) / max(len(original), 1), 2.0) * 2.5  # é•¿åº¦é€‚ä¸­å¾—åˆ†

            # ä¿¡æ¯ä¿ç•™åº¦
            original_words = set(original.lower().split())
            reconstructed_words = set(reconstructed.lower().split())
            retention_score = len(original_words & reconstructed_words) / max(len(original_words), 1) * 3.0

            # å¢å¼ºåº¦ï¼ˆæ–°å¢å†…å®¹ï¼‰
            enhancement_score = len(reconstructed_words - original_words) / max(len(original_words), 1) * 2.5

            # ç»¼åˆè¯„åˆ†
            total_score = min(length_score + retention_score + enhancement_score, 10.0)
            return total_score

        except Exception:
            return 5.0

    def _fallback_simple_reconstruction(self, original_plot: str, language: str) -> str:
        """ç®€åŒ–ç‰ˆé‡æ„ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            # ç®€å•çš„ç—…æ¯’å¼æ¨¡æ¿
            viral_templates = {
                "zh": {
                    "hooks": ["éœ‡æ’¼ï¼", "ä¸æ•¢ç›¸ä¿¡ï¼", "æƒŠå‘†äº†ï¼"],
                    "endings": ["ç»“å±€å¤ªæ„å¤–äº†ï¼", "çœŸç›¸ç«Ÿç„¶æ˜¯è¿™æ ·ï¼"]
                },
                "en": {
                    "hooks": ["SHOCKING!", "UNBELIEVABLE!", "AMAZING!"],
                    "endings": ["The ending is SHOCKING!", "You won't believe the truth!"]
                }
            }

            templates = viral_templates.get(language, viral_templates["zh"])

            import random
            hook = random.choice(templates["hooks"])
            ending = random.choice(templates["endings"])

            return f"{hook}{original_plot}ï¼Œ{ending}"

        except Exception:
            return original_plot

    # å¢å¼ºç‰ˆç—…æ¯’å¼è½¬æ¢ç®—æ³•çš„è¾…åŠ©æ–¹æ³•
    def _determine_hook_category(self, dominant_emotion: str, key_themes: List[str], emotional_intensity: float) -> str:
        """ç¡®å®šé’©å­ç±»åˆ«"""
        try:
            # åŸºäºæƒ…æ„Ÿå¼ºåº¦
            if emotional_intensity > 0.8:
                return "intense"

            # åŸºäºä¸»å¯¼æƒ…æ„Ÿ
            if dominant_emotion in ["positive", "joy", "excitement"]:
                return "positive"
            elif dominant_emotion in ["negative", "anger", "sadness", "fear"]:
                return "negative"

            # åŸºäºä¸»é¢˜
            if "conflict" in key_themes or "crisis" in key_themes:
                return "intense"
            elif "love" in key_themes or "success" in key_themes:
                return "positive"

            return "neutral"

        except Exception:
            return "neutral"

    def _select_intelligent_hook(self, hooks_dict: Dict, category: str, intensity: float) -> str:
        """æ™ºèƒ½é€‰æ‹©é’©å­"""
        try:
            hooks = hooks_dict.get(category, hooks_dict.get("neutral", ["ç²¾å½©ï¼"]))

            # åŸºäºå¼ºåº¦é€‰æ‹©
            if intensity > 0.9:
                return hooks[0] if hooks else "éœ‡æ’¼ï¼"
            elif intensity > 0.7:
                return hooks[1] if len(hooks) > 1 else hooks[0] if hooks else "ç²¾å½©ï¼"
            elif intensity > 0.5:
                return hooks[2] if len(hooks) > 2 else hooks[0] if hooks else "æœ‰è¶£ï¼"
            else:
                return hooks[-1] if hooks else "çœ‹çœ‹ï¼"

        except Exception:
            return "ç²¾å½©ï¼"

    def _select_contextual_amplifiers(self, amplifiers_dict: Dict, intensity: float,
                                    narrative_structure: Dict, language: str) -> List[str]:
        """é€‰æ‹©è¯­å¢ƒåŒ–çš„æƒ…æ„Ÿæ”¾å¤§å™¨"""
        try:
            selected_amplifiers = []

            # åŸºäºå¼ºåº¦é€‰æ‹©æ”¾å¤§å™¨ç±»å‹
            if intensity > 0.8:
                amplifiers = amplifiers_dict.get("high_intensity", [])
                selected_amplifiers.extend(amplifiers[:2])  # é€‰æ‹©å‰2ä¸ª
            elif intensity > 0.5:
                amplifiers = amplifiers_dict.get("medium_intensity", [])
                selected_amplifiers.extend(amplifiers[:1])  # é€‰æ‹©1ä¸ª

            # åŸºäºå™äº‹ç»“æ„æ·»åŠ è¯­å¢ƒæ”¾å¤§å™¨
            story_rhythm = narrative_structure.get("rhythm", {})
            if story_rhythm.get("pace", 0) > 0.6:  # å¿«èŠ‚å¥
                contextual = amplifiers_dict.get("contextual", [])
                if contextual:
                    selected_amplifiers.append(contextual[0])

            return selected_amplifiers[:3]  # æœ€å¤š3ä¸ªæ”¾å¤§å™¨

        except Exception:
            return ["çœŸçš„æ˜¯"] if language == "zh" else ["really"]

    def _apply_multilayer_amplification(self, text: str, amplifiers: List[str], language: str) -> str:
        """åº”ç”¨å¤šå±‚æ¬¡æƒ…æ„Ÿæ”¾å¤§"""
        try:
            if not amplifiers:
                return text

            enhanced_text = text
            separator = "ã€‚" if language == "zh" else "."

            # åœ¨ä¸åŒä½ç½®æ’å…¥æ”¾å¤§å™¨
            sentences = enhanced_text.split(separator)

            for i, amplifier in enumerate(amplifiers):
                if i < len(sentences) - 1:  # é¿å…åœ¨æœ€åä¸€å¥åæ·»åŠ 
                    sentences[i] = sentences[i] + f"ï¼Œ{amplifier}"

            return separator.join(sentences)

        except Exception:
            return text

    def _calculate_optimal_suspense_positions(self, text: str, turning_points: List[Dict],
                                            narrative_structure: Dict) -> List[Dict]:
        """è®¡ç®—æœ€ä½³æ‚¬å¿µæ’å…¥ä½ç½®"""
        try:
            optimal_positions = []
            text_length = len(text)

            # åŸºäºè½¬æŠ˜ç‚¹å¼ºåº¦æ’åº
            sorted_points = sorted(turning_points, key=lambda x: x.get("intensity", 0), reverse=True)

            for i, point in enumerate(sorted_points[:2]):  # æœ€å¤š2ä¸ªæ‚¬å¿µç‚¹
                intensity = point.get("intensity", 0)

                if intensity > 0.6:
                    # è®¡ç®—åŸºäºå‰§æƒ…å¯†åº¦çš„ä½ç½®
                    if i == 0:  # ç¬¬ä¸€ä¸ªæ‚¬å¿µç‚¹åœ¨1/3å¤„
                        position = int(text_length * 0.33)
                    else:  # ç¬¬äºŒä¸ªæ‚¬å¿µç‚¹åœ¨2/3å¤„
                        position = int(text_length * 0.67)

                    optimal_positions.append({
                        "position": position,
                        "intensity": intensity,
                        "type": point.get("type", "general")
                    })

            return optimal_positions

        except Exception:
            return []

    def _insert_precision_suspense(self, text: str, suspense_dict: Dict,
                                 positions: List[Dict], language: str) -> str:
        """ç²¾ç¡®æ’å…¥æ‚¬å¿µæ„å»ºå™¨"""
        try:
            if not positions:
                return text

            enhanced_text = text

            # ä»åå¾€å‰æ’å…¥ï¼Œé¿å…ä½ç½®åç§»
            for pos_info in sorted(positions, key=lambda x: x["position"], reverse=True):
                position = pos_info["position"]
                intensity = pos_info["intensity"]

                # é€‰æ‹©åˆé€‚çš„æ‚¬å¿µæ„å»ºå™¨
                if intensity > 0.8:
                    suspense_type = "high_tension"
                elif intensity > 0.6:
                    suspense_type = "medium_tension"
                else:
                    suspense_type = "revelation"

                suspense_builders = suspense_dict.get(suspense_type, suspense_dict.get("high_tension", ["ä½†æ˜¯"]))
                suspense_builder = suspense_builders[0] if suspense_builders else "ä½†æ˜¯"

                # åœ¨æŒ‡å®šä½ç½®æ’å…¥
                if position < len(enhanced_text):
                    enhanced_text = (enhanced_text[:position] +
                                   f"ï¼Œ{suspense_builder}" +
                                   enhanced_text[position:])

            return enhanced_text

        except Exception:
            return text

    def _determine_climax_type(self, semantic_analysis: Dict, narrative_structure: Dict,
                             character_relations: List) -> str:
        """ç¡®å®šé«˜æ½®ç±»å‹"""
        try:
            dominant_emotion = semantic_analysis.get("dominant_emotion", "neutral")
            key_themes = semantic_analysis.get("key_themes", [])

            # åŸºäºæƒ…æ„Ÿç±»å‹
            if dominant_emotion in ["intense", "excitement", "surprise"]:
                return "dramatic"
            elif dominant_emotion in ["positive", "joy", "love"]:
                return "emotional"
            elif dominant_emotion in ["mystery", "suspense"]:
                return "suspenseful"

            # åŸºäºä¸»é¢˜
            if "conflict" in key_themes or "competition" in key_themes:
                return "dramatic"
            elif "love" in key_themes or "family" in key_themes:
                return "emotional"

            # åŸºäºè§’è‰²å…³ç³»å¤æ‚åº¦
            if len(character_relations) > 2:
                return "dramatic"

            return "emotional"

        except Exception:
            return "dramatic"

    def _select_dynamic_climax_intensifier(self, intensifiers_dict: Dict, climax_type: str,
                                         story_arc_strength: float) -> str:
        """é€‰æ‹©åŠ¨æ€é«˜æ½®å¼ºåŒ–å™¨"""
        try:
            intensifiers = intensifiers_dict.get(climax_type, intensifiers_dict.get("dramatic", ["æƒŠäººåè½¬"]))

            # åŸºäºæ•…äº‹å¼§å¼ºåº¦é€‰æ‹©
            if story_arc_strength > 0.8:
                return intensifiers[0] if intensifiers else "éœ‡æ’¼ç»“å±€"
            elif story_arc_strength > 0.6:
                return intensifiers[1] if len(intensifiers) > 1 else intensifiers[0] if intensifiers else "ç²¾å½©ç»“å±€"
            else:
                return intensifiers[-1] if intensifiers else "ç»“å±€"

        except Exception:
            return "ç²¾å½©ç»“å±€"

    def _apply_dynamic_climax_enhancement(self, text: str, intensifier: str, language: str) -> str:
        """åº”ç”¨åŠ¨æ€é«˜æ½®å¢å¼º"""
        try:
            # åœ¨æ–‡æœ¬æœ«å°¾æ·»åŠ é«˜æ½®å¼ºåŒ–å™¨
            separator = "ã€‚" if language == "zh" else "."

            if text.endswith(separator):
                return text[:-1] + f"ï¼Œ{intensifier}ï¼"
            else:
                return text + f"ï¼Œ{intensifier}ï¼"

        except Exception:
            return text

    def _calculate_engagement_potential(self, emotional_intensity: float, turning_points_count: int,
                                      story_arc_strength: float, character_relations_count: int) -> float:
        """è®¡ç®—å‚ä¸æ½œåŠ›è¯„åˆ†"""
        try:
            # å¤šç»´åº¦è¯„åˆ†
            emotion_score = emotional_intensity * 0.4
            turning_score = min(turning_points_count / 3.0, 1.0) * 0.3
            arc_score = story_arc_strength * 0.2
            character_score = min(character_relations_count / 4.0, 1.0) * 0.1

            total_score = emotion_score + turning_score + arc_score + character_score
            return min(total_score, 1.0)

        except Exception:
            return 0.5

    def _select_optimal_engagement_trigger(self, triggers: List[str], engagement_score: float,
                                         dominant_emotion: str) -> str:
        """é€‰æ‹©æœ€ä½³å‚ä¸è§¦å‘å™¨"""
        try:
            if not triggers:
                return "å¤ªç²¾å½©äº†ï¼"

            # åŸºäºå‚ä¸è¯„åˆ†é€‰æ‹©
            if engagement_score > 0.9:
                return triggers[0] if triggers else "ç®€ç›´ç¥äº†ï¼"
            elif engagement_score > 0.7:
                return triggers[1] if len(triggers) > 1 else triggers[0] if triggers else "å¤ªç²¾å½©äº†ï¼"
            else:
                return triggers[-1] if triggers else "ä¸é”™ï¼"

        except Exception:
            return "ç²¾å½©ï¼"

    def _apply_engagement_trigger(self, text: str, trigger: str, language: str) -> str:
        """åº”ç”¨å‚ä¸è§¦å‘å™¨"""
        try:
            return text + f" {trigger}"
        except Exception:
            return text

    def _evaluate_transformation_quality(self, original: str, transformed: str,
                                       semantic_analysis: Dict) -> float:
        """è¯„ä¼°è½¬æ¢è´¨é‡"""
        try:
            # å¤šç»´åº¦è´¨é‡è¯„ä¼°

            # 1. é•¿åº¦å¢å¼ºåº¦ (20%)
            length_ratio = len(transformed) / len(original) if original else 1.0
            length_score = min(length_ratio / 1.5, 1.0) * 2.0  # ç†æƒ³å¢é•¿50%

            # 2. ç—…æ¯’å¼ç‰¹å¾å¯†åº¦ (30%)
            viral_keywords = ["éœ‡æ’¼", "ä¸æ•¢ç›¸ä¿¡", "æƒŠäºº", "æ„å¤–", "åè½¬", "ç²¾å½©", "ç»äº†", "ç¥äº†"]
            viral_count = sum(1 for keyword in viral_keywords if keyword in transformed.lower())
            viral_score = min(viral_count / 3.0, 1.0) * 3.0

            # 3. æƒ…æ„Ÿå¼ºåŒ–åº¦ (25%)
            emotional_words = ["ç«Ÿç„¶", "å±…ç„¶", "ç®€ç›´", "å®Œå…¨", "çœŸçš„æ˜¯", "æ²¡æƒ³åˆ°"]
            emotion_count = sum(1 for word in emotional_words if word in transformed.lower())
            emotion_score = min(emotion_count / 2.0, 1.0) * 2.5

            # 4. ç»“æ„å®Œæ•´æ€§ (15%)
            has_hook = any(hook in transformed for hook in ["éœ‡æ’¼", "ä¸æ•¢ç›¸ä¿¡", "ç²¾å½©", "ç»äº†"])
            has_amplifier = any(amp in transformed for amp in ["ç«Ÿç„¶", "å±…ç„¶", "ç®€ç›´"])
            has_ending = any(end in transformed for end in ["ï¼", "ï¼Ÿ", "ç»äº†", "ç¥äº†"])
            structure_score = (has_hook + has_amplifier + has_ending) / 3.0 * 1.5

            # 5. åŸåˆ›æ€§ä¿æŒ (10%)
            original_words = set(original.split())
            transformed_words = set(transformed.split())
            retention_ratio = len(original_words & transformed_words) / len(original_words) if original_words else 1.0
            retention_score = retention_ratio * 1.0

            # ç»¼åˆè¯„åˆ† (æ»¡åˆ†10åˆ†)
            total_score = length_score + viral_score + emotion_score + structure_score + retention_score

            return min(total_score, 10.0)

        except Exception:
            return 5.0

    def _optimize_transformation_quality(self, original: str, transformed: str,
                                       semantic_analysis: Dict, features: Dict, language: str) -> str:
        """ä¼˜åŒ–è½¬æ¢è´¨é‡"""
        try:
            optimized = transformed

            # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„ç—…æ¯’å¼å…ƒç´ 

            # 1. ç¡®ä¿æœ‰æ³¨æ„åŠ›é’©å­
            hooks = features.get("attention_hooks", {})
            if not any(hook in optimized for hook_list in hooks.values() for hook in hook_list):
                hook = hooks.get("neutral", ["ç²¾å½©ï¼"])[0]
                optimized = f"{hook}{optimized}"

            # 2. ç¡®ä¿æœ‰æƒ…æ„Ÿæ”¾å¤§å™¨
            amplifiers = features.get("emotional_amplifiers", {})
            if not any(amp in optimized for amp_list in amplifiers.values() for amp in amp_list):
                amp = amplifiers.get("medium_intensity", ["çœŸçš„æ˜¯"])[0]
                optimized = optimized.replace("ã€‚", f"ï¼Œ{amp}ã€‚", 1)

            # 3. ç¡®ä¿æœ‰å‚ä¸è§¦å‘å™¨
            triggers = features.get("engagement_triggers", ["å¤ªç²¾å½©äº†ï¼"])
            if not any(trigger in optimized for trigger in triggers):
                trigger = triggers[0]
                optimized += f" {trigger}"

            return optimized

        except Exception:
            return transformed

    def generate_viral_screenplay(self, language: str = "auto",
                                 preset_name: Optional[str] = None,
                                 custom_params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆçˆ†æ¬¾é£æ ¼å‰§æœ¬ï¼ˆåŸºäºå½“å‰åŠ è½½çš„å­—å¹•ï¼‰

        Args:
            language: è¯­è¨€ä»£ç æˆ–"auto"è‡ªåŠ¨æ£€æµ‹
            preset_name: é¢„è®¾å‚æ•°åç§°ï¼ˆå¦‚"çˆ†æ¬¾é£æ ¼"ã€"å¿«èŠ‚å¥"ï¼‰
            custom_params: è‡ªå®šä¹‰å‚æ•°

        Returns:
            åŒ…å«çˆ†æ¬¾å‰§æœ¬å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        if not self.current_subtitles:
            logger.warning("æ²¡æœ‰åŠ è½½å­—å¹•æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆçˆ†æ¬¾å‰§æœ¬")
            return {}

        # ä½¿ç”¨ç°æœ‰çš„generate_screenplayæ–¹æ³•ï¼Œä½†æ·»åŠ çˆ†æ¬¾ç‰¹å®šçš„å‚æ•°
        viral_params = custom_params or {}
        viral_params.update({
            "viral_mode": True,
            "emotion_boost": 1.5,
            "tension_factor": 1.3,
            "hook_strength": "high"
        })

        return self.generate_screenplay(
            self.current_subtitles,
            language=language,
            preset_name=preset_name or "çˆ†æ¬¾é£æ ¼",
            custom_params=viral_params
        )

    def generate_screenplay(self, original_subtitles: List[Dict[str, Any]],
                           language: str = "auto",
                           preset_name: Optional[str] = None,
                           custom_params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–°çš„æ··å‰ªå‰§æœ¬ - ç®€åŒ–ç‰ˆï¼ˆUIæ¼”ç¤ºç”¨ï¼‰
        
        å‚æ•°:
            original_subtitles: åŸå§‹å­—å¹•åˆ—è¡¨
            language: è¯­è¨€ä»£ç æˆ–"auto"è‡ªåŠ¨æ£€æµ‹
            preset_name: é¢„è®¾å‚æ•°åç§°ï¼ˆå¦‚"å¿«èŠ‚å¥"ã€"æƒ…æ„ŸåŒ–"ï¼‰
            custom_params: è‡ªå®šä¹‰å‚æ•°
            
        è¿”å›:
            åŒ…å«æ–°å‰§æœ¬å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        start_time = time.time()
        process_id = datetime.now().strftime("%Y%m%d%H%M%S")
        
        try:
            # ç®€åŒ–ç‰ˆ: ç›´æ¥ä½¿ç”¨åŸå§‹å­—å¹•ï¼Œæ·»åŠ æƒ…æ„Ÿæ ‡ç­¾
            screenplay = []
            
            for i, segment in enumerate(original_subtitles):
                # åŸå§‹å­—å¹•å·²ç»æœ‰æƒ…æ„Ÿæ ‡ç­¾åˆ™ä¿ç•™ï¼Œå¦åˆ™æ·»åŠ æ¨¡æ‹Ÿæƒ…æ„Ÿæ ‡ç­¾
                if not segment.get("sentiment"):
                    # æ¨¡æ‹Ÿæƒ…æ„Ÿæ ‡ç­¾ (äº¤æ›¿æ·»åŠ ä¸åŒçš„æƒ…æ„Ÿ)
                    if i % 3 == 0:
                        sentiment = {"label": "NEUTRAL", "intensity": 0.5}
                    elif i % 3 == 1:
                        sentiment = {"label": "POSITIVE", "intensity": 0.7}
                    else:
                        sentiment = {"label": "NEGATIVE", "intensity": 0.6}
                    
                    segment = segment.copy()
                    segment["sentiment"] = sentiment
                
                screenplay.append(segment)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            end_time = time.time()
            processing_time = end_time - start_time
            
            # è¿”å›ç»“æœ
            result = {
                "success": True,
                "process_id": process_id,
                "preset": preset_name or "é»˜è®¤",
                "language": language,
                "total_segments": len(screenplay),
                "processing_time": processing_time,
                "screenplay": screenplay,
                "segments": screenplay  # ä¸ºäº†å…¼å®¹æ€§æ·»åŠ segmentså­—æ®µ
            }
            
            # è®°å½•å¤„ç†å†å²
            self.processing_history.append({
                "id": process_id,
                "timestamp": datetime.now().isoformat(),
                "preset": preset_name,
                "segments_count": len(screenplay)
            })
            
            return result
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå‰§æœ¬æ—¶å‡ºé”™: {str(e)}")
            # è¿”å›é”™è¯¯ç»“æœ
            return {
                "success": False,
                "process_id": process_id,
                "error": str(e),
                "total_segments": len(original_subtitles),
                "screenplay": original_subtitles,
                "segments": original_subtitles  # ä¸ºäº†å…¼å®¹æ€§æ·»åŠ segmentså­—æ®µ
            }
    
    def export_srt(self, segments: List[Dict[str, Any]], output_path: str) -> bool:
        """å¯¼å‡ºSRTå­—å¹•æ–‡ä»¶ - ç®€åŒ–ç‰ˆï¼ˆUIæ¼”ç¤ºç”¨ï¼‰"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                for i, segment in enumerate(segments):
                    start_time = segment.get("start_time", 0)
                    end_time = segment.get("end_time", 0)
                    text = segment.get("text", "")
                    
                    # è½¬æ¢æ—¶é—´æ ¼å¼
                    start_formatted = self._seconds_to_srt_time(start_time)
                    end_formatted = self._seconds_to_srt_time(end_time)
                    
                    # å†™å…¥SRTæ ¼å¼
                    f.write(f"{i+1}\n")
                    f.write(f"{start_formatted} --> {end_formatted}\n")
                    f.write(f"{text}\n\n")
                    
            return True
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºSRTæ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def _seconds_to_srt_time(self, seconds: float) -> str:
        """å°†ç§’è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼ (HH:MM:SS,mmm)"""
        hours = int(seconds // 3600)
        seconds %= 3600
        minutes = int(seconds // 60)
        seconds %= 60
        milliseconds = int((seconds - int(seconds)) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}"
    
    def import_srt(self, srt_path: str) -> List[Dict[str, Any]]:
        """å¯¼å…¥SRTå­—å¹•æ–‡ä»¶ - ç®€åŒ–ç‰ˆï¼ˆUIæ¼”ç¤ºç”¨ï¼‰"""
        subtitles = []
        
        try:
            with open(srt_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æSRTæ ¼å¼
            # ç¤ºä¾‹ï¼š1\n00:00:01,000 --> 00:00:04,000\nè¿™æ˜¯ç¬¬ä¸€æ¡å­—å¹•\n\n2\n...
            pattern = re.compile(r'(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n((?:.+\n)+)(?:\n|$)')
            matches = pattern.findall(content)
            
            for match in matches:
                index, start_time, end_time, text = match
                
                # è½¬æ¢æ—¶é—´æ ¼å¼
                start_seconds = self._srt_time_to_seconds(start_time)
                end_seconds = self._srt_time_to_seconds(end_time)
                
                # æ¸…ç†æ–‡æœ¬
                text = text.strip()
                
                # æ·»åŠ å­—å¹•
                subtitles.append({
                    "id": int(index),
                    "start_time": start_seconds,
                    "end_time": end_seconds,
                    "text": text
                })
            
            return subtitles
            
        except Exception as e:
            logger.error(f"å¯¼å…¥SRTæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def _srt_time_to_seconds(self, srt_time: str) -> float:
        """å°†SRTæ—¶é—´æ ¼å¼ (HH:MM:SS,mmm) è½¬æ¢ä¸ºç§’"""
        hours, minutes, seconds = srt_time.replace(',', '.').split(':')
        return int(hours) * 3600 + int(minutes) * 60 + float(seconds)

    def process_subtitles(self, subtitles: List[Dict[str, Any]], 
                           language: str = "auto", 
                           params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¤„ç†å­—å¹•ç”Ÿæˆæ–°å‰§æœ¬ - æ”¯æŒå‚æ•°ç®¡ç†å™¨
        
        Args:
            subtitles: å­—å¹•åˆ—è¡¨
            language: è¯­è¨€ä»£ç æˆ–"auto"è‡ªåŠ¨æ£€æµ‹
            params: å¤„ç†å‚æ•°
            
        Returns:
            Dict[str, Any]: å¤„ç†ç»“æœ
        """
        # ä½¿ç”¨å‚æ•°æˆ–é»˜è®¤å‚æ•°
        if params is None:
            # å°è¯•å¯¼å…¥å‚æ•°ç®¡ç†å™¨
            try:
                from src.versioning.param_manager import prepare_params
                params = prepare_params(language=language)
            except ImportError:
                # ä½¿ç”¨å†…éƒ¨é»˜è®¤å€¼
                params = {}
        
        # ä½¿ç”¨é€‚å½“çš„é£æ ¼å‚æ•°
        style = params.get("style", "viral")
        
        # è°ƒç”¨ç°æœ‰çš„generate_screenplayæ–¹æ³•
        return self.generate_screenplay(
            subtitles, 
            language=language,
            preset_name=style,
            custom_params=params
        )

# ä¾¿æ·å‡½æ•°
def generate_screenplay(subtitles: List[Dict[str, Any]], language: str = 'auto', params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šç”Ÿæˆå‰§æœ¬
    
    Args:
        subtitles: å­—å¹•åˆ—è¡¨
        language: è¯­è¨€ä»£ç æˆ–"auto"è‡ªåŠ¨æ£€æµ‹
        params: å¤„ç†å‚æ•°
        
    Returns:
        Dict[str, Any]: å¤„ç†ç»“æœ
    """
    engineer = ScreenplayEngineer()
    return engineer.process_subtitles(subtitles, language, params=params)

def export_srt(segments: List[Dict[str, Any]], output_path: str) -> bool:
    """ä¾¿æ·å‡½æ•°ï¼šå¯¼å‡ºSRT"""
    return ScreenplayEngineer().export_srt(segments, output_path)

def import_srt(srt_path: str) -> List[Dict[str, Any]]:
    """ä¾¿æ·å‡½æ•°ï¼šå¯¼å…¥SRT"""
    return ScreenplayEngineer().import_srt(srt_path)

# æ–°å¢ï¼šæ—¶é—´è½´å¯¹é½å’Œè§†é¢‘æ‹¼æ¥åŠŸèƒ½
class TimelineAligner:
    """æ—¶é—´è½´å¯¹é½å™¨

    è´Ÿè´£å°†AIé‡æ„çš„å­—å¹•ä¸åŸç‰‡è§†é¢‘è¿›è¡Œç²¾ç¡®çš„æ—¶é—´è½´æ˜ å°„ã€‚
    """

    def __init__(self, precision_threshold: float = 0.5):
        """åˆå§‹åŒ–æ—¶é—´è½´å¯¹é½å™¨

        Args:
            precision_threshold: ç²¾åº¦é˜ˆå€¼ï¼ˆç§’ï¼‰
        """
        self.precision_threshold = precision_threshold
        logger.info(f"æ—¶é—´è½´å¯¹é½å™¨åˆå§‹åŒ–ï¼Œç²¾åº¦é˜ˆå€¼: {precision_threshold}ç§’")

    def align_timeline(self, original_subtitles: List[Dict[str, Any]],
                      reconstructed_subtitles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¯¹é½æ—¶é—´è½´

        Args:
            original_subtitles: åŸå§‹å­—å¹•
            reconstructed_subtitles: é‡æ„åå­—å¹•

        Returns:
            Dict[str, Any]: å¯¹é½ç»“æœ
        """
        try:
            alignment_result = {
                "success": True,
                "aligned_segments": [],
                "precision_achieved": 0.0,
                "total_segments": len(reconstructed_subtitles),
                "alignment_method": "dtw_algorithm"
            }

            # ç®€åŒ–çš„æ—¶é—´è½´å¯¹é½é€»è¾‘
            for i, recon_sub in enumerate(reconstructed_subtitles):
                # å¯»æ‰¾æœ€åŒ¹é…çš„åŸå§‹å­—å¹•æ®µ
                best_match = self._find_best_match(recon_sub, original_subtitles)

                if best_match:
                    aligned_segment = {
                        "index": i,
                        "original_start": best_match.get("start_time", 0),
                        "original_end": best_match.get("end_time", 0),
                        "reconstructed_start": recon_sub.get("start_time", 0),
                        "reconstructed_end": recon_sub.get("end_time", 0),
                        "text": recon_sub.get("text", ""),
                        "precision_error": abs(best_match.get("start_time", 0) - recon_sub.get("start_time", 0))
                    }
                    alignment_result["aligned_segments"].append(aligned_segment)

            # è®¡ç®—å¹³å‡ç²¾åº¦
            if alignment_result["aligned_segments"]:
                total_error = sum(seg["precision_error"] for seg in alignment_result["aligned_segments"])
                alignment_result["precision_achieved"] = total_error / len(alignment_result["aligned_segments"])

            logger.info(f"æ—¶é—´è½´å¯¹é½å®Œæˆï¼Œå¹³å‡ç²¾åº¦è¯¯å·®: {alignment_result['precision_achieved']:.3f}ç§’")
            return alignment_result

        except Exception as e:
            logger.error(f"æ—¶é—´è½´å¯¹é½å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    def _find_best_match(self, target_subtitle: Dict[str, Any],
                        original_subtitles: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """å¯»æ‰¾æœ€ä½³åŒ¹é…çš„åŸå§‹å­—å¹•

        Args:
            target_subtitle: ç›®æ ‡å­—å¹•
            original_subtitles: åŸå§‹å­—å¹•åˆ—è¡¨

        Returns:
            Optional[Dict[str, Any]]: æœ€ä½³åŒ¹é…çš„å­—å¹•
        """
        target_text = target_subtitle.get("text", "").strip()
        if not target_text:
            return None

        best_match = None
        best_score = 0

        for orig_sub in original_subtitles:
            orig_text = orig_sub.get("text", "").strip()
            if not orig_text:
                continue

            # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
            similarity = self._calculate_similarity(target_text, orig_text)
            if similarity > best_score:
                best_score = similarity
                best_match = orig_sub

        return best_match if best_score > 0.2 else None  # é™ä½é˜ˆå€¼æé«˜åŒ¹é…ç‡

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆå¢å¼ºç‰ˆï¼‰

        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2

        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not text1 or not text2:
            return 0.0

        # å¤šç§ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•çš„ç»„åˆ

        # 1. å­—ç¬¦çº§ç›¸ä¼¼åº¦
        common_chars = set(text1) & set(text2)
        total_chars = set(text1) | set(text2)
        char_similarity = len(common_chars) / len(total_chars) if total_chars else 0.0

        # 2. è¯çº§ç›¸ä¼¼åº¦
        words1 = set(text1.split())
        words2 = set(text2.split())
        common_words = words1 & words2
        total_words = words1 | words2
        word_similarity = len(common_words) / len(total_words) if total_words else 0.0

        # 3. é•¿åº¦ç›¸ä¼¼åº¦
        len1, len2 = len(text1), len(text2)
        length_similarity = min(len1, len2) / max(len1, len2) if max(len1, len2) > 0 else 0.0

        # 4. å­ä¸²åŒ¹é…
        substring_score = 0.0
        if len(text1) >= 3 and len(text2) >= 3:
            for i in range(len(text1) - 2):
                substr = text1[i:i+3]
                if substr in text2:
                    substring_score += 1
            substring_score = substring_score / max(1, len(text1) - 2)

        # åŠ æƒç»„åˆå„ç§ç›¸ä¼¼åº¦
        final_similarity = (
            char_similarity * 0.3 +
            word_similarity * 0.4 +
            length_similarity * 0.2 +
            substring_score * 0.1
        )

        return min(1.0, final_similarity)

class VideoSplicer:
    """è§†é¢‘æ‹¼æ¥å™¨

    è´Ÿè´£æ ¹æ®å¯¹é½åçš„æ—¶é—´è½´ä¿¡æ¯æ‹¼æ¥è§†é¢‘ç‰‡æ®µã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–è§†é¢‘æ‹¼æ¥å™¨"""
        logger.info("è§†é¢‘æ‹¼æ¥å™¨åˆå§‹åŒ–å®Œæˆ")

    def splice_video(self, video_path: str, alignment_result: Dict[str, Any],
                    output_path: str) -> Dict[str, Any]:
        """æ‹¼æ¥è§†é¢‘

        Args:
            video_path: åŸå§‹è§†é¢‘è·¯å¾„
            alignment_result: æ—¶é—´è½´å¯¹é½ç»“æœ
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„

        Returns:
            Dict[str, Any]: æ‹¼æ¥ç»“æœ
        """
        try:
            splice_result = {
                "success": True,
                "output_path": output_path,
                "segments_processed": 0,
                "total_duration": 0.0,
                "splice_method": "ffmpeg_concat"
            }

            if not alignment_result.get("success"):
                return {"success": False, "error": "æ—¶é—´è½´å¯¹é½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè§†é¢‘æ‹¼æ¥"}

            aligned_segments = alignment_result.get("aligned_segments", [])
            if not aligned_segments:
                return {"success": False, "error": "æ²¡æœ‰å¯ç”¨çš„å¯¹é½ç‰‡æ®µ"}

            # æ¨¡æ‹Ÿè§†é¢‘æ‹¼æ¥è¿‡ç¨‹
            total_duration = 0
            for segment in aligned_segments:
                segment_duration = segment["original_end"] - segment["original_start"]
                total_duration += segment_duration
                splice_result["segments_processed"] += 1

            splice_result["total_duration"] = total_duration

            logger.info(f"è§†é¢‘æ‹¼æ¥å®Œæˆï¼Œå¤„ç† {splice_result['segments_processed']} ä¸ªç‰‡æ®µï¼Œæ€»æ—¶é•¿ {total_duration:.1f}ç§’")
            return splice_result

        except Exception as e:
            logger.error(f"è§†é¢‘æ‹¼æ¥å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    logging.basicConfig(level=logging.INFO)
    
    # ç¤ºä¾‹ï¼šä»SRTæ–‡ä»¶å¯¼å…¥
    test_srt = "../data/input/subtitles/test.srt"
    if os.path.exists(test_srt):
        subtitles = import_srt(test_srt)
        
        if subtitles:
            # ç”Ÿæˆæ–°å‰§æœ¬
            result = generate_screenplay(subtitles, "zh")
            
            # å¯¼å‡ºæ–°SRT
            if result['status'] == 'success':
                output_path = "../data/output/generated_srt/test_output.srt"
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                export_srt(result['segments'], output_path)
                
                print(f"ç”Ÿæˆå®Œæˆ: {len(result['segments'])} ä¸ªç‰‡æ®µ, "
                     f"æ€»æ—¶é•¿: {result['total_duration']:.2f}ç§’")

    def reconstruct_screenplay_workflow(self, subtitles: List[Dict], analysis: Dict, language: str) -> List[Dict]:
        """
        é‡æ„å‰§æœ¬ - å·¥ä½œæµç¨‹æ¥å£

        Args:
            subtitles: åŸå§‹å­—å¹•åˆ—è¡¨
            analysis: å‰§æƒ…åˆ†æç»“æœ
            language: è¯­è¨€ä»£ç 

        Returns:
            List[Dict]: é‡æ„åçš„å­—å¹•åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ­ å¼€å§‹å‰§æœ¬é‡æ„ï¼Œè¯­è¨€: {language}")

            # è½¬æ¢å­—å¹•æ ¼å¼ä¸ºå†…éƒ¨æ ¼å¼
            segments = []
            for i, subtitle in enumerate(subtitles):
                segment = {
                    "id": i + 1,
                    "start_time": subtitle.get("start", "00:00:00,000"),
                    "end_time": subtitle.get("end", "00:00:00,000"),
                    "text": subtitle.get("text", ""),
                    "duration": self._calculate_duration(
                        subtitle.get("start", "00:00:00,000"),
                        subtitle.get("end", "00:00:00,000")
                    )
                }
                segments.append(segment)

            # ä½¿ç”¨ç°æœ‰çš„ç”Ÿæˆæ–¹æ³•
            result = self.generate_viral_clips(segments, language=language)

            if result["status"] == "success":
                # è½¬æ¢å›æ ‡å‡†æ ¼å¼
                reconstructed_subtitles = []
                for segment in result["segments"]:
                    subtitle = {
                        "start": segment["start_time"],
                        "end": segment["end_time"],
                        "text": segment["text"],
                        "duration": segment["duration"]
                    }
                    reconstructed_subtitles.append(subtitle)

                logger.info(f"âœ… å‰§æœ¬é‡æ„å®Œæˆï¼Œç”Ÿæˆ {len(reconstructed_subtitles)} ä¸ªç‰‡æ®µ")
                return reconstructed_subtitles
            else:
                logger.error(f"âŒ å‰§æœ¬é‡æ„å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return subtitles  # è¿”å›åŸå§‹å­—å¹•ä½œä¸ºå›é€€

        except Exception as e:
            logger.error(f"âŒ å‰§æœ¬é‡æ„å¼‚å¸¸: {e}")
            return subtitles  # è¿”å›åŸå§‹å­—å¹•ä½œä¸ºå›é€€

    def _calculate_duration(self, start_time: str, end_time: str) -> float:
        """è®¡ç®—æ—¶é•¿ï¼ˆç§’ï¼‰"""
        try:
            def time_to_seconds(time_str):
                parts = time_str.replace(',', '.').split(':')
                hours = int(parts[0])
                minutes = int(parts[1])
                seconds = float(parts[2])
                return hours * 3600 + minutes * 60 + seconds

            start_seconds = time_to_seconds(start_time)
            end_seconds = time_to_seconds(end_time)
            return end_seconds - start_seconds
        except:
            return 3.0  # é»˜è®¤3ç§’
