#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¡¬ä»¶æ£€æµ‹è°ƒè¯•å’ŒéªŒè¯å·¥å…·

æä¾›è¯¦ç»†çš„ç¡¬ä»¶æ£€æµ‹ä¿¡æ¯å’ŒéªŒè¯åŠŸèƒ½ï¼Œå¸®åŠ©æ’æŸ¥è®¾å¤‡æ£€æµ‹é—®é¢˜ã€‚
"""

import logging
import json
import time
from typing import Dict, Any, List, Optional
from pathlib import Path

# å°è¯•å¯¼å…¥ç›¸å…³æ¨¡å—
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import GPUtil
    GPUTIL_AVAILABLE = True
except ImportError:
    GPUTIL_AVAILABLE = False

try:
    import pynvml
    PYNVML_AVAILABLE = True
except ImportError:
    PYNVML_AVAILABLE = False

logger = logging.getLogger(__name__)


class HardwareDebugger:
    """ç¡¬ä»¶æ£€æµ‹è°ƒè¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è°ƒè¯•å™¨"""
        self.debug_info = {
            "timestamp": time.time(),
            "available_modules": {
                "torch": TORCH_AVAILABLE,
                "gputil": GPUTIL_AVAILABLE,
                "pynvml": PYNVML_AVAILABLE
            },
            "detection_results": {},
            "validation_results": {},
            "recommendations": {}
        }
    
    def run_comprehensive_detection(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„ç¡¬ä»¶æ£€æµ‹"""
        logger.info("ğŸ” å¼€å§‹å…¨é¢ç¡¬ä»¶æ£€æµ‹...")
        
        # æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„GPUæ£€æµ‹æ–¹æ³•
        self._test_all_gpu_detection_methods()
        
        # æ£€æµ‹å†…å­˜ä¿¡æ¯
        self._test_memory_detection()
        
        # æ£€æµ‹CPUä¿¡æ¯
        self._test_cpu_detection()
        
        # éªŒè¯æ£€æµ‹ç»“æœä¸€è‡´æ€§
        self._validate_detection_consistency()
        
        # ç”Ÿæˆæ¨èé…ç½®
        self._generate_debug_recommendations()
        
        logger.info("âœ… å…¨é¢ç¡¬ä»¶æ£€æµ‹å®Œæˆ")
        return self.debug_info
    
    def _test_all_gpu_detection_methods(self):
        """æµ‹è¯•æ‰€æœ‰GPUæ£€æµ‹æ–¹æ³•"""
        logger.info("ğŸ” æµ‹è¯•GPUæ£€æµ‹æ–¹æ³•...")
        
        gpu_results = {}
        
        # æ–¹æ³•1: PyTorch CUDA
        if TORCH_AVAILABLE:
            gpu_results["pytorch_cuda"] = self._test_pytorch_cuda()
        else:
            gpu_results["pytorch_cuda"] = {"available": False, "error": "PyTorch not installed"}
        
        # æ–¹æ³•2: GPUtil
        if GPUTIL_AVAILABLE:
            gpu_results["gputil"] = self._test_gputil()
        else:
            gpu_results["gputil"] = {"available": False, "error": "GPUtil not installed"}
        
        # æ–¹æ³•3: pynvml
        if PYNVML_AVAILABLE:
            gpu_results["pynvml"] = self._test_pynvml()
        else:
            gpu_results["pynvml"] = {"available": False, "error": "pynvml not installed"}
        
        # æ–¹æ³•4: ç³»ç»Ÿä¿¡æ¯æ¨æ–­
        gpu_results["system_inference"] = self._test_system_inference()
        
        self.debug_info["detection_results"]["gpu"] = gpu_results
    
    def _test_pytorch_cuda(self) -> Dict[str, Any]:
        """æµ‹è¯•PyTorch CUDAæ£€æµ‹"""
        try:
            result = {
                "available": torch.cuda.is_available(),
                "device_count": 0,
                "devices": [],
                "error": None
            }
            
            if result["available"]:
                result["device_count"] = torch.cuda.device_count()
                for i in range(result["device_count"]):
                    try:
                        device_info = {
                            "id": i,
                            "name": torch.cuda.get_device_name(i),
                            "properties": {}
                        }
                        
                        props = torch.cuda.get_device_properties(i)
                        device_info["properties"] = {
                            "total_memory": props.total_memory,
                            "major": props.major,
                            "minor": props.minor,
                            "multiprocessor_count": props.multiprocessor_count
                        }
                        
                        result["devices"].append(device_info)
                    except Exception as e:
                        result["devices"].append({"id": i, "error": str(e)})
            
            return result
            
        except Exception as e:
            return {"available": False, "error": str(e)}
    
    def _test_gputil(self) -> Dict[str, Any]:
        """æµ‹è¯•GPUtilæ£€æµ‹"""
        try:
            gpus = GPUtil.getGPUs()
            result = {
                "available": len(gpus) > 0,
                "device_count": len(gpus),
                "devices": [],
                "error": None
            }
            
            for i, gpu in enumerate(gpus):
                device_info = {
                    "id": i,
                    "name": gpu.name,
                    "memory_total": gpu.memoryTotal,
                    "memory_free": gpu.memoryFree,
                    "memory_used": gpu.memoryUsed,
                    "temperature": gpu.temperature,
                    "load": gpu.load
                }
                result["devices"].append(device_info)
            
            return result
            
        except Exception as e:
            return {"available": False, "error": str(e)}
    
    def _test_pynvml(self) -> Dict[str, Any]:
        """æµ‹è¯•pynvmlæ£€æµ‹"""
        try:
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            
            result = {
                "available": device_count > 0,
                "device_count": device_count,
                "devices": [],
                "error": None
            }
            
            for i in range(device_count):
                try:
                    handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                    name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                    
                    device_info = {
                        "id": i,
                        "name": name,
                        "memory_total": memory_info.total,
                        "memory_free": memory_info.free,
                        "memory_used": memory_info.used
                    }
                    result["devices"].append(device_info)
                except Exception as e:
                    result["devices"].append({"id": i, "error": str(e)})
            
            return result
            
        except Exception as e:
            return {"available": False, "error": str(e)}
    
    def _test_system_inference(self) -> Dict[str, Any]:
        """æµ‹è¯•ç³»ç»Ÿä¿¡æ¯æ¨æ–­"""
        try:
            import platform
            processor = platform.processor().lower()
            
            result = {
                "processor": processor,
                "inferred_gpu": "none",
                "confidence": "low"
            }
            
            if "intel" in processor:
                result["inferred_gpu"] = "intel_integrated"
                result["confidence"] = "medium"
            elif "amd" in processor:
                result["inferred_gpu"] = "amd_integrated"
                result["confidence"] = "medium"
            
            return result
            
        except Exception as e:
            return {"error": str(e)}
    
    def _test_memory_detection(self):
        """æµ‹è¯•å†…å­˜æ£€æµ‹"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            
            self.debug_info["detection_results"]["memory"] = {
                "total_gb": memory.total / (1024**3),
                "available_gb": memory.available / (1024**3),
                "used_gb": memory.used / (1024**3),
                "percent": memory.percent
            }
        except Exception as e:
            self.debug_info["detection_results"]["memory"] = {"error": str(e)}
    
    def _test_cpu_detection(self):
        """æµ‹è¯•CPUæ£€æµ‹"""
        try:
            import psutil
            import platform
            
            self.debug_info["detection_results"]["cpu"] = {
                "count": psutil.cpu_count(),
                "freq_current": psutil.cpu_freq().current if psutil.cpu_freq() else 0,
                "freq_max": psutil.cpu_freq().max if psutil.cpu_freq() else 0,
                "architecture": platform.architecture()[0],
                "processor": platform.processor()
            }
        except Exception as e:
            self.debug_info["detection_results"]["cpu"] = {"error": str(e)}
    
    def _validate_detection_consistency(self):
        """éªŒè¯æ£€æµ‹ç»“æœä¸€è‡´æ€§"""
        logger.info("ğŸ” éªŒè¯æ£€æµ‹ç»“æœä¸€è‡´æ€§...")
        
        gpu_results = self.debug_info["detection_results"].get("gpu", {})
        validation = {
            "gpu_count_consistency": True,
            "gpu_memory_consistency": True,
            "issues": []
        }
        
        # æ£€æŸ¥GPUæ•°é‡ä¸€è‡´æ€§
        gpu_counts = []
        for method, result in gpu_results.items():
            if result.get("available") and "device_count" in result:
                gpu_counts.append((method, result["device_count"]))
        
        if len(set(count for _, count in gpu_counts)) > 1:
            validation["gpu_count_consistency"] = False
            validation["issues"].append(f"GPUæ•°é‡ä¸ä¸€è‡´: {gpu_counts}")
        
        # æ£€æŸ¥GPUå†…å­˜ä¸€è‡´æ€§ï¼ˆå¦‚æœæœ‰å¤šç§æ£€æµ‹æ–¹æ³•éƒ½æˆåŠŸï¼‰
        gpu_memories = []
        for method, result in gpu_results.items():
            if result.get("available") and result.get("devices"):
                total_memory = 0
                for device in result["devices"]:
                    if "memory_total" in device:
                        total_memory += device["memory_total"]
                    elif "properties" in device and "total_memory" in device["properties"]:
                        total_memory += device["properties"]["total_memory"]
                if total_memory > 0:
                    gpu_memories.append((method, total_memory))
        
        if len(gpu_memories) > 1:
            # å…è®¸10%çš„è¯¯å·®
            base_memory = gpu_memories[0][1]
            for method, memory in gpu_memories[1:]:
                if abs(memory - base_memory) / base_memory > 0.1:
                    validation["gpu_memory_consistency"] = False
                    validation["issues"].append(f"GPUå†…å­˜ä¸ä¸€è‡´: {gpu_memories}")
                    break
        
        self.debug_info["validation_results"] = validation
    
    def _generate_debug_recommendations(self):
        """ç”Ÿæˆè°ƒè¯•æ¨è"""
        recommendations = []
        
        gpu_results = self.debug_info["detection_results"].get("gpu", {})
        validation = self.debug_info["validation_results"]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPUæ£€æµ‹æ–¹æ³•
        available_methods = [method for method, result in gpu_results.items() 
                           if result.get("available")]
        
        if not available_methods:
            recommendations.append("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•GPUï¼Œå»ºè®®æ£€æŸ¥é©±åŠ¨å®‰è£…")
        elif len(available_methods) == 1:
            recommendations.append(f"âš ï¸ åªæœ‰ä¸€ç§GPUæ£€æµ‹æ–¹æ³•å¯ç”¨: {available_methods[0]}")
        else:
            recommendations.append(f"âœ… å¤šç§GPUæ£€æµ‹æ–¹æ³•å¯ç”¨: {available_methods}")
        
        # æ£€æŸ¥ä¸€è‡´æ€§é—®é¢˜
        if not validation.get("gpu_count_consistency"):
            recommendations.append("âš ï¸ GPUæ•°é‡æ£€æµ‹ä¸ä¸€è‡´ï¼Œå¯èƒ½å­˜åœ¨é©±åŠ¨é—®é¢˜")
        
        if not validation.get("gpu_memory_consistency"):
            recommendations.append("âš ï¸ GPUå†…å­˜æ£€æµ‹ä¸ä¸€è‡´ï¼Œå»ºè®®ä½¿ç”¨æœ€å‡†ç¡®çš„æ£€æµ‹æ–¹æ³•")
        
        # æ¨èæœ€ä½³æ£€æµ‹æ–¹æ³•
        if "gputil" in available_methods:
            recommendations.append("âœ… æ¨èä½¿ç”¨GPUtilè¿›è¡ŒGPUæ£€æµ‹ï¼ˆæœ€å‡†ç¡®ï¼‰")
        elif "pytorch_cuda" in available_methods:
            recommendations.append("âœ… æ¨èä½¿ç”¨PyTorch CUDAè¿›è¡ŒGPUæ£€æµ‹")
        elif "pynvml" in available_methods:
            recommendations.append("âœ… æ¨èä½¿ç”¨pynvmlè¿›è¡ŒGPUæ£€æµ‹")
        
        self.debug_info["recommendations"] = recommendations
    
    def save_debug_report(self, output_path: Optional[Path] = None) -> Path:
        """ä¿å­˜è°ƒè¯•æŠ¥å‘Š"""
        if output_path is None:
            output_path = Path("logs") / f"hardware_debug_{int(time.time())}.json"
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.debug_info, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“„ è°ƒè¯•æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        return output_path
    
    def print_summary(self):
        """æ‰“å°è°ƒè¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ” ç¡¬ä»¶æ£€æµ‹è°ƒè¯•æ‘˜è¦")
        print("="*60)
        
        # å¯ç”¨æ¨¡å—
        print("\nğŸ“¦ å¯ç”¨æ¨¡å—:")
        for module, available in self.debug_info["available_modules"].items():
            status = "âœ…" if available else "âŒ"
            print(f"  {status} {module}")
        
        # GPUæ£€æµ‹ç»“æœ
        print("\nğŸ® GPUæ£€æµ‹ç»“æœ:")
        gpu_results = self.debug_info["detection_results"].get("gpu", {})
        for method, result in gpu_results.items():
            if result.get("available"):
                count = result.get("device_count", 0)
                print(f"  âœ… {method}: {count} ä¸ªGPU")
            else:
                error = result.get("error", "æœªçŸ¥é”™è¯¯")
                print(f"  âŒ {method}: {error}")
        
        # æ¨è
        print("\nğŸ’¡ æ¨è:")
        for rec in self.debug_info.get("recommendations", []):
            print(f"  {rec}")
        
        print("\n" + "="*60)


def run_hardware_debug() -> Dict[str, Any]:
    """è¿è¡Œç¡¬ä»¶è°ƒè¯•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    debugger = HardwareDebugger()
    results = debugger.run_comprehensive_detection()
    debugger.print_summary()
    debugger.save_debug_report()
    return results


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œè°ƒè¯•
    run_hardware_debug()
