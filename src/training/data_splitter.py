#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åˆ†ç¦»å™¨ - æŒ‰è¯­è¨€å’Œéš¾åº¦åˆ†ç¦»è®­ç»ƒæ•°æ®
æ”¯æŒä¸­è‹±æ–‡æ•°æ®è‡ªåŠ¨åˆ†ç±»å’Œè´¨é‡è¯„ä¼°
"""

import os
import sys
import json
import re
from typing import Dict, List, Any, Tuple, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

class DataSplitter:
    """æ•°æ®åˆ†ç¦»å™¨ - å¤šè¯­è¨€æ•°æ®åˆ†ç¦»å’Œåˆ†ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åˆ†ç¦»å™¨"""
        self.language_patterns = {
            "zh": {
                "char_range": ('\u4e00', '\u9fff'),  # ä¸­æ–‡å­—ç¬¦èŒƒå›´
                "min_ratio": 0.3,  # æœ€å°ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
                "keywords": ["çš„", "æ˜¯", "åœ¨", "äº†", "å’Œ", "æœ‰", "æˆ‘", "ä½ ", "ä»–"]
            },
            "en": {
                "char_range": (ord('a'), ord('z')),  # è‹±æ–‡å­—ç¬¦èŒƒå›´
                "min_ratio": 0.5,  # æœ€å°è‹±æ–‡å­—ç¬¦æ¯”ä¾‹
                "keywords": ["the", "and", "is", "in", "to", "of", "a", "that", "it"]
            }
        }

        print("ğŸ”„ æ•°æ®åˆ†ç¦»å™¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“Š æ”¯æŒè¯­è¨€: ä¸­æ–‡(zh), è‹±æ–‡(en)")

    def detect_language(self, text: str) -> Tuple[str, float]:
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            (è¯­è¨€ä»£ç , ç½®ä¿¡åº¦)
        """
        if not text:
            return "unknown", 0.0

        text_lower = text.lower()
        total_chars = len(text)

        # ä¸­æ–‡æ£€æµ‹
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0

        # è‹±æ–‡æ£€æµ‹
        english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
        english_ratio = english_chars / total_chars if total_chars > 0 else 0

        # å…³é”®è¯æ£€æµ‹
        chinese_keywords = sum(1 for keyword in self.language_patterns["zh"]["keywords"] if keyword in text)
        english_keywords = sum(1 for keyword in self.language_patterns["en"]["keywords"] if keyword in text_lower)

        # ç»¼åˆåˆ¤æ–­
        chinese_score = chinese_ratio * 0.7 + (chinese_keywords / 10) * 0.3
        english_score = english_ratio * 0.7 + (english_keywords / 10) * 0.3

        if chinese_score > english_score and chinese_ratio >= 0.3:
            return "zh", chinese_score
        elif english_score > chinese_score and english_ratio >= 0.5:
            return "en", english_score
        else:
            return "mixed", max(chinese_score, english_score)

    def assess_difficulty(self, text: str, language: str) -> Tuple[str, Dict[str, Any]]:
        """
        è¯„ä¼°æ–‡æœ¬éš¾åº¦

        Args:
            text: è¾“å…¥æ–‡æœ¬
            language: è¯­è¨€ä»£ç 

        Returns:
            (éš¾åº¦ç­‰çº§, è¯¦ç»†ä¿¡æ¯)
        """
        difficulty_info = {
            "length": len(text),
            "word_count": 0,
            "complexity_score": 0,
            "factors": []
        }

        if language == "zh":
            # ä¸­æ–‡éš¾åº¦è¯„ä¼°
            chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
            difficulty_info["word_count"] = chinese_chars

            # å¤æ‚åº¦å› å­
            if chinese_chars > 100:
                difficulty_info["factors"].append("é•¿æ–‡æœ¬")
                difficulty_info["complexity_score"] += 2

            # æ£€æŸ¥å¤æ‚å¥å¼
            complex_patterns = ["è™½ç„¶", "ä½†æ˜¯", "ä¸ä»…", "è€Œä¸”", "å› ä¸º", "æ‰€ä»¥"]
            complex_count = sum(1 for pattern in complex_patterns if pattern in text)
            if complex_count > 2:
                difficulty_info["factors"].append("å¤æ‚å¥å¼")
                difficulty_info["complexity_score"] += 1

        elif language == "en":
            # è‹±æ–‡éš¾åº¦è¯„ä¼°
            words = re.findall(r'\b[a-zA-Z]+\b', text)
            difficulty_info["word_count"] = len(words)

            # å¤æ‚åº¦å› å­
            if len(words) > 50:
                difficulty_info["factors"].append("long text")
                difficulty_info["complexity_score"] += 2

            # æ£€æŸ¥å¤æ‚è¯æ±‡
            complex_words = [word for word in words if len(word) > 8]
            if len(complex_words) > 3:
                difficulty_info["factors"].append("complex vocabulary")
                difficulty_info["complexity_score"] += 1

            # æ£€æŸ¥å¤æ‚å¥å¼
            if ";" in text or text.count(",") > 3:
                difficulty_info["factors"].append("complex sentences")
                difficulty_info["complexity_score"] += 1

        # ç¡®å®šéš¾åº¦ç­‰çº§
        if difficulty_info["complexity_score"] >= 4:
            difficulty = "hard"
        elif difficulty_info["complexity_score"] >= 2:
            difficulty = "medium"
        else:
            difficulty = "easy"

        return difficulty, difficulty_info

    def split_by_language(self, training_data: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        æŒ‰è¯­è¨€åˆ†ç¦»è®­ç»ƒæ•°æ®

        Args:
            training_data: åŸå§‹è®­ç»ƒæ•°æ®

        Returns:
            æŒ‰è¯­è¨€åˆ†ç»„çš„æ•°æ®
        """
        split_data = {
            "zh": [],
            "en": [],
            "mixed": [],
            "unknown": []
        }

        statistics = {
            "total_samples": len(training_data),
            "language_distribution": {"zh": 0, "en": 0, "mixed": 0, "unknown": 0},
            "quality_scores": []
        }

        for item in training_data:
            original = item.get("original", "")
            viral = item.get("viral", "")

            # æ£€æµ‹è¯­è¨€
            lang, confidence = self.detect_language(original)

            # è¯„ä¼°éš¾åº¦
            difficulty, difficulty_info = self.assess_difficulty(original, lang)

            # åˆ›å»ºå¢å¼ºçš„æ•°æ®é¡¹
            enhanced_item = {
                **item,
                "language": lang,
                "confidence": confidence,
                "difficulty": difficulty,
                "difficulty_info": difficulty_info,
                "quality_score": confidence * 0.6 + (1 - difficulty_info["complexity_score"] / 5) * 0.4
            }

            # åˆ†ç»„
            split_data[lang].append(enhanced_item)
            statistics["language_distribution"][lang] += 1
            statistics["quality_scores"].append(enhanced_item["quality_score"])

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if statistics["quality_scores"]:
            statistics["avg_quality"] = sum(statistics["quality_scores"]) / len(statistics["quality_scores"])
            statistics["min_quality"] = min(statistics["quality_scores"])
            statistics["max_quality"] = max(statistics["quality_scores"])

        return {
            "data": split_data,
            "statistics": statistics
        }

    def split_by_difficulty(self, language_data: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        æŒ‰éš¾åº¦åˆ†ç¦»æ•°æ®

        Args:
            language_data: å•è¯­è¨€æ•°æ®

        Returns:
            æŒ‰éš¾åº¦åˆ†ç»„çš„æ•°æ®
        """
        difficulty_split = {
            "easy": [],
            "medium": [],
            "hard": []
        }

        for item in language_data:
            difficulty = item.get("difficulty", "medium")
            difficulty_split[difficulty].append(item)

        return difficulty_split

    def create_training_splits(self, data: Dict[str, Any],
                             train_ratio: float = 0.8,
                             val_ratio: float = 0.1,
                             test_ratio: float = 0.1) -> Dict[str, Any]:
        """
        åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®åˆ†å‰²

        Args:
            data: åˆ†ç¦»åçš„æ•°æ®
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹

        Returns:
            åˆ†å‰²åçš„æ•°æ®é›†
        """
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 0.01:
            raise ValueError("æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ç­‰äº1.0")

        splits = {
            "train": {"zh": [], "en": []},
            "val": {"zh": [], "en": []},
            "test": {"zh": [], "en": []}
        }

        for lang in ["zh", "en"]:
            lang_data = data["data"][lang]
            total_samples = len(lang_data)

            if total_samples == 0:
                continue

            # è®¡ç®—åˆ†å‰²ç‚¹
            train_end = int(total_samples * train_ratio)
            val_end = train_end + int(total_samples * val_ratio)

            # åˆ†å‰²æ•°æ®
            splits["train"][lang] = lang_data[:train_end]
            splits["val"][lang] = lang_data[train_end:val_end]
            splits["test"][lang] = lang_data[val_end:]

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        splits["statistics"] = {
            "train_samples": sum(len(splits["train"][lang]) for lang in ["zh", "en"]),
            "val_samples": sum(len(splits["val"][lang]) for lang in ["zh", "en"]),
            "test_samples": sum(len(splits["test"][lang]) for lang in ["zh", "en"]),
            "language_distribution": {
                lang: {
                    "train": len(splits["train"][lang]),
                    "val": len(splits["val"][lang]),
                    "test": len(splits["test"][lang])
                }
                for lang in ["zh", "en"]
            }
        }

        return splits