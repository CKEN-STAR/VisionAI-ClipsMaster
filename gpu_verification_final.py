#!/usr/bin/env python3
"""
VisionAI-ClipsMaster GPUæ£€æµ‹åŠŸèƒ½å…¨é¢éªŒè¯æŠ¥å‘Š

æ­¤è„šæœ¬å¯¹GPUæ£€æµ‹åŠŸèƒ½è¿›è¡Œå…¨é¢éªŒè¯ï¼ŒåŒ…æ‹¬ï¼š
1. åŠŸèƒ½éªŒè¯æµ‹è¯•
2. çœŸå®æ€§éªŒè¯  
3. æœ‰æ•ˆæ€§æµ‹è¯•
4. å¼‚å¸¸åœºæ™¯æµ‹è¯•
"""

import os
import sys
import json
import time
import psutil
import platform
import subprocess
from datetime import datetime
from typing import Dict, List, Optional, Any

class GPUVerificationFinal:
    """GPUæ£€æµ‹åŠŸèƒ½æœ€ç»ˆéªŒè¯å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.report = {
            "verification_timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "verification_results": {},
            "performance_data": {},
            "recommendations": []
        }
        
    def _get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return {
            "platform": platform.platform(),
            "processor": platform.processor(),
            "python_version": platform.python_version(),
            "cpu_count_physical": psutil.cpu_count(logical=False),
            "cpu_count_logical": psutil.cpu_count(logical=True),
            "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
            "memory_available_gb": round(psutil.virtual_memory().available / (1024**3), 2)
        }
    
    def verify_pytorch_gpu_detection(self) -> Dict:
        """éªŒè¯PyTorch GPUæ£€æµ‹"""
        print("ğŸ” éªŒè¯PyTorch GPUæ£€æµ‹...")
        result = {
            "test_name": "PyTorch GPUæ£€æµ‹éªŒè¯",
            "status": "PASS",
            "details": {},
            "issues": []
        }
        
        try:
            import torch
            
            # åŸºæœ¬ä¿¡æ¯
            result["details"]["torch_version"] = torch.__version__
            result["details"]["cuda_available"] = torch.cuda.is_available()
            result["details"]["cuda_version"] = torch.version.cuda if torch.cuda.is_available() else None
            
            if torch.cuda.is_available():
                result["details"]["device_count"] = torch.cuda.device_count()
                result["details"]["current_device"] = torch.cuda.current_device()
                
                # è·å–æ¯ä¸ªè®¾å¤‡çš„è¯¦ç»†ä¿¡æ¯
                devices = []
                for i in range(torch.cuda.device_count()):
                    props = torch.cuda.get_device_properties(i)
                    device_info = {
                        "index": i,
                        "name": torch.cuda.get_device_name(i),
                        "total_memory_gb": round(props.total_memory / (1024**3), 2),
                        "compute_capability": f"{props.major}.{props.minor}",
                        "multiprocessor_count": props.multi_processor_count,
                        "memory_allocated": torch.cuda.memory_allocated(i),
                        "memory_reserved": torch.cuda.memory_reserved(i)
                    }
                    devices.append(device_info)
                
                result["details"]["devices"] = devices
                
                # æµ‹è¯•åŸºæœ¬GPUæ“ä½œ
                try:
                    test_tensor = torch.cuda.FloatTensor([1.0, 2.0, 3.0])
                    result["details"]["basic_operation"] = "SUCCESS"
                    del test_tensor
                    torch.cuda.empty_cache()
                except Exception as e:
                    result["details"]["basic_operation"] = f"FAILED: {str(e)}"
                    result["issues"].append(f"åŸºæœ¬GPUæ“ä½œå¤±è´¥: {str(e)}")
            else:
                result["details"]["no_gpu_reason"] = "CUDAä¸å¯ç”¨æˆ–æœªå®‰è£…CUDAç‰ˆæœ¬çš„PyTorch"
                
        except ImportError:
            result["status"] = "FAIL"
            result["issues"].append("PyTorchæœªå®‰è£…")
        except Exception as e:
            result["status"] = "FAIL"
            result["issues"].append(f"PyTorch GPUæ£€æµ‹å¤±è´¥: {str(e)}")
            
        return result
    
    def verify_system_gpu_detection(self) -> Dict:
        """éªŒè¯ç³»ç»Ÿçº§GPUæ£€æµ‹"""
        print("ğŸ” éªŒè¯ç³»ç»Ÿçº§GPUæ£€æµ‹...")
        result = {
            "test_name": "ç³»ç»Ÿçº§GPUæ£€æµ‹éªŒè¯",
            "status": "PASS",
            "details": {},
            "issues": []
        }
        
        # Windows WMIæ£€æµ‹
        try:
            wmic_result = subprocess.run(
                ["wmic", "path", "win32_VideoController", "get", "name,AdapterRAM,DriverVersion", "/format:csv"],
                capture_output=True, text=True, timeout=15
            )
            
            if wmic_result.returncode == 0:
                wmi_devices = []
                lines = wmic_result.stdout.strip().split('\n')[1:]  # Skip header
                
                for line in lines:
                    if line.strip() and ',' in line:
                        parts = [p.strip() for p in line.split(',')]
                        if len(parts) >= 4:
                            name = parts[3].strip()
                            ram = parts[1].strip()
                            driver = parts[2].strip()
                            
                            if name and name != "Name" and name:
                                device = {
                                    "name": name,
                                    "memory_bytes": int(ram) if ram.isdigit() else 0,
                                    "driver_version": driver if driver else "Unknown"
                                }
                                wmi_devices.append(device)
                
                result["details"]["wmi_detection"] = {
                    "success": True,
                    "devices": wmi_devices,
                    "device_count": len(wmi_devices)
                }
            else:
                result["details"]["wmi_detection"] = {
                    "success": False,
                    "error": wmic_result.stderr
                }
                result["issues"].append("WMI GPUæŸ¥è¯¢å¤±è´¥")
                
        except Exception as e:
            result["details"]["wmi_detection"] = {
                "success": False,
                "error": str(e)
            }
            result["issues"].append(f"WMIæ£€æµ‹å¤±è´¥: {str(e)}")
        
        # nvidia-smiæ£€æµ‹
        try:
            nvidia_smi_result = subprocess.run(
                ["nvidia-smi", "--query-gpu=name,memory.total,memory.free,driver_version", 
                 "--format=csv,noheader,nounits"],
                capture_output=True, text=True, timeout=10
            )
            
            if nvidia_smi_result.returncode == 0:
                nvidia_devices = []
                for i, line in enumerate(nvidia_smi_result.stdout.strip().split('\n')):
                    if line.strip():
                        parts = [p.strip() for p in line.split(',')]
                        if len(parts) >= 4:
                            device = {
                                "index": i,
                                "name": parts[0],
                                "memory_total_mb": int(parts[1]),
                                "memory_free_mb": int(parts[2]),
                                "driver_version": parts[3]
                            }
                            nvidia_devices.append(device)
                
                result["details"]["nvidia_smi_detection"] = {
                    "success": True,
                    "devices": nvidia_devices,
                    "device_count": len(nvidia_devices)
                }
            else:
                result["details"]["nvidia_smi_detection"] = {
                    "success": False,
                    "error": "nvidia-smiå‘½ä»¤å¤±è´¥"
                }
        except FileNotFoundError:
            result["details"]["nvidia_smi_detection"] = {
                "success": False,
                "error": "nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°"
            }
        except Exception as e:
            result["details"]["nvidia_smi_detection"] = {
                "success": False,
                "error": str(e)
            }
        
        return result
    
    def verify_device_manager_integration(self) -> Dict:
        """éªŒè¯è®¾å¤‡ç®¡ç†å™¨é›†æˆ"""
        print("ğŸ” éªŒè¯è®¾å¤‡ç®¡ç†å™¨é›†æˆ...")
        result = {
            "test_name": "è®¾å¤‡ç®¡ç†å™¨é›†æˆéªŒè¯",
            "status": "PASS",
            "details": {},
            "performance_metrics": {},
            "issues": []
        }
        
        try:
            # æ·»åŠ é¡¹ç›®è·¯å¾„
            project_path = os.path.join(os.path.dirname(__file__), 'VisionAI-ClipsMaster-Core')
            if project_path not in sys.path:
                sys.path.insert(0, project_path)
            
            from src.utils.device_manager import HybridDevice, DeviceType
            
            # æµ‹è¯•åˆå§‹åŒ–
            start_time = time.time()
            device_manager = HybridDevice()
            init_time = time.time() - start_time
            
            result["details"]["initialization"] = {
                "success": True,
                "time_seconds": init_time
            }
            
            # æµ‹è¯•è®¾å¤‡ä¿¡æ¯è·å–
            start_time = time.time()
            device_info = device_manager.get_device_info()
            info_time = time.time() - start_time
            
            result["details"]["device_info"] = {
                "success": bool(device_info),
                "time_seconds": info_time,
                "current_device": device_info.get("current_device"),
                "available_devices": [d.value for d in device_info.get("available_devices", [])],
                "has_stats": bool(device_info.get("stats"))
            }
            
            # æµ‹è¯•è®¾å¤‡é€‰æ‹©
            device_selection_results = {}
            for device_type in [DeviceType.AUTO, DeviceType.CPU_BASIC, DeviceType.CPU_AVX2, DeviceType.CUDA]:
                try:
                    start_time = time.time()
                    selected = device_manager.select_device(device_type)
                    selection_time = time.time() - start_time
                    
                    device_selection_results[device_type.value] = {
                        "requested": device_type.value,
                        "selected": selected.value if selected else None,
                        "time_seconds": selection_time,
                        "success": True
                    }
                except Exception as e:
                    device_selection_results[device_type.value] = {
                        "requested": device_type.value,
                        "selected": None,
                        "time_seconds": 0,
                        "success": False,
                        "error": str(e)
                    }
                    result["issues"].append(f"è®¾å¤‡é€‰æ‹©å¤±è´¥ {device_type.value}: {str(e)}")
            
            result["details"]["device_selection"] = device_selection_results
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            successful_selections = [r for r in device_selection_results.values() if r["success"]]
            if successful_selections:
                avg_time = sum(r["time_seconds"] for r in successful_selections) / len(successful_selections)
                max_time = max(r["time_seconds"] for r in successful_selections)
                
                result["performance_metrics"] = {
                    "average_selection_time": avg_time,
                    "max_selection_time": max_time,
                    "meets_1_5s_requirement": max_time <= 1.5,
                    "initialization_time": init_time,
                    "info_retrieval_time": info_time
                }
                
                if max_time > 1.5:
                    result["status"] = "WARN"
                    result["issues"].append(f"è®¾å¤‡åˆ‡æ¢æ—¶é—´ {max_time:.2f}s è¶…è¿‡è¦æ±‚çš„1.5s")
            
        except Exception as e:
            result["status"] = "FAIL"
            result["issues"].append(f"è®¾å¤‡ç®¡ç†å™¨é›†æˆéªŒè¯å¤±è´¥: {str(e)}")
        
        return result
    
    def verify_performance_benchmarks(self) -> Dict:
        """éªŒè¯æ€§èƒ½åŸºå‡†"""
        print("ğŸ” éªŒè¯æ€§èƒ½åŸºå‡†...")
        result = {
            "test_name": "æ€§èƒ½åŸºå‡†éªŒè¯",
            "status": "PASS",
            "details": {},
            "benchmarks": {},
            "issues": []
        }
        
        try:
            import torch
            
            # CPUåŸºå‡†æµ‹è¯•
            matrix_size = 512
            cpu_a = torch.randn(matrix_size, matrix_size)
            cpu_b = torch.randn(matrix_size, matrix_size)
            
            # CPUæ€§èƒ½æµ‹è¯•
            cpu_times = []
            for _ in range(3):
                start_time = time.time()
                cpu_result = torch.mm(cpu_a, cpu_b)
                cpu_times.append(time.time() - start_time)
            
            cpu_avg_time = sum(cpu_times) / len(cpu_times)
            
            result["benchmarks"]["cpu"] = {
                "matrix_size": f"{matrix_size}x{matrix_size}",
                "iterations": 3,
                "times": cpu_times,
                "average_time": cpu_avg_time,
                "operations_per_second": 1 / cpu_avg_time if cpu_avg_time > 0 else 0
            }
            
            # GPUåŸºå‡†æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                gpu_a = cpu_a.cuda()
                gpu_b = cpu_b.cuda()
                
                # é¢„çƒ­GPU
                for _ in range(2):
                    _ = torch.mm(gpu_a, gpu_b)
                torch.cuda.synchronize()
                
                # GPUæ€§èƒ½æµ‹è¯•
                gpu_times = []
                for _ in range(3):
                    start_time = time.time()
                    gpu_result = torch.mm(gpu_a, gpu_b)
                    torch.cuda.synchronize()
                    gpu_times.append(time.time() - start_time)
                
                gpu_avg_time = sum(gpu_times) / len(gpu_times)
                
                result["benchmarks"]["gpu"] = {
                    "matrix_size": f"{matrix_size}x{matrix_size}",
                    "iterations": 3,
                    "times": gpu_times,
                    "average_time": gpu_avg_time,
                    "operations_per_second": 1 / gpu_avg_time if gpu_avg_time > 0 else 0,
                    "speedup_vs_cpu": cpu_avg_time / gpu_avg_time if gpu_avg_time > 0 else 0
                }
                
                # éªŒè¯ç»“æœä¸€è‡´æ€§
                max_diff = torch.max(torch.abs(cpu_result - gpu_result.cpu())).item()
                result["details"]["result_consistency"] = {
                    "max_difference": max_diff,
                    "is_consistent": max_diff < 1e-4
                }
                
                if not result["details"]["result_consistency"]["is_consistent"]:
                    result["issues"].append("CPUå’ŒGPUè®¡ç®—ç»“æœä¸ä¸€è‡´")
            else:
                result["benchmarks"]["gpu"] = {"status": "GPUä¸å¯ç”¨"}
            
        except Exception as e:
            result["status"] = "FAIL"
            result["issues"].append(f"æ€§èƒ½åŸºå‡†éªŒè¯å¤±è´¥: {str(e)}")
        
        return result
    
    def run_comprehensive_verification(self) -> Dict:
        """è¿è¡Œå…¨é¢éªŒè¯"""
        print("ğŸš€ å¼€å§‹GPUæ£€æµ‹åŠŸèƒ½å…¨é¢éªŒè¯...")
        print("=" * 60)
        
        # æ‰§è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•
        verification_tests = [
            self.verify_pytorch_gpu_detection,
            self.verify_system_gpu_detection,
            self.verify_device_manager_integration,
            self.verify_performance_benchmarks
        ]
        
        for test_func in verification_tests:
            try:
                test_result = test_func()
                self.report["verification_results"][test_result["test_name"]] = test_result
                
                # æ‰“å°ç»“æœ
                status_emoji = {"PASS": "âœ…", "FAIL": "âŒ", "WARN": "âš ï¸"}
                print(f"{status_emoji.get(test_result['status'], 'â“')} {test_result['test_name']}: {test_result['status']}")
                
                if test_result["issues"]:
                    for issue in test_result["issues"]:
                        print(f"   âš ï¸ {issue}")
                        
            except Exception as e:
                error_result = {
                    "test_name": test_func.__name__,
                    "status": "FAIL",
                    "issues": [f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"]
                }
                self.report["verification_results"][test_func.__name__] = error_result
                print(f"âŒ {test_func.__name__}: FAIL - {str(e)}")
        
        # ç”Ÿæˆå»ºè®®å’Œæ€»ç»“
        self._generate_recommendations()
        self._generate_summary()
        
        return self.report
    
    def _generate_recommendations(self):
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        # åŸºäºéªŒè¯ç»“æœç”Ÿæˆå»ºè®®
        results = self.report["verification_results"]
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        pytorch_result = results.get("PyTorch GPUæ£€æµ‹éªŒè¯", {})
        if not pytorch_result.get("details", {}).get("cuda_available", False):
            recommendations.append("å»ºè®®å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥å¯ç”¨GPUåŠ é€Ÿ")
        
        # æ£€æŸ¥ç³»ç»ŸGPUæ£€æµ‹
        system_result = results.get("ç³»ç»Ÿçº§GPUæ£€æµ‹éªŒè¯", {})
        nvidia_detection = system_result.get("details", {}).get("nvidia_smi_detection", {})
        if not nvidia_detection.get("success", False):
            recommendations.append("å»ºè®®å®‰è£…NVIDIA GPUé©±åŠ¨ç¨‹åºå’ŒCUDAå·¥å…·åŒ…")
        
        # æ£€æŸ¥æ€§èƒ½
        device_manager_result = results.get("è®¾å¤‡ç®¡ç†å™¨é›†æˆéªŒè¯", {})
        performance_metrics = device_manager_result.get("performance_metrics", {})
        if not performance_metrics.get("meets_1_5s_requirement", True):
            recommendations.append("è®¾å¤‡åˆ‡æ¢æ€§èƒ½éœ€è¦ä¼˜åŒ–")
        
        # ç³»ç»Ÿå»ºè®®
        if self.report["system_info"]["memory_total_gb"] < 8:
            recommendations.append("å»ºè®®å‡çº§åˆ°8GBæˆ–æ›´å¤šå†…å­˜")
        
        self.report["recommendations"] = recommendations
    
    def _generate_summary(self):
        """ç”Ÿæˆæ€»ç»“"""
        results = self.report["verification_results"]
        total_tests = len(results)
        passed_tests = sum(1 for r in results.values() if r.get("status") == "PASS")
        failed_tests = sum(1 for r in results.values() if r.get("status") == "FAIL")
        warned_tests = sum(1 for r in results.values() if r.get("status") == "WARN")
        
        summary = {
            "total_tests": total_tests,
            "passed": passed_tests,
            "failed": failed_tests,
            "warned": warned_tests,
            "overall_status": "PASS" if failed_tests == 0 else "FAIL",
            "success_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0
        }
        
        self.report["summary"] = summary
        
        print("\n" + "=" * 60)
        print("ğŸ“Š éªŒè¯æ€»ç»“:")
        print(f"   æ€»éªŒè¯é¡¹: {total_tests}")
        print(f"   é€šè¿‡: {passed_tests} âœ…")
        print(f"   å¤±è´¥: {failed_tests} âŒ")
        print(f"   è­¦å‘Š: {warned_tests} âš ï¸")
        print(f"   æ€»ä½“çŠ¶æ€: {summary['overall_status']}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        
        if self.report["recommendations"]:
            print("\nğŸ’¡ å»ºè®®:")
            for rec in self.report["recommendations"]:
                print(f"   â€¢ {rec}")
    
    def save_report(self, filename: str = None):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"gpu_verification_final_report_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename

def main():
    """ä¸»å‡½æ•°"""
    verifier = GPUVerificationFinal()
    
    try:
        # è¿è¡Œå…¨é¢éªŒè¯
        report = verifier.run_comprehensive_verification()
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = verifier.save_report()
        
        # è¿”å›é€€å‡ºç 
        summary = report.get("summary", {})
        if summary.get("overall_status") == "FAIL":
            sys.exit(1)
        else:
            sys.exit(0)
            
    except Exception as e:
        print(f"âŒ éªŒè¯æ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
