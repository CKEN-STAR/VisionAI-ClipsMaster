#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster ç¯å¢ƒæ£€æµ‹å·¥å…·

ç‹¬ç«‹çš„ç¯å¢ƒæ£€æµ‹å·¥å…·ï¼Œç”¨äºæ£€æŸ¥ç³»ç»Ÿç¯å¢ƒæ˜¯å¦é€‚åˆè¿è¡ŒVisionAI-ClipsMasterã€‚
æ­¤è„šæœ¬ä¸ä¾èµ–äºä¸»åº”ç”¨ç¨‹åºçš„å…¶ä»–éƒ¨åˆ†ï¼Œå¯ä»¥å•ç‹¬è¿è¡Œã€‚
"""

import os
import sys
import json
import platform
import argparse
import logging
import subprocess
import time
import re
import ctypes
import shutil
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("env_checker")

def detect_environment() -> Dict[str, Any]:
    """ç²¾å‡†è¯†åˆ«è¿è¡Œæ—¶ç¯å¢ƒ

    æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶å’Œè½¯ä»¶é…ç½®ï¼Œåˆ¤æ–­æ˜¯å¦æ»¡è¶³åº”ç”¨éœ€æ±‚ã€‚
    è‡ªåŠ¨è¯†åˆ«CPUã€GPUã€å†…å­˜ã€æ“ä½œç³»ç»Ÿç­‰å…³é”®ä¿¡æ¯ï¼Œ
    ç”¨äºåç»­æ€§èƒ½ä¼˜åŒ–å’ŒåŠŸèƒ½å¯ç”¨çš„å†³ç­–ã€‚

    Returns:
        Dict[str, Any]: åŒ…å«ç¯å¢ƒè¯¦ç»†ä¿¡æ¯çš„å­—å…¸
    """
    # è·å–CPUä¿¡æ¯
    cpu_info = platform.processor()
    
    # è·å–GPUä¿¡æ¯
    gpu_info = get_gpu_info()
    
    # è·å–å†…å­˜ä¿¡æ¯ (ä»¥GBä¸ºå•ä½)
    try:
        import psutil
        ram_total = psutil.virtual_memory().total / (1024**3)
    except ImportError:
        # å¦‚æœæ²¡æœ‰psutilåº“ï¼Œå°è¯•å…¶ä»–æ–¹æ³•è·å–å†…å­˜ä¿¡æ¯
        ram_total = get_memory_fallback()
    
    # è·å–æ“ä½œç³»ç»Ÿä¿¡æ¯
    os_info = platform.platform()
    
    # è·å–å­˜å‚¨ç©ºé—´ä¿¡æ¯
    storage_info = get_storage_info()
    
    # æ„å»ºç¯å¢ƒä¿¡æ¯å­—å…¸
    environment = {
        "cpu": cpu_info,
        "gpu": gpu_info,
        "ram": ram_total,
        "os": os_info,
        "storage": storage_info,
        "detection_time": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # æ·»åŠ ç¼–ç¨‹ç¯å¢ƒä¿¡æ¯
    environment["python_version"] = platform.python_version()
    environment["is_64bit"] = platform.machine().endswith('64')
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºDockerå®¹å™¨
    environment["is_docker"] = is_running_in_docker()
    
    # åˆ¤æ–­æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥
    environment["has_network"] = check_network_connectivity()
    
    # è·å–CPUæ ¸å¿ƒæ•°
    try:
        environment["cpu_cores"] = os.cpu_count() or 0
    except:
        environment["cpu_cores"] = 0
    
    logger.info(f"å·²å®Œæˆç¯å¢ƒæ£€æµ‹: CPU={cpu_info}, RAM={ram_total:.2f}GB, GPU={gpu_info}")
    
    return environment


def get_gpu_info() -> str:
    """
    è·å–GPUä¿¡æ¯
    
    Returns:
        str: GPUå‹å·åç§°ï¼Œå¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°åˆ™è¿”å›"é›†æˆæ˜¾å¡"
    """
    system = platform.system()
    
    # æ£€æµ‹NVIDIA GPU
    try:
        nvidia_smi = shutil.which("nvidia-smi")
        if nvidia_smi:
            output = subprocess.check_output([nvidia_smi, "--query-gpu=name", "--format=csv,noheader"]).decode().strip()
            if output:
                return output
    except:
        pass
    
    # æ£€æµ‹å…¶ä»–GPU
    if system == "Windows":
        try:
            # å°è¯•ä½¿ç”¨wmic
            output = subprocess.check_output(["wmic", "path", "win32_VideoController", "get", "Name"], 
                                         stderr=subprocess.DEVNULL).decode('utf-8', errors='ignore').strip()
            # æå–ç¬¬ä¸€ä¸ªéç©ºè¡Œï¼Œè·³è¿‡æ ‡é¢˜è¡Œ
            lines = [line.strip() for line in output.split('\n') if line.strip()]
            if len(lines) > 1:
                return lines[1]  # ç¬¬ä¸€è¡Œæ˜¯"Name"æ ‡é¢˜
        except:
            pass
    elif system == "Linux":
        try:
            # å°è¯•ä»lspciè·å–
            output = subprocess.check_output(["lspci", "-v"], stderr=subprocess.DEVNULL).decode('utf-8', errors='ignore')
            # æŸ¥æ‰¾VGAæ§åˆ¶å™¨
            vga_match = re.search(r"VGA compatible controller: (.*?)(?:\\n|$)", output)
            if vga_match:
                return vga_match.group(1).strip()
        except:
            pass
    elif system == "Darwin":
        try:
            # åœ¨macOSä¸Šè·å–GPUä¿¡æ¯
            output = subprocess.check_output(["system_profiler", "SPDisplaysDataType"], 
                                         stderr=subprocess.DEVNULL).decode('utf-8', errors='ignore')
            # æå–å‹å·åç§°
            chip_match = re.search(r"Chipset Model: (.*?)(?:\\n|$)", output)
            if chip_match:
                return chip_match.group(1).strip()
        except:
            pass
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
    return "é›†æˆæ˜¾å¡"


def get_memory_fallback() -> float:
    """
    åœ¨æ²¡æœ‰psutilåº“çš„æƒ…å†µä¸‹è·å–å†…å­˜å¤§å°
    
    Returns:
        float: å†…å­˜å¤§å°(GB)
    """
    system = platform.system()
    
    if system == "Windows":
        try:
            # Windowsä¸‹ä½¿ç”¨ctypesè·å–å†…å­˜ä¿¡æ¯
            kernel32 = ctypes.windll.kernel32
            c_ulonglong = ctypes.c_ulonglong
            
            class MEMORYSTATUSEX(ctypes.Structure):
                _fields_ = [
                    ("dwLength", ctypes.c_ulong),
                    ("dwMemoryLoad", ctypes.c_ulong),
                    ("ullTotalPhys", c_ulonglong),
                    ("ullAvailPhys", c_ulonglong),
                    ("ullTotalPageFile", c_ulonglong),
                    ("ullAvailPageFile", c_ulonglong),
                    ("ullTotalVirtual", c_ulonglong),
                    ("ullAvailVirtual", c_ulonglong),
                    ("ullAvailExtendedVirtual", c_ulonglong),
                ]
            
            memoryStatus = MEMORYSTATUSEX()
            memoryStatus.dwLength = ctypes.sizeof(MEMORYSTATUSEX)
            kernel32.GlobalMemoryStatusEx(ctypes.byref(memoryStatus))
            
            return memoryStatus.ullTotalPhys / (1024**3)  # è½¬æ¢ä¸ºGB
        except:
            pass
    elif system == "Linux":
        try:
            # Linuxä¸‹ä»/proc/meminfoè¯»å–
            with open("/proc/meminfo", "r") as f:
                meminfo = f.read()
            
            # æå–MemTotal
            match = re.search(r"MemTotal:\\\\\\1+(\\\\\\1+) kB", meminfo)
            if match:
                return int(match.group(1)) / (1024**2)  # ä»kBè½¬æ¢ä¸ºGB
        except:
            pass
    elif system == "Darwin":
        try:
            # macOSä¸‹ä½¿ç”¨sysctl
            output = subprocess.check_output(["sysctl", "-n", "hw.memsize"]).decode().strip()
            return int(output) / (1024**3)  # è½¬æ¢ä¸ºGB
        except:
            pass
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
    return 4.0  # å‡è®¾4GBå†…å­˜


def get_storage_info() -> Dict[str, Any]:
    """
    è·å–å­˜å‚¨ç©ºé—´ä¿¡æ¯
    
    Returns:
        Dict[str, Any]: å­˜å‚¨ç©ºé—´ä¿¡æ¯
    """
    try:
        # è·å–å½“å‰ç›®å½•å­˜å‚¨ä¿¡æ¯
        path = os.path.abspath(os.curdir)
        
        if platform.system() == "Windows":
            # Windowsä¸‹ä½¿ç”¨ctypesè·å–ç£ç›˜ç©ºé—´
            free_bytes = ctypes.c_ulonglong(0)
            total_bytes = ctypes.c_ulonglong(0)
            ctypes.windll.kernel32.GetDiskFreeSpaceExW(
                ctypes.c_wchar_p(path), None, ctypes.pointer(total_bytes), ctypes.pointer(free_bytes)
            )
            total = total_bytes.value
            free = free_bytes.value
        else:
            # Linux/macOS
            stats = os.statvfs(path)
            total = stats.f_frsize * stats.f_blocks
            free = stats.f_frsize * stats.f_bavail
        
        # è®¡ç®—å·²ç”¨ç©ºé—´
        used = total - free
        
        return {
            "total_gb": total / (1024**3),
            "free_gb": free / (1024**3),
            "used_gb": used / (1024**3),
            "used_percent": (used / total) * 100 if total > 0 else 0
        }
    except Exception as e:
        logger.warning(f"è·å–å­˜å‚¨ç©ºé—´ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "total_gb": 0,
            "free_gb": 0,
            "used_gb": 0,
            "used_percent": 0
        }


def is_running_in_docker() -> bool:
    """
    æ£€æµ‹æ˜¯å¦åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œ
    
    Returns:
        bool: æ˜¯å¦åœ¨Dockerå®¹å™¨ä¸­
    """
    # æ–¹æ³•1: æ£€æŸ¥cgroup
    try:
        with open('/proc/1/cgroup', 'r') as f:
            return 'docker' in f.read() or '.scope' in f.read()
    except:
        pass
    
    # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦å­˜åœ¨/.dockerenvæ–‡ä»¶
    if os.path.exists('/.dockerenv'):
        return True
    
    return False


def check_network_connectivity() -> bool:
    """
    æ£€æŸ¥ç½‘ç»œè¿æ¥
    
    Returns:
        bool: æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥
    """
    try:
        # å°è¯•è¿æ¥åˆ°ä¸€ä¸ªå¯é çš„æœåŠ¡å™¨
        if platform.system() == "Windows":
            output = subprocess.check_output(["ping", "-n", "1", "-w", "1000", "8.8.8.8"], 
                                         stderr=subprocess.DEVNULL)
        else:
            output = subprocess.check_output(["ping", "-c", "1", "-W", "1", "8.8.8.8"], 
                                         stderr=subprocess.DEVNULL)
        return True
    except:
        return False


def get_device_tier(device_info: Dict[str, Any]) -> str:
    """
    æ ¹æ®è®¾å¤‡ä¿¡æ¯ç¡®å®šè®¾å¤‡ç­‰çº§
    
    Args:
        device_info: è®¾å¤‡ä¿¡æ¯å­—å…¸
        
    Returns:
        str: è®¾å¤‡ç­‰çº§ ('entry', 'mid', 'high', 'premium')
    """
    ram = device_info.get("ram", 0)
    has_gpu = device_info.get("gpu", "é›†æˆæ˜¾å¡") != "é›†æˆæ˜¾å¡" and "Intel" not in device_info.get("gpu", "")
    
    if ram >= 32 and has_gpu:
        return "premium"
    elif ram >= 16 and has_gpu:
        return "high"
    elif ram >= 8:
        return "mid"
    else:
        return "entry"


def get_supported_features(device_tier: str, has_gpu: bool) -> Tuple[List[str], List[str], List[str]]:
    """
    è·å–è®¾å¤‡æ”¯æŒçš„åŠŸèƒ½
    
    Args:
        device_tier: è®¾å¤‡ç­‰çº§
        has_gpu: æ˜¯å¦æœ‰ç‹¬ç«‹GPU
        
    Returns:
        Tuple[List[str], List[str], List[str]]: æ”¯æŒçš„åŠŸèƒ½ã€å—é™åŠŸèƒ½ã€ä¸æ”¯æŒçš„åŠŸèƒ½
    """
    all_features = [
        "basic_processing",  # åŸºæœ¬è§†é¢‘å¤„ç†
        "4k_processing",     # 4Kè§†é¢‘å¤„ç†
        "real_time_enhancement",  # å®æ—¶è§†é¢‘å¢å¼º
        "batch_processing",  # æ‰¹é‡å¤„ç†
        "multi_language"     # å¤šè¯­è¨€å¤„ç†
    ]
    
    supported = []
    limited = []
    unsupported = []
    
    # æ‰€æœ‰ç­‰çº§éƒ½æ”¯æŒåŸºæœ¬å¤„ç†å’Œå¤šè¯­è¨€
    supported.append("basic_processing")
    supported.append("multi_language")
    
    if device_tier in ["premium", "high"]:
        # é¡¶çº§å’Œé«˜ç«¯è®¾å¤‡æ”¯æŒæ‰€æœ‰åŠŸèƒ½
        supported.append("4k_processing")
        supported.append("real_time_enhancement")
        supported.append("batch_processing")
    elif device_tier == "mid":
        # ä¸­ç«¯è®¾å¤‡æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œ4Kå¯èƒ½å—é™ï¼Œå®æ—¶å¢å¼ºå¯èƒ½ä¸æ”¯æŒ
        supported.append("batch_processing")
        if has_gpu:
            limited.append("4k_processing")
            limited.append("real_time_enhancement")
        else:
            limited.append("4k_processing")
            unsupported.append("real_time_enhancement")
    else:
        # å…¥é—¨çº§è®¾å¤‡ï¼Œæ‰¹é‡å¤„ç†å—é™ï¼Œå…¶ä»–é«˜çº§åŠŸèƒ½ä¸æ”¯æŒ
        limited.append("batch_processing")
        unsupported.append("4k_processing")
        unsupported.append("real_time_enhancement")
    
    return supported, limited, unsupported


def get_upgrade_recommendations(device_info: Dict[str, Any], 
                              limited_features: List[str], 
                              unsupported_features: List[str]) -> List[str]:
    """
    ç”Ÿæˆå‡çº§å»ºè®®
    
    Args:
        device_info: è®¾å¤‡ä¿¡æ¯
        limited_features: å—é™åŠŸèƒ½
        unsupported_features: ä¸æ”¯æŒçš„åŠŸèƒ½
        
    Returns:
        List[str]: å‡çº§å»ºè®®åˆ—è¡¨
    """
    recommendations = []
    
    ram = device_info.get("ram", 0)
    cpu_cores = device_info.get("cpu_cores", 0)
    has_gpu = device_info.get("gpu", "é›†æˆæ˜¾å¡") != "é›†æˆæ˜¾å¡" and "Intel" not in device_info.get("gpu", "")
    storage_free = device_info.get("storage", {}).get("free_gb", 0)
    
    if ram < 8:
        recommendations.append("å»ºè®®è‡³å°‘å‡çº§è‡³8GBå†…å­˜ä»¥æ”¯æŒæ›´å¤šåŠŸèƒ½")
    elif ram < 16 and ("4k_processing" in limited_features or "4k_processing" in unsupported_features):
        recommendations.append("å»ºè®®å‡çº§è‡³16GBæˆ–æ›´é«˜å†…å­˜ä»¥è·å¾—æ›´å¥½çš„4Kå¤„ç†èƒ½åŠ›")
    
    if not has_gpu:
        if "real_time_enhancement" in unsupported_features:
            recommendations.append("å»ºè®®æ·»åŠ æ”¯æŒCUDAæˆ–ROCmçš„ç‹¬ç«‹GPUä»¥å¯ç”¨å®æ—¶å¢å¼ºåŠŸèƒ½")
        elif "4k_processing" in limited_features:
            recommendations.append("å»ºè®®æ·»åŠ ç‹¬ç«‹GPUä»¥æå‡4Kè§†é¢‘å¤„ç†èƒ½åŠ›")
    
    if cpu_cores < 4:
        recommendations.append("å»ºè®®ä½¿ç”¨è‡³å°‘4æ ¸CPUä»¥æå‡å¤„ç†é€Ÿåº¦")
    
    if storage_free < 20:
        recommendations.append("å»ºè®®ç¡®ä¿è‡³å°‘20GBå¯ç”¨å­˜å‚¨ç©ºé—´ä»¥æ”¯æŒæ‰¹é‡å¤„ç†")
    
    return recommendations


def generate_compatibility_report(device_info: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç”Ÿæˆå®Œæ•´çš„å…¼å®¹æ€§æŠ¥å‘Š
    
    Args:
        device_info: è®¾å¤‡ä¿¡æ¯
        
    Returns:
        Dict[str, Any]: å…¼å®¹æ€§æŠ¥å‘Š
    """
    # ç¡®å®šè®¾å¤‡ç­‰çº§
    device_tier = get_device_tier(device_info)
    
    # ç¡®å®šGPUæ”¯æŒ
    gpu_info = device_info.get("gpu", "é›†æˆæ˜¾å¡")
    has_gpu = gpu_info != "é›†æˆæ˜¾å¡" and "Intel" not in gpu_info
    
    # è·å–æ”¯æŒçš„åŠŸèƒ½
    supported, limited, unsupported = get_supported_features(device_tier, has_gpu)
    
    # ç”Ÿæˆå‡çº§å»ºè®®
    recommendations = get_upgrade_recommendations(device_info, limited, unsupported)
    
    # ç”Ÿæˆæ€§èƒ½é¢„æœŸ
    expected_performance = get_expected_performance(device_tier)
    
    # ç»„è£…æŠ¥å‘Š
    return {
        "device_tier": device_tier,
        "expected_performance": expected_performance,
        "supported_features": supported,
        "limited_features": limited,
        "unsupported_features": unsupported,
        "recommendations": recommendations
    }


def get_expected_performance(device_tier: str) -> Dict[str, Any]:
    """
    è·å–é¢„æœŸæ€§èƒ½æŒ‡æ ‡
    
    Args:
        device_tier: è®¾å¤‡ç­‰çº§
        
    Returns:
        Dict[str, Any]: æ€§èƒ½æŒ‡æ ‡
    """
    performance = {
        "entry": {
            "fps": 15,
            "processing_speed": "0.5xå®æ—¶",
            "max_resolution": "1080p",
            "concurrent_tasks": 1
        },
        "mid": {
            "fps": 30,
            "processing_speed": "1xå®æ—¶",
            "max_resolution": "2K",
            "concurrent_tasks": 2
        },
        "high": {
            "fps": 45,
            "processing_speed": "2xå®æ—¶",
            "max_resolution": "4K",
            "concurrent_tasks": 4
        },
        "premium": {
            "fps": 60,
            "processing_speed": "3xå®æ—¶",
            "max_resolution": "4K+",
            "concurrent_tasks": 8
        }
    }
    
    return performance.get(device_tier, performance["entry"])


def get_optimal_config(device_info: Dict[str, Any], compatibility: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ ¹æ®è®¾å¤‡ä¿¡æ¯ç”Ÿæˆæœ€ä½³åº”ç”¨é…ç½®
    
    Args:
        device_info: è®¾å¤‡ä¿¡æ¯
        compatibility: å…¼å®¹æ€§æŠ¥å‘Š
        
    Returns:
        Dict[str, Any]: æœ€ä½³åº”ç”¨é…ç½®
    """
    device_tier = compatibility.get("device_tier", "entry")
    ram = device_info.get("ram", 4)
    gpu = device_info.get("gpu", "é›†æˆæ˜¾å¡")
    cpu_cores = device_info.get("cpu_cores", 2)
    has_gpu = gpu != "é›†æˆæ˜¾å¡" and "Intel" not in gpu
    
    # ç¡®å®šæ‰¹å¤„ç†å¤§å°
    batch_size = 1
    if device_tier == "premium":
        batch_size = 16
    elif device_tier == "high":
        batch_size = 8
    elif device_tier == "mid" and ram >= 8:
        batch_size = 4
    
    # ç¡®å®šçº¿ç¨‹æ•°
    threads = min(max(2, cpu_cores), 8)
    
    # ç¡®å®šé¢„è§ˆè´¨é‡
    preview_quality = "low"  # é»˜è®¤ä½è´¨é‡
    if device_tier in ["premium", "high"]:
        preview_quality = "high"
    elif device_tier == "mid":
        preview_quality = "medium"
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨æ¨¡å‹é‡åŒ–
    use_quantization = ram < 16
    use_low_memory = ram < 8
    
    # ç¡®å®šé¦–é€‰æ¨¡å‹
    # é»˜è®¤ä½¿ç”¨è‹±æ–‡Mistralæ¨¡å‹ï¼Œä½†å¦‚æœç³»ç»Ÿè¯­è¨€æ˜¯ä¸­æ–‡åˆ™ä½¿ç”¨Qwen
    system_locale = get_system_locale()
    is_chinese = "zh" in system_locale.lower()
    
    if is_chinese:
        if ram >= 16 and has_gpu:
            preferred_model = "qwen2.5-7b"
        else:
            preferred_model = "qwen2.5-1.8b"  # è½»é‡ç‰ˆ
    else:
        if ram >= 16 and has_gpu:
            preferred_model = "mistral-7b"
        else:
            preferred_model = "mistral-instruct"  # è½»é‡ç‰ˆ
    
    return {
        "general": {
            "debug_mode": False,
            "log_level": "INFO",
            "data_dir": str(Path.home() / ".visionai" / "data"),
            "temp_dir": str(Path.home() / ".visionai" / "temp"),
            "max_history": 100
        },
        "performance": {
            "threads": threads,
            "use_gpu": has_gpu,
            "memory_limit": int(ram * 0.7 * 1024),  # MB, æœ€å¤šä½¿ç”¨70%å†…å­˜
            "batch_size": batch_size,
            "preview_quality": preview_quality
        },
        "models": {
            "use_quantization": use_quantization,
            "use_low_memory": use_low_memory,
            "preferred_model": preferred_model
        }
    }


def get_system_locale() -> str:
    """
    æ£€æµ‹ç³»ç»Ÿè¯­è¨€ç¯å¢ƒ
    
    Returns:
        str: ç³»ç»Ÿè¯­è¨€ä»£ç 
    """
    try:
        import locale
        return locale.getdefaultlocale()[0] or "en_US"
    except:
        return "en_US"  # é»˜è®¤è‹±è¯­


def check_and_print_environment(verbose: bool = False, save_report: bool = False):
    """
    æ£€æµ‹å¹¶æ‰“å°ç¯å¢ƒä¿¡æ¯
    
    Args:
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        save_report: æ˜¯å¦ä¿å­˜æŠ¥å‘Š
    """
    # è·å–ç¯å¢ƒä¿¡æ¯
    environment = detect_environment()
    
    # ç”Ÿæˆå…¼å®¹æ€§æŠ¥å‘Š
    compatibility = generate_compatibility_report(environment)
    
    # ç”Ÿæˆæœ€ä½³é…ç½®
    optimal_config = get_optimal_config(environment, compatibility)
    
    # æ‰“å°æ ‡é¢˜
    print("\n" + "=" * 70)
    print("VisionAI-ClipsMaster ç¯å¢ƒå…¼å®¹æ€§æŠ¥å‘Š")
    print("=" * 70)
    
    # ç¡®å®šè®¾å¤‡ç­‰çº§æ˜¾ç¤ºåç§°
    tier = compatibility.get("device_tier", "entry")
    tier_names = {
        "entry": "å…¥é—¨çº§",
        "mid": "ä¸­ç«¯",
        "high": "é«˜ç«¯", 
        "premium": "é¡¶çº§"
    }
    tier_display = tier_names.get(tier, "æœªçŸ¥")
    
    # æ‰“å°åŸºæœ¬ç¯å¢ƒä¿¡æ¯
    print(f"\nğŸ“Š è®¾å¤‡æ‘˜è¦: {tier_display}è®¾å¤‡ - "
          f"CPU: {environment.get('cpu', 'Unknown')}, "
          f"RAM: {environment.get('ram', 0):.1f}GB, "
          f"GPU: {environment.get('gpu', 'Unknown')}")
    
    # è·å–ç¡¬ä»¶ä¿¡æ¯
    print("\nğŸ–¥ï¸  ç¡¬ä»¶ä¿¡æ¯:")
    print(f"  CPU: {environment.get('cpu', 'Unknown')}")
    print(f"  GPU: {environment.get('gpu', 'Unknown')}")
    print(f"  å†…å­˜: {environment.get('ram', 0):.2f} GB")
    print(f"  ç£ç›˜å¯ç”¨ç©ºé—´: {environment.get('storage', {}).get('free_gb', 0):.2f} GB")
    
    # è·å–æ“ä½œç³»ç»Ÿä¿¡æ¯
    print(f"\nğŸ’» æ“ä½œç³»ç»Ÿ: {environment.get('os', 'Unknown')}")
    print(f"  Pythonç‰ˆæœ¬: {environment.get('python_version', 'Unknown')}")
    
    # æ‰“å°è®¾å¤‡ç­‰çº§
    print(f"\nğŸ“± è®¾å¤‡ç­‰çº§: {tier_display}")
    
    # æ‰“å°åŠŸèƒ½æ”¯æŒçŠ¶æ€
    print("\nâœ… åŠŸèƒ½æ”¯æŒçŠ¶æ€:")
    features = [
        ("basic_processing", "åŸºæœ¬è§†é¢‘å¤„ç†"),
        ("4k_processing", "4Kè§†é¢‘å¤„ç†"),
        ("real_time_enhancement", "å®æ—¶è§†é¢‘å¢å¼º"),
        ("batch_processing", "æ‰¹é‡å¤„ç†"),
        ("multi_language", "å¤šè¯­è¨€æ”¯æŒ")
    ]
    
    for feature_id, feature_name in features:
        if feature_id in compatibility.get("supported_features", []):
            print(f"  {feature_name}: âœ“ æ”¯æŒ")
        elif feature_id in compatibility.get("limited_features", []):
            print(f"  {feature_name}: âš  å—é™æ”¯æŒ")
        else:
            print(f"  {feature_name}: âœ— ä¸æ”¯æŒ")
    
    # æ‰“å°é¢„æœŸæ€§èƒ½
    performance = compatibility.get("expected_performance", {})
    print("\nâš¡ é¢„æœŸæ€§èƒ½:")
    print(f"  å¤„ç†é€Ÿåº¦: {performance.get('processing_speed', 'æœªçŸ¥')}")
    print(f"  æœ€å¤§åˆ†è¾¨ç‡: {performance.get('max_resolution', 'æœªçŸ¥')}")
    print(f"  å¹¶å‘ä»»åŠ¡æ•°: {performance.get('concurrent_tasks', 1)}")
    
    # æ‰“å°ä¼˜åŒ–å»ºè®®
    if compatibility.get("recommendations", []):
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(compatibility["recommendations"], 1):
            print(f"  {i}. {rec}")
    
    # æ‰“å°åº”ç”¨æ¨èé…ç½®
    if verbose:
        print("\nâš™ï¸ æ¨èåº”ç”¨é…ç½®:")
        print(f"  ä½¿ç”¨GPUåŠ é€Ÿ: {'æ˜¯' if optimal_config['performance']['use_gpu'] else 'å¦'}")
        print(f"  ä½¿ç”¨çº¿ç¨‹æ•°: {optimal_config['performance']['threads']}")
        print(f"  æ‰¹å¤„ç†å¤§å°: {optimal_config['performance']['batch_size']}")
        print(f"  é¢„è§ˆè´¨é‡: {optimal_config['performance']['preview_quality']}")
        print(f"  å†…å­˜é™åˆ¶: {optimal_config['performance']['memory_limit'] / 1024:.1f} GB")
        print(f"  ä½¿ç”¨é‡åŒ–æ¨¡å‹: {'æ˜¯' if optimal_config['models']['use_quantization'] else 'å¦'}")
        print(f"  ä½å†…å­˜æ¨¡å¼: {'æ˜¯' if optimal_config['models']['use_low_memory'] else 'å¦'}")
        print(f"  æ¨èæ¨¡å‹: {optimal_config['models']['preferred_model']}")
    
    # ä¿å­˜æŠ¥å‘Š
    if save_report:
        output_dir = Path("test_output")
        output_dir.mkdir(exist_ok=True)
        report_path = output_dir / "environment_report.json"
        
        report = {
            "environment": environment,
            "compatibility": compatibility,
            "optimal_config": optimal_config
        }
        
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    
    # ç»“è®º
    print("\n" + "-" * 70)
    if tier in ["high", "premium"]:
        print("âœ… ç»“è®º: å½“å‰è®¾å¤‡å®Œå…¨é€‚åˆè¿è¡ŒVisionAI-ClipsMasterçš„æ‰€æœ‰åŠŸèƒ½ã€‚")
    elif tier == "mid":
        print("âœ… ç»“è®º: å½“å‰è®¾å¤‡å¯ä»¥è¿è¡ŒVisionAI-ClipsMasterçš„å¤§éƒ¨åˆ†åŠŸèƒ½ï¼Œä½†é«˜çº§åŠŸèƒ½å¯èƒ½å—é™ã€‚")
    else:
        print("âš ï¸ ç»“è®º: å½“å‰è®¾å¤‡å¯ä»¥è¿è¡ŒåŸºæœ¬åŠŸèƒ½ï¼Œä½†é«˜çº§åŠŸèƒ½å°†å—åˆ°é™åˆ¶ã€‚å»ºè®®å‚è€ƒä¼˜åŒ–å»ºè®®ã€‚")
    print("=" * 70 + "\n")


def main():
    """ä¸»å‡½æ•°å…¥å£"""
    parser = argparse.ArgumentParser(description="VisionAI-ClipsMasterç¯å¢ƒæ£€æµ‹å·¥å…·")
    parser.add_argument("-v", "--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯")
    parser.add_argument("-s", "--save", action="store_true", help="ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶")
    parser.add_argument("-j", "--json", action="store_true", help="è¾“å‡ºJSONæ ¼å¼ç»“æœ")
    args = parser.parse_args()
    
    if args.json:
        # è¾“å‡ºJSONæ ¼å¼ç»“æœ
        environment = detect_environment()
        compatibility = generate_compatibility_report(environment)
        optimal_config = get_optimal_config(environment, compatibility)
        
        report = {
            "environment": environment,
            "compatibility": compatibility,
            "optimal_config": optimal_config
        }
        
        print(json.dumps(report, indent=2, ensure_ascii=False))
    else:
        # è¾“å‡ºäººç±»å¯è¯»æ ¼å¼
        check_and_print_environment(verbose=args.verbose, save_report=args.save)


if __name__ == "__main__":
    main() 