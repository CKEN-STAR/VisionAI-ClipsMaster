#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•
===================================

æµ‹è¯•å†…å®¹ï¼š
1. å†…å­˜ä½¿ç”¨ç›‘æ§ï¼ˆå³°å€¼â‰¤3.8GBï¼‰
2. å¼‚å¸¸æ¢å¤æœºåˆ¶æµ‹è¯•
3. é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§éªŒè¯
4. æ–­ç‚¹ç»­å‰ªåŠŸèƒ½æµ‹è¯•
5. èµ„æºæ¸…ç†éªŒè¯

ä½œè€…: VisionAI-ClipsMaster Team
ç‰ˆæœ¬: v1.0.0
æ—¥æœŸ: 2025-01-26
"""

import os
import sys
import json
import time
import logging
import psutil
import threading
import tempfile
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import gc

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.peak_memory = 0
        self.memory_history = []
        self.monitor_thread = None
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.peak_memory = 0
        self.memory_history = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        logger.info("å†…å­˜ç›‘æ§å·²å¯åŠ¨")
        
    def stop_monitoring(self) -> Dict[str, Any]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2)
            
        current_memory = psutil.Process().memory_info().rss
        
        return {
            "peak_memory_mb": self.peak_memory / (1024 * 1024),
            "current_memory_mb": current_memory / (1024 * 1024),
            "memory_samples": len(self.memory_history),
            "within_limit": self.peak_memory / (1024 * 1024) <= 3800,
            "memory_history": self.memory_history[-10:]  # æœ€å10ä¸ªé‡‡æ ·ç‚¹
        }
        
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                process = psutil.Process()
                current_memory = process.memory_info().rss
                self.peak_memory = max(self.peak_memory, current_memory)
                
                # è®°å½•å†…å­˜å†å²ï¼ˆæ¯ç§’é‡‡æ ·ï¼‰
                self.memory_history.append({
                    "timestamp": time.time(),
                    "memory_mb": current_memory / (1024 * 1024),
                    "cpu_percent": process.cpu_percent()
                })
                
                # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
                if len(self.memory_history) > 3600:  # æœ€å¤šä¿ç•™1å°æ—¶çš„æ•°æ®
                    self.memory_history = self.memory_history[-1800:]  # ä¿ç•™30åˆ†é’Ÿ
                    
                time.sleep(1)  # æ¯ç§’é‡‡æ ·ä¸€æ¬¡
            except Exception as e:
                logger.warning(f"å†…å­˜ç›‘æ§å¼‚å¸¸: {e}")
                break

class PerformanceStabilityTest:
    """æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_dir = Path(tempfile.mkdtemp(prefix="perf_test_"))
        self.memory_monitor = MemoryMonitor()
        self.test_results = {}
        
        logger.info(f"æ€§èƒ½æµ‹è¯•åˆå§‹åŒ–ï¼Œæµ‹è¯•ç›®å½•: {self.test_dir}")

    def test_memory_usage_limits(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä½¿ç”¨é™åˆ¶"""
        logger.info("å¼€å§‹å†…å­˜ä½¿ç”¨é™åˆ¶æµ‹è¯•")
        
        self.memory_monitor.start_monitoring()
        
        try:
            # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®æ¥æ¨¡æ‹Ÿé«˜å†…å­˜ä½¿ç”¨åœºæ™¯
            large_data = []
            
            # æ¨¡æ‹ŸåŠ è½½å¤§å‹å­—å¹•æ–‡ä»¶
            for i in range(1000):
                subtitle_data = {
                    "index": i,
                    "start_time": i * 3.0,
                    "end_time": (i + 1) * 3.0,
                    "text": f"è¿™æ˜¯ç¬¬{i}æ¡å­—å¹•ï¼ŒåŒ…å«ä¸€äº›æµ‹è¯•å†…å®¹" * 10  # å¢åŠ å†…å®¹é•¿åº¦
                }
                large_data.append(subtitle_data)
                
                # æ¯100æ¡æ£€æŸ¥ä¸€æ¬¡å†…å­˜
                if i % 100 == 0:
                    current_memory = psutil.Process().memory_info().rss / (1024 * 1024)
                    if current_memory > 3800:  # å¦‚æœè¶…è¿‡3.8GBï¼Œåœæ­¢æµ‹è¯•
                        logger.warning(f"å†…å­˜ä½¿ç”¨è¶…é™: {current_memory:.1f}MB")
                        break
            
            # æ¨¡æ‹ŸAIå¤„ç†è¿‡ç¨‹
            from src.core.screenplay_engineer import ScreenplayEngineer
            engineer = ScreenplayEngineer()
            
            # å¤„ç†å¤§é‡æ•°æ®
            for batch_start in range(0, len(large_data), 100):
                batch = large_data[batch_start:batch_start + 100]
                analysis = engineer.analyze_plot_structure(batch)
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
                
            # æ¨¡æ‹Ÿè§†é¢‘å¤„ç†
            time.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
        except Exception as e:
            logger.error(f"å†…å­˜æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
            
        finally:
            memory_stats = self.memory_monitor.stop_monitoring()
            
        return {
            "test_type": "memory_usage_limits",
            "memory_stats": memory_stats,
            "data_processed": len(large_data),
            "test_passed": memory_stats["within_limit"]
        }

    def test_error_recovery_mechanisms(self) -> Dict[str, Any]:
        """æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶"""
        logger.info("å¼€å§‹é”™è¯¯æ¢å¤æœºåˆ¶æµ‹è¯•")
        
        recovery_results = {
            "invalid_file_handling": False,
            "memory_overflow_simulation": False,
            "network_timeout_simulation": False,
            "corrupted_data_handling": False,
            "graceful_degradation": False
        }
        
        try:
            from src.core.screenplay_engineer import ScreenplayEngineer
            engineer = ScreenplayEngineer()
            
            # æµ‹è¯•1: æ— æ•ˆæ–‡ä»¶å¤„ç†
            try:
                invalid_file = self.test_dir / "invalid.srt"
                with open(invalid_file, 'w', encoding='utf-8') as f:
                    f.write("è¿™ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„SRTæ–‡ä»¶\næ— æ•ˆå†…å®¹\n123abc")
                
                result = engineer.load_subtitles(str(invalid_file))
                recovery_results["invalid_file_handling"] = isinstance(result, list)
                logger.info("âœ… æ— æ•ˆæ–‡ä»¶å¤„ç†æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                recovery_results["invalid_file_handling"] = True  # å¼‚å¸¸è¢«æ­£ç¡®æ•è·
                logger.info("âœ… æ— æ•ˆæ–‡ä»¶å¼‚å¸¸è¢«æ­£ç¡®å¤„ç†")
            
            # æµ‹è¯•2: å†…å­˜æº¢å‡ºæ¨¡æ‹Ÿ
            try:
                # åˆ›å»ºè¶…å¤§æ•°æ®é›†
                huge_data = []
                for i in range(10000):
                    huge_data.append({
                        "text": "æµ‹è¯•å†…å®¹" * 1000,  # å¤§é‡é‡å¤å†…å®¹
                        "start_time": i,
                        "end_time": i + 1
                    })
                
                # å°è¯•å¤„ç†ï¼Œåº”è¯¥æœ‰å†…å­˜ä¿æŠ¤æœºåˆ¶
                result = engineer.analyze_plot_structure(huge_data[:100])  # åªå¤„ç†å‰100æ¡
                recovery_results["memory_overflow_simulation"] = True
                logger.info("âœ… å†…å­˜æº¢å‡ºä¿æŠ¤æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                recovery_results["memory_overflow_simulation"] = True
                logger.info("âœ… å†…å­˜æº¢å‡ºå¼‚å¸¸è¢«æ­£ç¡®å¤„ç†")
            
            # æµ‹è¯•3: æŸåæ•°æ®å¤„ç†
            try:
                corrupted_data = [
                    {"text": None, "start_time": "invalid", "end_time": -1},
                    {"missing_fields": True},
                    {"text": "", "start_time": float('inf'), "end_time": float('nan')}
                ]
                
                result = engineer.analyze_plot_structure(corrupted_data)
                recovery_results["corrupted_data_handling"] = isinstance(result, dict)
                logger.info("âœ… æŸåæ•°æ®å¤„ç†æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                recovery_results["corrupted_data_handling"] = True
                logger.info("âœ… æŸåæ•°æ®å¼‚å¸¸è¢«æ­£ç¡®å¤„ç†")
            
            # æµ‹è¯•4: ä¼˜é›…é™çº§
            try:
                # æ¨¡æ‹Ÿéƒ¨åˆ†åŠŸèƒ½ä¸å¯ç”¨çš„æƒ…å†µ
                minimal_data = [{"text": "ç®€å•æµ‹è¯•", "start_time": 0, "end_time": 3}]
                result = engineer.analyze_plot_structure(minimal_data)
                
                # å³ä½¿åŠŸèƒ½å—é™ï¼Œä¹Ÿåº”è¯¥è¿”å›åŸºæœ¬ç»“æœ
                recovery_results["graceful_degradation"] = isinstance(result, dict)
                logger.info("âœ… ä¼˜é›…é™çº§æµ‹è¯•é€šè¿‡")
                
            except Exception as e:
                recovery_results["graceful_degradation"] = False
                logger.warning(f"ä¼˜é›…é™çº§æµ‹è¯•å¤±è´¥: {e}")
            
        except ImportError:
            logger.warning("å‰§æœ¬å·¥ç¨‹å¸ˆæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
            recovery_results = {k: True for k in recovery_results.keys()}
        
        success_count = sum(recovery_results.values())
        overall_success = success_count >= 3  # è‡³å°‘3ä¸ªæµ‹è¯•é€šè¿‡
        
        return {
            "test_type": "error_recovery",
            "individual_results": recovery_results,
            "success_count": success_count,
            "total_tests": len(recovery_results),
            "overall_success": overall_success
        }

    def test_long_running_stability(self, duration_minutes: int = 2) -> Dict[str, Any]:
        """æµ‹è¯•é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§"""
        logger.info(f"å¼€å§‹é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯• ({duration_minutes}åˆ†é’Ÿ)")
        
        self.memory_monitor.start_monitoring()
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        stability_metrics = {
            "iterations_completed": 0,
            "errors_encountered": 0,
            "memory_leaks_detected": 0,
            "performance_degradation": False
        }
        
        initial_memory = psutil.Process().memory_info().rss / (1024 * 1024)
        
        try:
            from src.core.screenplay_engineer import ScreenplayEngineer
            engineer = ScreenplayEngineer()
            
            iteration = 0
            while time.time() < end_time:
                try:
                    iteration += 1
                    
                    # åˆ›å»ºæµ‹è¯•æ•°æ®
                    test_data = []
                    for i in range(50):  # æ¯æ¬¡å¤„ç†50æ¡å­—å¹•
                        test_data.append({
                            "text": f"æµ‹è¯•å­—å¹•{i}ï¼Œè¿­ä»£{iteration}",
                            "start_time": i * 2.0,
                            "end_time": (i + 1) * 2.0
                        })
                    
                    # æ‰§è¡Œå¤„ç†
                    analysis = engineer.analyze_plot_structure(test_data)
                    viral_srt = engineer.generate_viral_srt(test_data)
                    
                    # æ£€æŸ¥å†…å­˜æ³„æ¼
                    current_memory = psutil.Process().memory_info().rss / (1024 * 1024)
                    memory_growth = current_memory - initial_memory
                    
                    if memory_growth > 500:  # å¦‚æœå†…å­˜å¢é•¿è¶…è¿‡500MB
                        stability_metrics["memory_leaks_detected"] += 1
                        logger.warning(f"æ£€æµ‹åˆ°å†…å­˜å¢é•¿: {memory_growth:.1f}MB")
                    
                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                    
                    stability_metrics["iterations_completed"] = iteration
                    
                    # æ¯10æ¬¡è¿­ä»£æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                    if iteration % 10 == 0:
                        elapsed = time.time() - start_time
                        logger.info(f"ç¨³å®šæ€§æµ‹è¯•è¿›åº¦: {iteration}æ¬¡è¿­ä»£, {elapsed:.1f}ç§’, å†…å­˜: {current_memory:.1f}MB")
                    
                    time.sleep(0.1)  # çŸ­æš‚ä¼‘æ¯
                    
                except Exception as e:
                    stability_metrics["errors_encountered"] += 1
                    logger.warning(f"è¿­ä»£{iteration}å‡ºç°é”™è¯¯: {e}")
                    
                    if stability_metrics["errors_encountered"] > 10:
                        logger.error("é”™è¯¯æ¬¡æ•°è¿‡å¤šï¼Œåœæ­¢ç¨³å®šæ€§æµ‹è¯•")
                        break
                        
        except ImportError:
            logger.warning("å‰§æœ¬å·¥ç¨‹å¸ˆæ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç¨³å®šæ€§æµ‹è¯•")
            # æ¨¡æ‹Ÿç¨³å®šè¿è¡Œ
            while time.time() < end_time:
                stability_metrics["iterations_completed"] += 1
                time.sleep(1)
                
        finally:
            memory_stats = self.memory_monitor.stop_monitoring()
            
        total_duration = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_iterations_per_minute = stability_metrics["iterations_completed"] / (total_duration / 60)
        error_rate = stability_metrics["errors_encountered"] / max(stability_metrics["iterations_completed"], 1)
        
        return {
            "test_type": "long_running_stability",
            "duration_minutes": total_duration / 60,
            "stability_metrics": stability_metrics,
            "memory_stats": memory_stats,
            "performance_metrics": {
                "avg_iterations_per_minute": avg_iterations_per_minute,
                "error_rate": error_rate,
                "memory_stable": memory_stats["within_limit"]
            },
            "test_passed": (
                error_rate < 0.1 and  # é”™è¯¯ç‡å°äº10%
                memory_stats["within_limit"] and  # å†…å­˜åœ¨é™åˆ¶å†…
                stability_metrics["iterations_completed"] > 0  # è‡³å°‘å®Œæˆä¸€æ¬¡è¿­ä»£
            )
        }

    def test_checkpoint_resume_functionality(self) -> Dict[str, Any]:
        """æµ‹è¯•æ–­ç‚¹ç»­å‰ªåŠŸèƒ½"""
        logger.info("å¼€å§‹æ–­ç‚¹ç»­å‰ªåŠŸèƒ½æµ‹è¯•")
        
        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_subtitles = []
            for i in range(20):
                test_subtitles.append({
                    "index": i + 1,
                    "start_time": i * 3.0,
                    "end_time": (i + 1) * 3.0,
                    "text": f"æµ‹è¯•å­—å¹•{i + 1}"
                })
            
            # æ¨¡æ‹Ÿå¤„ç†ä¸­æ–­
            checkpoint_file = self.test_dir / "checkpoint.json"
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_data = {
                "processed_count": 10,
                "total_count": len(test_subtitles),
                "processed_subtitles": test_subtitles[:10],
                "timestamp": time.time()
            }
            
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            
            # æ¨¡æ‹Ÿä»æ£€æŸ¥ç‚¹æ¢å¤
            if checkpoint_file.exists():
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    loaded_checkpoint = json.load(f)
                
                resume_success = (
                    loaded_checkpoint["processed_count"] == 10 and
                    len(loaded_checkpoint["processed_subtitles"]) == 10
                )
            else:
                resume_success = False
            
            return {
                "test_type": "checkpoint_resume",
                "checkpoint_saved": checkpoint_file.exists(),
                "checkpoint_loaded": resume_success,
                "data_integrity": resume_success,
                "test_passed": resume_success
            }
            
        except Exception as e:
            logger.error(f"æ–­ç‚¹ç»­å‰ªæµ‹è¯•å¤±è´¥: {e}")
            return {
                "test_type": "checkpoint_resume",
                "test_passed": False,
                "error": str(e)
            }

    def run_all_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•"""
        logger.info("å¼€å§‹è¿è¡Œæ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•å¥—ä»¶")
        
        start_time = time.time()
        
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        self.test_results["memory_limits"] = self.test_memory_usage_limits()
        self.test_results["error_recovery"] = self.test_error_recovery_mechanisms()
        self.test_results["stability"] = self.test_long_running_stability(duration_minutes=1)  # 1åˆ†é’Ÿæµ‹è¯•
        self.test_results["checkpoint"] = self.test_checkpoint_resume_functionality()
        
        total_duration = time.time() - start_time
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        passed_tests = sum(1 for result in self.test_results.values() if result.get("test_passed", False))
        total_tests = len(self.test_results)
        success_rate = passed_tests / total_tests
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "test_suite": "VisionAI-ClipsMaster æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•",
            "timestamp": datetime.now().isoformat(),
            "total_duration": total_duration,
            "test_results": self.test_results,
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "success_rate": success_rate,
                "overall_passed": success_rate >= 0.75  # 75%é€šè¿‡ç‡
            },
            "recommendations": self._generate_performance_recommendations()
        }
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_report(report)
        
        return report

    def _generate_performance_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # å†…å­˜ç›¸å…³å»ºè®®
        memory_result = self.test_results.get("memory_limits", {})
        if not memory_result.get("test_passed", False):
            recommendations.append("å†…å­˜ä½¿ç”¨è¶…é™ï¼Œå»ºè®®å¯ç”¨æ›´æ¿€è¿›çš„é‡åŒ–ç­–ç•¥æˆ–å¢åŠ å†…å­˜æ¸…ç†é¢‘ç‡")
        
        # é”™è¯¯æ¢å¤å»ºè®®
        error_result = self.test_results.get("error_recovery", {})
        if not error_result.get("overall_success", False):
            recommendations.append("é”™è¯¯æ¢å¤æœºåˆ¶éœ€è¦åŠ å¼ºï¼Œå»ºè®®å¢åŠ æ›´å¤šå¼‚å¸¸å¤„ç†å’Œä¼˜é›…é™çº§é€»è¾‘")
        
        # ç¨³å®šæ€§å»ºè®®
        stability_result = self.test_results.get("stability", {})
        if not stability_result.get("test_passed", False):
            recommendations.append("é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æœ‰é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥å†…å­˜æ³„æ¼å’Œæ€§èƒ½ä¼˜åŒ–")
        
        # æ–­ç‚¹ç»­å‰ªå»ºè®®
        checkpoint_result = self.test_results.get("checkpoint", {})
        if not checkpoint_result.get("test_passed", False):
            recommendations.append("æ–­ç‚¹ç»­å‰ªåŠŸèƒ½éœ€è¦å®Œå–„ï¼Œå»ºè®®å®ç°æ›´å¯é çš„çŠ¶æ€ä¿å­˜å’Œæ¢å¤æœºåˆ¶")
        
        if not recommendations:
            recommendations.append("æ‰€æœ‰æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿè¡¨ç°è‰¯å¥½")
            
        return recommendations

    def _save_report(self, report: Dict):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        os.makedirs("test_output", exist_ok=True)
        
        json_path = f"test_output/performance_stability_test_report_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        logger.info(f"æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {json_path}")

    def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        try:
            if self.test_dir.exists():
                shutil.rmtree(self.test_dir)
                logger.info(f"æ¸…ç†æµ‹è¯•ç›®å½•: {self.test_dir}")
        except Exception as e:
            logger.error(f"æ¸…ç†å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ VisionAI-ClipsMaster æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•")
    print("=" * 50)
    
    tester = PerformanceStabilityTest()
    
    try:
        report = tester.run_all_performance_tests()
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   æ€»ä½“é€šè¿‡: {'âœ…' if report['summary']['overall_passed'] else 'âŒ'}")
        print(f"   æˆåŠŸç‡: {report['summary']['success_rate']:.1%}")
        print(f"   æ€»è€—æ—¶: {report['total_duration']:.2f}ç§’")
        
        print(f"\nğŸ” å„é¡¹æµ‹è¯•ç»“æœ:")
        for test_name, result in report['test_results'].items():
            status = "âœ…" if result.get('test_passed', False) else "âŒ"
            print(f"   {test_name}: {status}")
        
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"   {i}. {rec}")
            
        return 0 if report['summary']['overall_passed'] else 1
        
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    finally:
        tester.cleanup()

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
