#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster æ—¶é—´è½´ç²¾åº¦æœ€ç»ˆéªŒè¯æµ‹è¯•
éªŒè¯æ”¹è¿›åçš„alignment_engineer.pyæ˜¯å¦è¾¾åˆ°95%ç²¾åº¦ç›®æ ‡

æœ€ç»ˆéªŒè¯ç›®æ ‡ï¼š
1. å¹³å‡è¯¯å·® â‰¤ 0.2ç§’
2. ç²¾åº¦è¾¾æ ‡ç‡ â‰¥ 95%
3. æœ€å¤§è¯¯å·® â‰¤ 0.5ç§’
4. å¤„ç†å„ç§è¾¹ç•Œæƒ…å†µ
"""

import os
import sys
import json
import time
import logging
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# å¯¼å…¥ä¼˜åŒ–åçš„å¯¹é½å·¥ç¨‹å¸ˆ
from src.core.alignment_engineer import (
    PrecisionAlignmentEngineer, 
    AlignmentPrecision,
    align_subtitles_with_precision
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FinalTimecodeValidator:
    """æ—¶é—´è½´ç²¾åº¦æœ€ç»ˆéªŒè¯å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.validation_results = {
            "validation_start_time": datetime.now().isoformat(),
            "test_scenarios": {},
            "final_metrics": {},
            "validation_passed": False
        }
        
    def run_final_validation(self) -> Dict[str, Any]:
        """è¿è¡Œæœ€ç»ˆéªŒè¯"""
        logger.info("å¼€å§‹æ—¶é—´è½´ç²¾åº¦æœ€ç»ˆéªŒè¯")
        
        try:
            # åœºæ™¯1: æ ‡å‡†å¯¹é½æµ‹è¯•
            scenario1 = self._test_standard_alignment()
            self.validation_results["test_scenarios"]["standard_alignment"] = scenario1
            
            # åœºæ™¯2: è¾¹ç•Œæƒ…å†µæµ‹è¯•
            scenario2 = self._test_boundary_cases()
            self.validation_results["test_scenarios"]["boundary_cases"] = scenario2
            
            # åœºæ™¯3: é«˜ç²¾åº¦è¦æ±‚æµ‹è¯•
            scenario3 = self._test_high_precision_requirements()
            self.validation_results["test_scenarios"]["high_precision"] = scenario3
            
            # åœºæ™¯4: å®é™…æ•°æ®æ¨¡æ‹Ÿæµ‹è¯•
            scenario4 = self._test_realistic_data()
            self.validation_results["test_scenarios"]["realistic_data"] = scenario4
            
            # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
            final_metrics = self._calculate_final_metrics([scenario1, scenario2, scenario3, scenario4])
            self.validation_results["final_metrics"] = final_metrics
            
            # åˆ¤æ–­æ˜¯å¦é€šè¿‡éªŒè¯
            self.validation_results["validation_passed"] = self._evaluate_final_validation(final_metrics)
            
        except Exception as e:
            logger.error(f"æœ€ç»ˆéªŒè¯å¤±è´¥: {str(e)}")
            self.validation_results["error"] = str(e)
        
        self.validation_results["validation_end_time"] = datetime.now().isoformat()
        return self.validation_results
    
    def _test_standard_alignment(self) -> Dict[str, Any]:
        """æµ‹è¯•æ ‡å‡†å¯¹é½åœºæ™¯"""
        logger.info("æµ‹è¯•åœºæ™¯1: æ ‡å‡†å¯¹é½")
        
        # æ ‡å‡†æµ‹è¯•æ•°æ®ï¼šè½»å¾®æ—¶é—´åç§»
        original_subtitles = [
            {"start": "00:00:01,000", "end": "00:00:03,000", "text": "å¼€åœºä»‹ç»"},
            {"start": "00:00:05,000", "end": "00:00:07,000", "text": "ä¸»è¦å†…å®¹"},
            {"start": "00:00:10,000", "end": "00:00:12,000", "text": "å…³é”®ä¿¡æ¯"},
            {"start": "00:00:15,000", "end": "00:00:17,000", "text": "é‡è¦è½¬æŠ˜"},
            {"start": "00:00:20,000", "end": "00:00:22,000", "text": "ç²¾å½©é«˜æ½®"},
            {"start": "00:00:25,000", "end": "00:00:27,000", "text": "å®Œç¾ç»“å°¾"}
        ]
        
        reconstructed_subtitles = [
            {"start": "00:00:01,100", "end": "00:00:03,100", "text": "å¼€åœºä»‹ç»"},
            {"start": "00:00:05,150", "end": "00:00:07,150", "text": "ä¸»è¦å†…å®¹"},
            {"start": "00:00:10,200", "end": "00:00:12,200", "text": "å…³é”®ä¿¡æ¯"},
            {"start": "00:00:15,050", "end": "00:00:17,050", "text": "é‡è¦è½¬æŠ˜"},
            {"start": "00:00:20,180", "end": "00:00:22,180", "text": "ç²¾å½©é«˜æ½®"},
            {"start": "00:00:25,120", "end": "00:00:27,120", "text": "å®Œç¾ç»“å°¾"}
        ]
        
        return self._execute_alignment_test("æ ‡å‡†å¯¹é½", original_subtitles, reconstructed_subtitles, 30.0)
    
    def _test_boundary_cases(self) -> Dict[str, Any]:
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        logger.info("æµ‹è¯•åœºæ™¯2: è¾¹ç•Œæƒ…å†µ")
        
        # è¾¹ç•Œæµ‹è¯•æ•°æ®ï¼šåŒ…å«è§†é¢‘å¼€å¤´ã€ç»“å°¾ã€å¤§é—´éš™
        original_subtitles = [
            {"start": "00:00:00,500", "end": "00:00:02,000", "text": "è§†é¢‘å¼€å¤´"},
            {"start": "00:00:05,000", "end": "00:00:06,500", "text": "æ—©æœŸå†…å®¹"},
            {"start": "00:00:15,000", "end": "00:00:16,500", "text": "ä¸­æœŸå†…å®¹"},  # å¤§é—´éš™
            {"start": "00:00:28,000", "end": "00:00:29,500", "text": "åæœŸå†…å®¹"},  # å¤§é—´éš™
            {"start": "00:00:29,800", "end": "00:00:30,000", "text": "è§†é¢‘ç»“å°¾"}   # æ¥è¿‘ç»“å°¾
        ]
        
        reconstructed_subtitles = [
            {"start": "00:00:00,600", "end": "00:00:02,100", "text": "è§†é¢‘å¼€å¤´"},
            {"start": "00:00:05,200", "end": "00:00:06,700", "text": "æ—©æœŸå†…å®¹"},
            {"start": "00:00:15,300", "end": "00:00:16,800", "text": "ä¸­æœŸå†…å®¹"},
            {"start": "00:00:28,100", "end": "00:00:29,600", "text": "åæœŸå†…å®¹"},
            {"start": "00:00:29,900", "end": "00:00:30,100", "text": "è§†é¢‘ç»“å°¾"}
        ]
        
        return self._execute_alignment_test("è¾¹ç•Œæƒ…å†µ", original_subtitles, reconstructed_subtitles, 30.0)
    
    def _test_high_precision_requirements(self) -> Dict[str, Any]:
        """æµ‹è¯•é«˜ç²¾åº¦è¦æ±‚"""
        logger.info("æµ‹è¯•åœºæ™¯3: é«˜ç²¾åº¦è¦æ±‚")
        
        # é«˜ç²¾åº¦æµ‹è¯•æ•°æ®ï¼šéå¸¸å°çš„æ—¶é—´åç§»
        original_subtitles = [
            {"start": "00:00:01,000", "end": "00:00:02,500", "text": "ç²¾ç¡®æ—¶é—´ç‚¹1"},
            {"start": "00:00:03,000", "end": "00:00:04,500", "text": "ç²¾ç¡®æ—¶é—´ç‚¹2"},
            {"start": "00:00:05,000", "end": "00:00:06,500", "text": "ç²¾ç¡®æ—¶é—´ç‚¹3"},
            {"start": "00:00:07,000", "end": "00:00:08,500", "text": "ç²¾ç¡®æ—¶é—´ç‚¹4"},
            {"start": "00:00:09,000", "end": "00:00:10,500", "text": "ç²¾ç¡®æ—¶é—´ç‚¹5"}
        ]
        
        reconstructed_subtitles = [
            {"start": "00:00:01,050", "end": "00:00:02,550", "text": "ç²¾ç¡®æ—¶é—´ç‚¹1"},  # 0.05ç§’åç§»
            {"start": "00:00:03,080", "end": "00:00:04,580", "text": "ç²¾ç¡®æ—¶é—´ç‚¹2"},  # 0.08ç§’åç§»
            {"start": "00:00:05,120", "end": "00:00:06,620", "text": "ç²¾ç¡®æ—¶é—´ç‚¹3"},  # 0.12ç§’åç§»
            {"start": "00:00:07,030", "end": "00:00:08,530", "text": "ç²¾ç¡®æ—¶é—´ç‚¹4"},  # 0.03ç§’åç§»
            {"start": "00:00:09,090", "end": "00:00:10,590", "text": "ç²¾ç¡®æ—¶é—´ç‚¹5"}   # 0.09ç§’åç§»
        ]
        
        return self._execute_alignment_test("é«˜ç²¾åº¦è¦æ±‚", original_subtitles, reconstructed_subtitles, 12.0)
    
    def _test_realistic_data(self) -> Dict[str, Any]:
        """æµ‹è¯•çœŸå®æ•°æ®æ¨¡æ‹Ÿ"""
        logger.info("æµ‹è¯•åœºæ™¯4: çœŸå®æ•°æ®æ¨¡æ‹Ÿ")
        
        # çœŸå®æ•°æ®æ¨¡æ‹Ÿï¼šä¸è§„åˆ™é—´éš”å’Œåç§»
        original_subtitles = [
            {"start": "00:00:01,200", "end": "00:00:03,800", "text": "çœŸå®åœºæ™¯å¼€å§‹"},
            {"start": "00:00:04,500", "end": "00:00:06,200", "text": "å¯¹è¯å†…å®¹A"},
            {"start": "00:00:07,800", "end": "00:00:09,100", "text": "å¯¹è¯å†…å®¹B"},
            {"start": "00:00:12,300", "end": "00:00:14,700", "text": "æƒ…èŠ‚å‘å±•"},
            {"start": "00:00:16,100", "end": "00:00:18,900", "text": "å…³é”®è½¬æŠ˜"},
            {"start": "00:00:22,400", "end": "00:00:24,100", "text": "é«˜æ½®éƒ¨åˆ†"},
            {"start": "00:00:26,800", "end": "00:00:28,500", "text": "ç»“å±€æ”¶å°¾"}
        ]
        
        reconstructed_subtitles = [
            {"start": "00:00:01,350", "end": "00:00:03,950", "text": "çœŸå®åœºæ™¯å¼€å§‹"},  # 0.15ç§’åç§»
            {"start": "00:00:04,680", "end": "00:00:06,380", "text": "å¯¹è¯å†…å®¹A"},    # 0.18ç§’åç§»
            {"start": "00:00:12,450", "end": "00:00:14,850", "text": "æƒ…èŠ‚å‘å±•"},      # è·³è¿‡ä¸€ä¸ªï¼Œ0.15ç§’åç§»
            {"start": "00:00:16,280", "end": "00:00:19,080", "text": "å…³é”®è½¬æŠ˜"},      # 0.18ç§’åç§»
            {"start": "00:00:22,600", "end": "00:00:24,300", "text": "é«˜æ½®éƒ¨åˆ†"},      # 0.2ç§’åç§»
            {"start": "00:00:26,950", "end": "00:00:28,650", "text": "ç»“å±€æ”¶å°¾"}       # 0.15ç§’åç§»
        ]
        
        return self._execute_alignment_test("çœŸå®æ•°æ®æ¨¡æ‹Ÿ", original_subtitles, reconstructed_subtitles, 30.0)
    
    def _execute_alignment_test(self, test_name: str, original_subtitles: List[Dict], 
                               reconstructed_subtitles: List[Dict], video_duration: float) -> Dict[str, Any]:
        """æ‰§è¡Œå¯¹é½æµ‹è¯•"""
        try:
            start_time = time.time()
            
            # æ‰§è¡Œå¯¹é½
            alignment_result = align_subtitles_with_precision(
                original_subtitles, reconstructed_subtitles, video_duration, "high"
            )
            
            processing_time = time.time() - start_time
            
            # æå–è¯¯å·®æ•°æ®
            errors = [point.precision_error for point in alignment_result.alignment_points]
            
            if errors:
                avg_error = sum(errors) / len(errors)
                max_error = max(errors)
                min_error = min(errors)
                
                # è®¡ç®—å„ç²¾åº¦ç­‰çº§çš„è¾¾æ ‡ç‡
                ultra_high_count = sum(1 for e in errors if e <= 0.1)
                high_count = sum(1 for e in errors if e <= 0.2)
                standard_count = sum(1 for e in errors if e <= 0.5)
                
                ultra_high_rate = ultra_high_count / len(errors) * 100
                high_rate = high_count / len(errors) * 100
                standard_rate = standard_count / len(errors) * 100
            else:
                avg_error = max_error = min_error = 0.0
                ultra_high_rate = high_rate = standard_rate = 100.0
            
            return {
                "test_name": test_name,
                "success": True,
                "metrics": {
                    "average_error_seconds": round(avg_error, 4),
                    "max_error_seconds": round(max_error, 4),
                    "min_error_seconds": round(min_error, 4),
                    "ultra_high_precision_rate": round(ultra_high_rate, 1),  # â‰¤0.1ç§’
                    "high_precision_rate": round(high_rate, 1),              # â‰¤0.2ç§’
                    "standard_precision_rate": round(standard_rate, 1),      # â‰¤0.5ç§’
                    "total_alignment_points": len(errors),
                    "processing_time_seconds": round(processing_time, 4),
                    "engine_precision_rate": alignment_result.precision_rate,
                    "engine_quality_score": alignment_result.quality_score
                },
                "target_achievement": {
                    "average_error_target": avg_error <= 0.2,
                    "precision_rate_target": high_rate >= 95.0,
                    "max_error_target": max_error <= 0.5
                }
            }
            
        except Exception as e:
            logger.error(f"æµ‹è¯• {test_name} å¤±è´¥: {str(e)}")
            return {
                "test_name": test_name,
                "success": False,
                "error": str(e),
                "metrics": {},
                "target_achievement": {
                    "average_error_target": False,
                    "precision_rate_target": False,
                    "max_error_target": False
                }
            }
    
    def _calculate_final_metrics(self, scenarios: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        successful_scenarios = [s for s in scenarios if s.get("success", False)]
        
        if not successful_scenarios:
            return {
                "overall_success_rate": 0.0,
                "average_error_across_scenarios": 999.0,
                "max_error_across_scenarios": 999.0,
                "precision_rate_across_scenarios": 0.0,
                "targets_achieved": 0,
                "total_targets": 12  # 4ä¸ªåœºæ™¯ Ã— 3ä¸ªç›®æ ‡
            }
        
        # èšåˆæ‰€æœ‰åœºæ™¯çš„æŒ‡æ ‡
        all_avg_errors = [s["metrics"]["average_error_seconds"] for s in successful_scenarios]
        all_max_errors = [s["metrics"]["max_error_seconds"] for s in successful_scenarios]
        all_precision_rates = [s["metrics"]["high_precision_rate"] for s in successful_scenarios]
        
        # è®¡ç®—ç›®æ ‡è¾¾æˆæƒ…å†µ
        targets_achieved = 0
        total_targets = 0
        
        for scenario in scenarios:
            if scenario.get("success", False):
                targets = scenario.get("target_achievement", {})
                targets_achieved += sum(targets.values())
                total_targets += len(targets)
        
        return {
            "overall_success_rate": len(successful_scenarios) / len(scenarios) * 100,
            "average_error_across_scenarios": round(sum(all_avg_errors) / len(all_avg_errors), 4),
            "max_error_across_scenarios": round(max(all_max_errors), 4),
            "precision_rate_across_scenarios": round(sum(all_precision_rates) / len(all_precision_rates), 1),
            "targets_achieved": targets_achieved,
            "total_targets": total_targets,
            "target_achievement_rate": round(targets_achieved / total_targets * 100, 1) if total_targets > 0 else 0
        }
    
    def _evaluate_final_validation(self, final_metrics: Dict[str, Any]) -> bool:
        """è¯„ä¼°æœ€ç»ˆéªŒè¯æ˜¯å¦é€šè¿‡"""
        criteria = [
            final_metrics["overall_success_rate"] >= 100.0,           # æ‰€æœ‰åœºæ™¯æˆåŠŸ
            final_metrics["average_error_across_scenarios"] <= 0.2,   # å¹³å‡è¯¯å·®â‰¤0.2ç§’
            final_metrics["max_error_across_scenarios"] <= 0.5,       # æœ€å¤§è¯¯å·®â‰¤0.5ç§’
            final_metrics["precision_rate_across_scenarios"] >= 95.0, # ç²¾åº¦â‰¥95%
            final_metrics["target_achievement_rate"] >= 90.0          # ç›®æ ‡è¾¾æˆç‡â‰¥90%
        ]
        
        return all(criteria)
    
    def save_validation_results(self, filename: Optional[str] = None):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"final_timecode_precision_validation_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        return filename


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("VisionAI-ClipsMaster æ—¶é—´è½´ç²¾åº¦æœ€ç»ˆéªŒè¯")
    print("=" * 70)
    
    validator = FinalTimecodeValidator()
    
    try:
        # è¿è¡Œæœ€ç»ˆéªŒè¯
        results = validator.run_final_validation()
        
        # ä¿å­˜ç»“æœ
        report_path = validator.save_validation_results()
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "=" * 70)
        print("æœ€ç»ˆéªŒè¯ç»“æœ")
        print("=" * 70)
        
        validation_passed = results["validation_passed"]
        print(f"éªŒè¯çŠ¶æ€: {'âœ… é€šè¿‡' if validation_passed else 'âŒ æœªé€šè¿‡'}")
        
        if "final_metrics" in results:
            metrics = results["final_metrics"]
            print(f"æ€»ä½“æˆåŠŸç‡: {metrics.get('overall_success_rate', 0):.1f}%")
            print(f"å¹³å‡è¯¯å·®: {metrics.get('average_error_across_scenarios', 0):.4f}ç§’ (ç›®æ ‡: â‰¤0.2ç§’)")
            print(f"æœ€å¤§è¯¯å·®: {metrics.get('max_error_across_scenarios', 0):.4f}ç§’ (ç›®æ ‡: â‰¤0.5ç§’)")
            print(f"ç²¾åº¦è¾¾æ ‡ç‡: {metrics.get('precision_rate_across_scenarios', 0):.1f}% (ç›®æ ‡: â‰¥95%)")
            print(f"ç›®æ ‡è¾¾æˆç‡: {metrics.get('target_achievement_rate', 0):.1f}%")
        
        print(f"\nè¯¦ç»†æŠ¥å‘Š: {report_path}")
        
        if validation_passed:
            print("\nğŸ‰ æ—¶é—´è½´ç²¾åº¦æ”¹è¿›éªŒè¯æˆåŠŸï¼å¯ä»¥éƒ¨ç½²ä½¿ç”¨ã€‚")
        else:
            print("\nâš ï¸  æ—¶é—´è½´ç²¾åº¦éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
        
        return results
        
    except Exception as e:
        print(f"éªŒè¯æ‰§è¡Œå¤±è´¥: {str(e)}")
        return None


if __name__ == "__main__":
    main()
