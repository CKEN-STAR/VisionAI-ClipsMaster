#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster å¯é çš„æœ€ç»ˆéªŒè¯æµ‹è¯•
ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½éªŒè¯ï¼Œç¡®ä¿æ‰€æœ‰ä¼˜åŒ–åçš„åŠŸèƒ½å®Œå…¨å¯ç”¨
"""

import os
import sys
import json
import time
import psutil
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class ReliableFinalValidationTest:
    """å¯é çš„æœ€ç»ˆéªŒè¯æµ‹è¯•ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•"""
        self.test_start_time = time.time()
        self.test_results = {}
        self.logger = self._setup_logger()
        
        self.logger.info("ğŸ¯ å¼€å§‹VisionAI-ClipsMasterå¯é æœ€ç»ˆéªŒè¯æµ‹è¯•")
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def _log_memory_usage(self, stage: str) -> float:
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.logger.info(f"ğŸ’¾ å†…å­˜ä½¿ç”¨ [{stage}]: {memory_mb:.2f}MB")
            return memory_mb
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ— æ³•è·å–å†…å­˜ä¿¡æ¯: {e}")
            return 0.0
    
    def test_ui_components_import(self) -> Dict[str, Any]:
        """æµ‹è¯•UIç»„ä»¶å¯¼å…¥"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•1: UIç»„ä»¶å¯¼å…¥éªŒè¯")
        self.logger.info("=" * 60)
        
        test_result = {
            "test_name": "ui_components_import",
            "start_time": time.time(),
            "status": "running",
            "details": {}
        }
        
        memory_start = self._log_memory_usage("ui_import_start")
        
        try:
            # æµ‹è¯•UIæ¨¡å—å¯¼å…¥
            self.logger.info("ğŸ” æµ‹è¯•UIæ¨¡å—å¯¼å…¥...")
            import simple_ui_fixed
            test_result["details"]["ui_module_import"] = "success"
            self.logger.info("âœ… UIæ¨¡å—å¯¼å…¥æˆåŠŸ")
            
            # æµ‹è¯•æ ¸å¿ƒç»„ä»¶å¯¼å…¥
            self.logger.info("ğŸ” æµ‹è¯•æ ¸å¿ƒç»„ä»¶å¯¼å…¥...")
            components = [
                ("ClipGenerator", "src.core.clip_generator"),
                ("ModelTrainer", "src.training.trainer"),
                ("JianyingProExporter", "src.exporters.jianying_pro_exporter"),
                ("EnhancedTrainer", "src.training.enhanced_trainer"),
                ("GPUCPUManager", "src.core.gpu_cpu_manager"),
                ("PathManager", "src.core.path_manager")
            ]
            
            imported_components = 0
            for component_name, module_path in components:
                try:
                    module = __import__(module_path, fromlist=[component_name])
                    getattr(module, component_name)
                    imported_components += 1
                    self.logger.info(f"âœ… {component_name}: å¯¼å…¥æˆåŠŸ")
                except Exception as e:
                    self.logger.error(f"âŒ {component_name}: å¯¼å…¥å¤±è´¥ - {e}")
            
            success_rate = imported_components / len(components)
            test_result["details"]["components_imported"] = imported_components
            test_result["details"]["total_components"] = len(components)
            test_result["details"]["success_rate"] = success_rate
            
            if success_rate >= 0.9:
                test_result["status"] = "passed"
                self.logger.info(f"âœ… UIç»„ä»¶å¯¼å…¥æµ‹è¯•é€šè¿‡: {success_rate:.1%}")
            else:
                test_result["status"] = "failed"
                self.logger.error(f"âŒ UIç»„ä»¶å¯¼å…¥æµ‹è¯•å¤±è´¥: {success_rate:.1%}")
                
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            self.logger.error(f"âŒ UIç»„ä»¶å¯¼å…¥æµ‹è¯•å¼‚å¸¸: {e}")
        
        test_result["duration"] = time.time() - test_result["start_time"]
        memory_end = self._log_memory_usage("ui_import_end")
        test_result["details"]["memory_usage_mb"] = memory_end - memory_start
        
        return test_result
    
    def test_enhanced_systems(self) -> Dict[str, Any]:
        """æµ‹è¯•å¢å¼ºç³»ç»ŸåŠŸèƒ½"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•2: å¢å¼ºç³»ç»ŸåŠŸèƒ½éªŒè¯")
        self.logger.info("=" * 60)
        
        test_result = {
            "test_name": "enhanced_systems",
            "start_time": time.time(),
            "status": "running",
            "details": {},
            "sub_tests": {}
        }
        
        memory_start = self._log_memory_usage("enhanced_systems_start")
        
        try:
            # å­æµ‹è¯•1: å¢å¼ºè®­ç»ƒå™¨
            self.logger.info("ğŸ” å­æµ‹è¯•2.1: å¢å¼ºè®­ç»ƒå™¨")
            try:
                from src.training.enhanced_trainer import EnhancedTrainer
                trainer = EnhancedTrainer(use_gpu=False)
                
                # æµ‹è¯•æ•°æ®å‡†å¤‡åŠŸèƒ½
                test_data = [
                    {"original": "æ™®é€šå‰§æƒ…æè¿°", "viral": "éœ‡æ’¼å‰§æƒ…ï¼"},
                    {"original": "å¹³æ·¡å¯¹è¯å†…å®¹", "viral": "ç²¾å½©å¯¹è¯ï¼"}
                ] * 2
                
                train_inputs, train_outputs, val_inputs, val_outputs = trainer.prepare_training_data(test_data)
                
                test_result["sub_tests"]["enhanced_trainer"] = {
                    "status": "passed",
                    "device": str(trainer.device),
                    "batch_size": trainer.config["batch_size"]
                }
                self.logger.info(f"âœ… å¢å¼ºè®­ç»ƒå™¨æµ‹è¯•é€šè¿‡: è®¾å¤‡={trainer.device}")
                
            except Exception as e:
                test_result["sub_tests"]["enhanced_trainer"] = {"status": "failed", "error": str(e)}
                self.logger.error(f"âŒ å¢å¼ºè®­ç»ƒå™¨æµ‹è¯•å¤±è´¥: {e}")
            
            # å­æµ‹è¯•2: GPU/CPUç®¡ç†å™¨
            self.logger.info("ğŸ” å­æµ‹è¯•2.2: GPU/CPUç®¡ç†å™¨")
            try:
                from src.core.gpu_cpu_manager import GPUCPUManager
                manager = GPUCPUManager()
                
                system_report = manager.get_system_report()
                optimal_config = manager.get_optimal_config("training")
                
                test_result["sub_tests"]["gpu_cpu_manager"] = {
                    "status": "passed",
                    "recommended_device": system_report["recommended_device"],
                    "batch_size": optimal_config["batch_size"]
                }
                self.logger.info(f"âœ… GPU/CPUç®¡ç†å™¨æµ‹è¯•é€šè¿‡: è®¾å¤‡={system_report['recommended_device']}")
                
            except Exception as e:
                test_result["sub_tests"]["gpu_cpu_manager"] = {"status": "failed", "error": str(e)}
                self.logger.error(f"âŒ GPU/CPUç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
            
            # å­æµ‹è¯•3: è·¯å¾„ç®¡ç†å™¨
            self.logger.info("ğŸ” å­æµ‹è¯•2.3: è·¯å¾„ç®¡ç†å™¨")
            try:
                from src.core.path_manager import PathManager
                path_manager = PathManager()
                
                # æµ‹è¯•è·¯å¾„æ ‡å‡†åŒ–
                test_path = "data/input/test.mp4"
                normalized = path_manager.normalize_path(test_path)
                portable = path_manager.create_portable_path(normalized)
                
                test_result["sub_tests"]["path_manager"] = {
                    "status": "passed",
                    "platform": path_manager.platform,
                    "portable_path_created": portable is not None
                }
                self.logger.info(f"âœ… è·¯å¾„ç®¡ç†å™¨æµ‹è¯•é€šè¿‡: å¹³å°={path_manager.platform}")
                
            except Exception as e:
                test_result["sub_tests"]["path_manager"] = {"status": "failed", "error": str(e)}
                self.logger.error(f"âŒ è·¯å¾„ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
            
            # è¯„ä¼°æ•´ä½“ç»“æœ
            passed_subtests = sum(1 for result in test_result["sub_tests"].values() 
                                if result.get("status") == "passed")
            total_subtests = len(test_result["sub_tests"])
            success_rate = passed_subtests / total_subtests if total_subtests > 0 else 0
            
            test_result["details"]["passed_subtests"] = passed_subtests
            test_result["details"]["total_subtests"] = total_subtests
            test_result["details"]["success_rate"] = success_rate
            
            if success_rate >= 0.8:
                test_result["status"] = "passed"
                self.logger.info(f"âœ… å¢å¼ºç³»ç»ŸåŠŸèƒ½æµ‹è¯•é€šè¿‡: {success_rate:.1%}")
            else:
                test_result["status"] = "failed"
                self.logger.error(f"âŒ å¢å¼ºç³»ç»ŸåŠŸèƒ½æµ‹è¯•å¤±è´¥: {success_rate:.1%}")
                
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            self.logger.error(f"âŒ å¢å¼ºç³»ç»ŸåŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {e}")
        
        test_result["duration"] = time.time() - test_result["start_time"]
        memory_end = self._log_memory_usage("enhanced_systems_end")
        test_result["details"]["memory_usage_mb"] = memory_end - memory_start
        
        return test_result
    
    def test_core_workflow(self) -> Dict[str, Any]:
        """æµ‹è¯•æ ¸å¿ƒå·¥ä½œæµç¨‹"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•3: æ ¸å¿ƒå·¥ä½œæµç¨‹éªŒè¯")
        self.logger.info("=" * 60)
        
        test_result = {
            "test_name": "core_workflow",
            "start_time": time.time(),
            "status": "running",
            "details": {}
        }
        
        memory_start = self._log_memory_usage("core_workflow_start")
        
        try:
            # è¿è¡Œç®€åŒ–çš„ç«¯åˆ°ç«¯æµ‹è¯•
            self.logger.info("ğŸ” æ‰§è¡Œæ ¸å¿ƒå·¥ä½œæµç¨‹æµ‹è¯•...")
            
            import subprocess
            result = subprocess.run(
                [sys.executable, "complete_e2e_integration_test.py"],
                capture_output=True,
                text=True,
                timeout=90,
                encoding='utf-8',
                errors='ignore'
            )
            
            if result.returncode == 0:
                test_result["status"] = "passed"
                test_result["details"]["e2e_success"] = True
                self.logger.info("âœ… æ ¸å¿ƒå·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
            else:
                test_result["status"] = "partial"
                test_result["details"]["e2e_success"] = False
                self.logger.warning("âš ï¸ æ ¸å¿ƒå·¥ä½œæµç¨‹æµ‹è¯•éƒ¨åˆ†é€šè¿‡")
            
            test_result["details"]["return_code"] = result.returncode
            
        except subprocess.TimeoutExpired:
            test_result["status"] = "failed"
            test_result["error"] = "å·¥ä½œæµç¨‹æµ‹è¯•è¶…æ—¶"
            self.logger.error("âŒ æ ¸å¿ƒå·¥ä½œæµç¨‹æµ‹è¯•è¶…æ—¶")
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            self.logger.error(f"âŒ æ ¸å¿ƒå·¥ä½œæµç¨‹æµ‹è¯•å¼‚å¸¸: {e}")
        
        test_result["duration"] = time.time() - test_result["start_time"]
        memory_end = self._log_memory_usage("core_workflow_end")
        test_result["details"]["memory_usage_mb"] = memory_end - memory_start
        
        return test_result
    
    def test_system_stability(self) -> Dict[str, Any]:
        """æµ‹è¯•ç³»ç»Ÿç¨³å®šæ€§"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•4: ç³»ç»Ÿç¨³å®šæ€§éªŒè¯")
        self.logger.info("=" * 60)
        
        test_result = {
            "test_name": "system_stability",
            "start_time": time.time(),
            "status": "running",
            "details": {}
        }
        
        memory_start = self._log_memory_usage("stability_start")
        
        try:
            # å†…å­˜ä½¿ç”¨æ£€æŸ¥
            current_memory = memory_start
            memory_within_limit = current_memory < 3800  # 3.8GBé™åˆ¶
            
            # é”™è¯¯å¤„ç†æµ‹è¯•
            error_handling_ok = True
            try:
                from src.core.path_manager import PathManager
                path_manager = PathManager()
                # æµ‹è¯•å¤„ç†ä¸å­˜åœ¨çš„æ–‡ä»¶
                result = path_manager.resolve_file_path("nonexistent_file.mp4")
                # åº”è¯¥è¿”å›Noneè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            except Exception:
                error_handling_ok = False
            
            # è·¨å¹³å°å…¼å®¹æ€§
            import platform
            platform_info = {
                "system": platform.system(),
                "architecture": platform.architecture()[0],
                "python_version": platform.python_version()
            }
            
            test_result["details"]["memory_within_limit"] = memory_within_limit
            test_result["details"]["current_memory_mb"] = current_memory
            test_result["details"]["error_handling_ok"] = error_handling_ok
            test_result["details"]["platform_info"] = platform_info
            
            # è¯„ä¼°ç¨³å®šæ€§
            stability_checks = [memory_within_limit, error_handling_ok]
            stability_score = sum(stability_checks) / len(stability_checks)
            
            if stability_score >= 0.8:
                test_result["status"] = "passed"
                self.logger.info(f"âœ… ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•é€šè¿‡: {stability_score:.1%}")
            else:
                test_result["status"] = "failed"
                self.logger.error(f"âŒ ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•å¤±è´¥: {stability_score:.1%}")
            
            test_result["details"]["stability_score"] = stability_score
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            self.logger.error(f"âŒ ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•å¼‚å¸¸: {e}")
        
        test_result["duration"] = time.time() - test_result["start_time"]
        memory_end = self._log_memory_usage("stability_end")
        test_result["details"]["memory_usage_mb"] = memory_end - memory_start

        return test_result

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•")

        # æµ‹è¯•æ–¹æ³•åˆ—è¡¨
        test_methods = [
            ("UIç»„ä»¶å¯¼å…¥", self.test_ui_components_import),
            ("å¢å¼ºç³»ç»ŸåŠŸèƒ½", self.test_enhanced_systems),
            ("æ ¸å¿ƒå·¥ä½œæµç¨‹", self.test_core_workflow),
            ("ç³»ç»Ÿç¨³å®šæ€§", self.test_system_stability)
        ]

        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        for test_name, test_method in test_methods:
            self.logger.info(f"\nğŸ¯ å¼€å§‹æ‰§è¡Œ: {test_name}")
            try:
                result = test_method()
                self.test_results[result["test_name"]] = result
            except Exception as e:
                self.logger.error(f"âŒ æµ‹è¯•æ–¹æ³• {test_name} æ‰§è¡Œå¤±è´¥: {e}")
                self.test_results[f"failed_{test_name}"] = {
                    "test_name": test_name,
                    "status": "failed",
                    "error": str(e),
                    "duration": 0
                }

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        return self.generate_final_report()

    def generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        total_duration = time.time() - self.test_start_time

        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results.values() if r.get("status") == "passed")
        partial_tests = sum(1 for r in self.test_results.values() if r.get("status") == "partial")
        failed_tests = sum(1 for r in self.test_results.values() if r.get("status") == "failed")

        success_rate = (passed_tests + partial_tests * 0.5) / total_tests if total_tests > 0 else 0

        # å†…å­˜ä½¿ç”¨ç»Ÿè®¡
        memory_usages = []
        for result in self.test_results.values():
            if "details" in result and "memory_usage_mb" in result["details"]:
                memory_usages.append(result["details"]["memory_usage_mb"])

        max_memory_usage = max(memory_usages) if memory_usages else 0

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "test_summary": {
                "test_start_time": datetime.fromtimestamp(self.test_start_time).isoformat(),
                "test_end_time": datetime.now().isoformat(),
                "total_duration": total_duration,
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "partial_tests": partial_tests,
                "failed_tests": failed_tests,
                "success_rate": success_rate,
                "overall_status": self._determine_overall_status(success_rate)
            },
            "performance_metrics": {
                "max_memory_usage_mb": max_memory_usage,
                "memory_within_limit": max_memory_usage < 100,  # åˆç†çš„å†…å­˜ä½¿ç”¨
                "total_duration": total_duration
            },
            "test_results": self.test_results,
            "validation_criteria_met": self._check_validation_criteria()
        }

        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"reliable_final_validation_report_{timestamp}.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # æ‰“å°æ‘˜è¦æŠ¥å‘Š
        self._print_summary_report(report, report_file)

        return report

    def _determine_overall_status(self, success_rate: float) -> str:
        """ç¡®å®šæ•´ä½“çŠ¶æ€"""
        if success_rate >= 0.95:
            return "ä¼˜ç§€"
        elif success_rate >= 0.85:
            return "è‰¯å¥½"
        elif success_rate >= 0.70:
            return "å¯æ¥å—"
        else:
            return "éœ€è¦æ”¹è¿›"

    def _check_validation_criteria(self) -> Dict[str, bool]:
        """æ£€æŸ¥éªŒè¯æ ‡å‡†"""
        criteria = {}

        # UIå¯åŠ¨æˆåŠŸç‡
        ui_test = self.test_results.get("ui_components_import", {})
        criteria["ui_startup_success"] = ui_test.get("status") == "passed"

        # åŠŸèƒ½æ¨¡å—é€šè¿‡ç‡
        enhanced_test = self.test_results.get("enhanced_systems", {})
        enhanced_rate = enhanced_test.get("details", {}).get("success_rate", 0)
        criteria["functional_modules_pass"] = enhanced_rate >= 0.95

        # ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æˆåŠŸç‡
        workflow_test = self.test_results.get("core_workflow", {})
        criteria["e2e_workflow_success"] = workflow_test.get("status") in ["passed", "partial"]

        # ç³»ç»Ÿç¨³å®šæ€§
        stability_test = self.test_results.get("system_stability", {})
        criteria["system_stability"] = stability_test.get("status") == "passed"

        # å†…å­˜ä½¿ç”¨
        memory_ok = True
        for result in self.test_results.values():
            if "details" in result and "memory_usage_mb" in result["details"]:
                if result["details"]["memory_usage_mb"] > 1000:  # 1GBé™åˆ¶
                    memory_ok = False
                    break
        criteria["memory_usage_ok"] = memory_ok

        return criteria

    def _print_summary_report(self, report: Dict[str, Any], report_file: str):
        """æ‰“å°æ‘˜è¦æŠ¥å‘Š"""
        summary = report["test_summary"]
        performance = report["performance_metrics"]
        criteria = report["validation_criteria_met"]

        self.logger.info("=" * 80)
        self.logger.info("ğŸ‰ VisionAI-ClipsMaster å¯é æœ€ç»ˆéªŒè¯æµ‹è¯•å®Œæˆ")
        self.logger.info("=" * 80)

        # åŸºæœ¬ç»Ÿè®¡
        self.logger.info(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        self.logger.info(f"   æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        self.logger.info(f"   âœ… é€šè¿‡: {summary['passed_tests']}")
        self.logger.info(f"   âš ï¸ éƒ¨åˆ†é€šè¿‡: {summary['partial_tests']}")
        self.logger.info(f"   âŒ å¤±è´¥: {summary['failed_tests']}")
        self.logger.info(f"   ğŸ¯ æˆåŠŸç‡: {summary['success_rate']:.1%}")
        self.logger.info(f"   ğŸ“ˆ æ•´ä½“çŠ¶æ€: {summary['overall_status']}")

        # æ€§èƒ½æŒ‡æ ‡
        self.logger.info(f"\nğŸ’¾ æ€§èƒ½æŒ‡æ ‡:")
        self.logger.info(f"   æœ€å¤§å†…å­˜å¢é‡: {performance['max_memory_usage_mb']:.2f}MB")
        self.logger.info(f"   å†…å­˜ä½¿ç”¨: {'âœ… æ­£å¸¸' if performance['memory_within_limit'] else 'âŒ è¶…é™'}")
        self.logger.info(f"   æ€»è€—æ—¶: {performance['total_duration']:.2f}ç§’")

        # è¯¦ç»†ç»“æœ
        self.logger.info(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, result in self.test_results.items():
            status_icon = "âœ…" if result["status"] == "passed" else "âš ï¸" if result["status"] == "partial" else "âŒ"
            duration = result.get("duration", 0)
            self.logger.info(f"   {status_icon} {test_name}: {result['status']} ({duration:.2f}s)")

        # éªŒè¯æ ‡å‡†
        self.logger.info(f"\nğŸ¯ éªŒè¯æ ‡å‡†è¾¾æˆæƒ…å†µ:")
        criteria_names = {
            "ui_startup_success": "UIå¯åŠ¨æˆåŠŸç‡",
            "functional_modules_pass": "åŠŸèƒ½æ¨¡å—é€šè¿‡ç‡",
            "e2e_workflow_success": "ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æˆåŠŸç‡",
            "system_stability": "ç³»ç»Ÿç¨³å®šæ€§",
            "memory_usage_ok": "å†…å­˜ä½¿ç”¨æ§åˆ¶"
        }

        for criterion, met in criteria.items():
            status_icon = "âœ…" if met else "âŒ"
            criterion_name = criteria_names.get(criterion, criterion)
            self.logger.info(f"   {status_icon} {criterion_name}: {'è¾¾æˆ' if met else 'æœªè¾¾æˆ'}")

        self.logger.info(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šæ–‡ä»¶: {report_file}")

        # æœ€ç»ˆç»“è®º
        if summary["success_rate"] >= 0.95:
            self.logger.info("ğŸ‰ ç³»ç»ŸéªŒè¯å®Œå…¨æˆåŠŸï¼æ‰€æœ‰åŠŸèƒ½å®Œå…¨å¯ç”¨ï¼")
        elif summary["success_rate"] >= 0.85:
            self.logger.info("âœ… ç³»ç»ŸéªŒè¯åŸºæœ¬æˆåŠŸï¼æ ¸å¿ƒåŠŸèƒ½å¯ç”¨ï¼")
        else:
            self.logger.warning("âš ï¸ ç³»ç»ŸéªŒè¯å‘ç°é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ï¼")

    def cleanup_test_environment(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        self.logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")

        try:
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()

            self.logger.info("âœ… æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")

        except Exception as e:
            self.logger.warning(f"âš ï¸ æ¸…ç†æµ‹è¯•ç¯å¢ƒæ—¶å‡ºç°è­¦å‘Š: {e}")

def main():
    """ä¸»å‡½æ•°"""
    validator = ReliableFinalValidationTest()

    try:
        # è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•
        report = validator.run_all_tests()

        # æ ¹æ®æˆåŠŸç‡è¿”å›çŠ¶æ€ç 
        success_rate = report["test_summary"]["success_rate"]
        if success_rate >= 0.95:
            return 0  # å®Œå…¨æˆåŠŸ
        elif success_rate >= 0.85:
            return 0  # åŸºæœ¬æˆåŠŸ
        else:
            return 1  # éœ€è¦æ”¹è¿›

    except Exception as e:
        validator.logger.error(f"âŒ éªŒè¯æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    finally:
        # æ¸…ç†ç¯å¢ƒ
        validator.cleanup_test_environment()

if __name__ == "__main__":
    sys.exit(main())
