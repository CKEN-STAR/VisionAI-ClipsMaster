#!/usr/bin/env python3
"""
æµ‹è¯•ç»“æœéªŒè¯å™¨
æ ¹æ®é¢„æœŸæ ‡å‡†éªŒè¯æµ‹è¯•ç»“æœï¼Œç”Ÿæˆè¯¦ç»†çš„éªŒè¯æŠ¥å‘Š
"""

import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Tuple
from datetime import datetime

class TestResultValidator:
    """æµ‹è¯•ç»“æœéªŒè¯å™¨"""
    
    def __init__(self, config_path: str = None):
        self.config_path = config_path or Path(__file__).parent / "test_config.yaml"
        self.config = self._load_config()
        self.validation_results = {
            "validation_timestamp": datetime.now().isoformat(),
            "overall_status": "unknown",
            "module_validations": {},
            "critical_failures": [],
            "warnings": [],
            "recommendations": [],
            "compliance_score": 0.0
        }
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {self.config_path}: {e}")
            return {}
    
    def validate_test_results(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯æµ‹è¯•ç»“æœ"""
        print("ğŸ” å¼€å§‹éªŒè¯æµ‹è¯•ç»“æœ...")
        
        # 1. éªŒè¯è§†é¢‘-å­—å¹•æ˜ å°„ç²¾åº¦
        if "alignment_precision" in test_results.get("test_modules", {}):
            self._validate_alignment_precision(test_results["test_modules"]["alignment_precision"])
        
        # 2. éªŒè¯AIå‰§æœ¬é‡æ„åŠŸèƒ½
        if "viral_srt_generation" in test_results.get("test_modules", {}):
            self._validate_viral_srt_generation(test_results["test_modules"]["viral_srt_generation"])
        
        # 3. éªŒè¯ç³»ç»Ÿé›†æˆ
        if "system_integration" in test_results.get("test_modules", {}):
            self._validate_system_integration(test_results["test_modules"]["system_integration"])
        
        # 4. éªŒè¯æ€§èƒ½æŒ‡æ ‡
        if "performance_metrics" in test_results:
            self._validate_performance_metrics(test_results["performance_metrics"])
        
        # 5. éªŒè¯å†…å­˜å‹åŠ›æµ‹è¯•
        if "memory_stress_test" in test_results:
            self._validate_memory_stress_test(test_results["memory_stress_test"])
        
        # 6. è®¡ç®—æ€»ä½“åˆè§„æ€§è¯„åˆ†
        self._calculate_compliance_score()
        
        # 7. ç”Ÿæˆæœ€ç»ˆçŠ¶æ€å’Œå»ºè®®
        self._generate_final_assessment()
        
        print(f"âœ… æµ‹è¯•ç»“æœéªŒè¯å®Œæˆ - åˆè§„æ€§è¯„åˆ†: {self.validation_results['compliance_score']:.1%}")
        
        return self.validation_results
    
    def _validate_alignment_precision(self, alignment_results: Dict[str, Any]):
        """éªŒè¯è§†é¢‘-å­—å¹•æ˜ å°„ç²¾åº¦"""
        module_name = "alignment_precision"
        config = self.config.get("alignment_precision_test", {})
        requirements = config.get("precision_requirements", {})
        
        validation = {
            "module_name": module_name,
            "status": "passed",
            "checks": [],
            "score": 0.0,
            "issues": []
        }
        
        # æ£€æŸ¥ç²¾åº¦æŒ‡æ ‡
        precision_metrics = alignment_results.get("precision_metrics", {})
        
        # 1. æ£€æŸ¥æˆåŠŸç‡
        success_rate = precision_metrics.get("success_rate", 0)
        min_accuracy = requirements.get("min_accuracy_rate", 0.95)
        
        check_result = {
            "check_name": "success_rate",
            "actual_value": success_rate,
            "expected_value": min_accuracy,
            "passed": success_rate >= min_accuracy,
            "weight": 0.4
        }
        validation["checks"].append(check_result)
        
        if not check_result["passed"]:
            validation["issues"].append(f"æˆåŠŸç‡ä¸è¾¾æ ‡: {success_rate:.1%} < {min_accuracy:.1%}")
        
        # 2. æ£€æŸ¥å¹³å‡ç²¾åº¦è¯¯å·®
        avg_error = precision_metrics.get("average_precision_error", 1.0)
        max_error = requirements.get("max_alignment_error_seconds", 0.5)
        
        check_result = {
            "check_name": "average_precision_error",
            "actual_value": avg_error,
            "expected_value": max_error,
            "passed": avg_error <= max_error,
            "weight": 0.4
        }
        validation["checks"].append(check_result)
        
        if not check_result["passed"]:
            validation["issues"].append(f"å¹³å‡ç²¾åº¦è¯¯å·®è¿‡å¤§: {avg_error:.3f}s > {max_error:.3f}s")
        
        # 3. æ£€æŸ¥æœ€å¤§ç²¾åº¦è¯¯å·®
        max_precision_error = precision_metrics.get("max_precision_error", 1.0)
        
        check_result = {
            "check_name": "max_precision_error",
            "actual_value": max_precision_error,
            "expected_value": max_error,
            "passed": max_precision_error <= max_error,
            "weight": 0.2
        }
        validation["checks"].append(check_result)
        
        if not check_result["passed"]:
            validation["issues"].append(f"æœ€å¤§ç²¾åº¦è¯¯å·®è¿‡å¤§: {max_precision_error:.3f}s > {max_error:.3f}s")
        
        # è®¡ç®—æ¨¡å—è¯„åˆ†
        total_weight = sum(check["weight"] for check in validation["checks"])
        weighted_score = sum(check["weight"] if check["passed"] else 0 for check in validation["checks"])
        validation["score"] = weighted_score / total_weight if total_weight > 0 else 0
        
        # ç¡®å®šæ¨¡å—çŠ¶æ€
        if validation["score"] >= 0.8:
            validation["status"] = "passed"
        elif validation["score"] >= 0.6:
            validation["status"] = "warning"
        else:
            validation["status"] = "failed"
            self.validation_results["critical_failures"].append(f"{module_name}: ç²¾åº¦éªŒè¯å¤±è´¥")
        
        self.validation_results["module_validations"][module_name] = validation
    
    def _validate_viral_srt_generation(self, viral_results: Dict[str, Any]):
        """éªŒè¯AIå‰§æœ¬é‡æ„åŠŸèƒ½"""
        module_name = "viral_srt_generation"
        config = self.config.get("viral_srt_generation_test", {})
        quality_requirements = config.get("quality_requirements", {})
        
        validation = {
            "module_name": module_name,
            "status": "passed",
            "checks": [],
            "score": 0.0,
            "issues": []
        }
        
        generation_metrics = viral_results.get("generation_metrics", {})
        
        # 1. æ£€æŸ¥ç”ŸæˆæˆåŠŸç‡
        success_rate = generation_metrics.get("success_rate", 0)
        min_success_rate = quality_requirements.get("min_generation_success_rate", 0.9)
        
        check_result = {
            "check_name": "generation_success_rate",
            "actual_value": success_rate,
            "expected_value": min_success_rate,
            "passed": success_rate >= min_success_rate,
            "weight": 0.3
        }
        validation["checks"].append(check_result)
        
        # 2. æ£€æŸ¥çˆ†æ¬¾ç‰¹å¾è¦†ç›–ç‡
        viral_coverage = generation_metrics.get("viral_feature_coverage", 0)
        min_viral_score = quality_requirements.get("min_viral_feature_score", 0.75)
        
        check_result = {
            "check_name": "viral_feature_coverage",
            "actual_value": viral_coverage,
            "expected_value": min_viral_score,
            "passed": viral_coverage >= min_viral_score,
            "weight": 0.3
        }
        validation["checks"].append(check_result)
        
        # 3. æ£€æŸ¥æ•´ä½“è´¨é‡è¯„åˆ†
        overall_quality = generation_metrics.get("overall_quality_score", 0)
        min_quality = quality_requirements.get("min_text_quality_score", 0.8)
        
        check_result = {
            "check_name": "overall_quality_score",
            "actual_value": overall_quality,
            "expected_value": min_quality,
            "passed": overall_quality >= min_quality,
            "weight": 0.2
        }
        validation["checks"].append(check_result)
        
        # 4. æ£€æŸ¥æ¨¡å‹åˆ‡æ¢æ•ˆç‡
        model_switching = generation_metrics.get("model_switching_efficiency", 0)
        min_switching_efficiency = 0.9
        
        check_result = {
            "check_name": "model_switching_efficiency",
            "actual_value": model_switching,
            "expected_value": min_switching_efficiency,
            "passed": model_switching >= min_switching_efficiency,
            "weight": 0.2
        }
        validation["checks"].append(check_result)
        
        # è®¡ç®—è¯„åˆ†å’ŒçŠ¶æ€
        self._calculate_module_score_and_status(validation, module_name)
        self.validation_results["module_validations"][module_name] = validation
    
    def _validate_system_integration(self, integration_results: Dict[str, Any]):
        """éªŒè¯ç³»ç»Ÿé›†æˆ"""
        module_name = "system_integration"
        config = self.config.get("system_integration_test", {})
        workflow_performance = config.get("workflow_performance", {})
        
        validation = {
            "module_name": module_name,
            "status": "passed",
            "checks": [],
            "score": 0.0,
            "issues": []
        }
        
        workflow_metrics = integration_results.get("workflow_metrics", {})
        
        # 1. æ£€æŸ¥å·¥ä½œæµæˆåŠŸç‡
        success_rate = workflow_metrics.get("success_rate", 0)
        min_workflow_success = workflow_performance.get("min_workflow_success_rate", 0.9)
        
        check_result = {
            "check_name": "workflow_success_rate",
            "actual_value": success_rate,
            "expected_value": min_workflow_success,
            "passed": success_rate >= min_workflow_success,
            "weight": 0.3
        }
        validation["checks"].append(check_result)
        
        # 2. æ£€æŸ¥å¯¼å‡ºå…¼å®¹æ€§
        export_compatibility = workflow_metrics.get("export_compatibility", 0)
        min_export_compatibility = 0.94
        
        check_result = {
            "check_name": "export_compatibility",
            "actual_value": export_compatibility,
            "expected_value": min_export_compatibility,
            "passed": export_compatibility >= min_export_compatibility,
            "weight": 0.25
        }
        validation["checks"].append(check_result)
        
        # 3. æ£€æŸ¥æ¢å¤å¯é æ€§
        recovery_reliability = workflow_metrics.get("recovery_reliability", 0)
        min_recovery_reliability = 0.9
        
        check_result = {
            "check_name": "recovery_reliability",
            "actual_value": recovery_reliability,
            "expected_value": min_recovery_reliability,
            "passed": recovery_reliability >= min_recovery_reliability,
            "weight": 0.2
        }
        validation["checks"].append(check_result)
        
        # 4. æ£€æŸ¥æ€§èƒ½æ•ˆç‡
        performance_efficiency = workflow_metrics.get("performance_efficiency", 0)
        min_performance_efficiency = 0.85
        
        check_result = {
            "check_name": "performance_efficiency",
            "actual_value": performance_efficiency,
            "expected_value": min_performance_efficiency,
            "passed": performance_efficiency >= min_performance_efficiency,
            "weight": 0.25
        }
        validation["checks"].append(check_result)
        
        # è®¡ç®—è¯„åˆ†å’ŒçŠ¶æ€
        self._calculate_module_score_and_status(validation, module_name)
        self.validation_results["module_validations"][module_name] = validation
    
    def _validate_performance_metrics(self, performance_metrics: Dict[str, Any]):
        """éªŒè¯æ€§èƒ½æŒ‡æ ‡"""
        # æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡
        test_case_success_rate = performance_metrics.get("test_case_success_rate", 0)
        if test_case_success_rate < 0.9:
            self.validation_results["warnings"].append(
                f"æµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡åä½: {test_case_success_rate:.1%}"
            )
        
        # æ£€æŸ¥æ¨¡å—æˆåŠŸç‡
        module_success_rate = performance_metrics.get("module_success_rate", 0)
        if module_success_rate < 1.0:
            self.validation_results["warnings"].append(
                f"æ¨¡å—æˆåŠŸç‡ä¸å®Œç¾: {module_success_rate:.1%}"
            )
    
    def _validate_memory_stress_test(self, memory_test: Dict[str, Any]):
        """éªŒè¯å†…å­˜å‹åŠ›æµ‹è¯•"""
        if not memory_test.get("test_passed", False):
            self.validation_results["critical_failures"].append(
                f"å†…å­˜å‹åŠ›æµ‹è¯•å¤±è´¥: å³°å€¼å†…å­˜ {memory_test.get('peak_memory_usage_gb', 0):.2f}GB"
            )
        
        if not memory_test.get("memory_stable", True):
            self.validation_results["warnings"].append("å†…å­˜ä½¿ç”¨ä¸å¤Ÿç¨³å®š")
    
    def _calculate_module_score_and_status(self, validation: Dict[str, Any], module_name: str):
        """è®¡ç®—æ¨¡å—è¯„åˆ†å’ŒçŠ¶æ€"""
        total_weight = sum(check["weight"] for check in validation["checks"])
        weighted_score = sum(check["weight"] if check["passed"] else 0 for check in validation["checks"])
        validation["score"] = weighted_score / total_weight if total_weight > 0 else 0
        
        # æ”¶é›†å¤±è´¥çš„æ£€æŸ¥
        failed_checks = [check for check in validation["checks"] if not check["passed"]]
        for check in failed_checks:
            validation["issues"].append(
                f"{check['check_name']}: {check['actual_value']} (æœŸæœ›: {check['expected_value']})"
            )
        
        # ç¡®å®šçŠ¶æ€
        if validation["score"] >= 0.8:
            validation["status"] = "passed"
        elif validation["score"] >= 0.6:
            validation["status"] = "warning"
            self.validation_results["warnings"].append(f"{module_name}: éƒ¨åˆ†æŒ‡æ ‡ä¸è¾¾æ ‡")
        else:
            validation["status"] = "failed"
            self.validation_results["critical_failures"].append(f"{module_name}: éªŒè¯å¤±è´¥")
    
    def _calculate_compliance_score(self):
        """è®¡ç®—æ€»ä½“åˆè§„æ€§è¯„åˆ†"""
        if not self.validation_results["module_validations"]:
            self.validation_results["compliance_score"] = 0.0
            return
        
        # ä½¿ç”¨é…ç½®ä¸­çš„æƒé‡
        quality_weights = self.config.get("reporting", {}).get("quality_assessment", {}).get("overall_quality_weights", {})
        default_weight = 1.0 / len(self.validation_results["module_validations"])
        
        total_weighted_score = 0.0
        total_weight = 0.0
        
        for module_name, validation in self.validation_results["module_validations"].items():
            weight = quality_weights.get(module_name, default_weight)
            total_weighted_score += validation["score"] * weight
            total_weight += weight
        
        self.validation_results["compliance_score"] = total_weighted_score / total_weight if total_weight > 0 else 0.0
    
    def _generate_final_assessment(self):
        """ç”Ÿæˆæœ€ç»ˆè¯„ä¼°"""
        compliance_score = self.validation_results["compliance_score"]
        critical_failures = len(self.validation_results["critical_failures"])
        
        # ç¡®å®šæ€»ä½“çŠ¶æ€
        if critical_failures > 0:
            self.validation_results["overall_status"] = "failed"
        elif compliance_score >= 0.8:
            self.validation_results["overall_status"] = "passed"
        elif compliance_score >= 0.6:
            self.validation_results["overall_status"] = "warning"
        else:
            self.validation_results["overall_status"] = "failed"
        
        # ç”Ÿæˆå»ºè®®
        if compliance_score < 0.6:
            self.validation_results["recommendations"].append("ç³»ç»Ÿéœ€è¦é‡å¤§æ”¹è¿›æ‰èƒ½è¾¾åˆ°ç”Ÿäº§å°±ç»ªçŠ¶æ€")
        elif compliance_score < 0.8:
            self.validation_results["recommendations"].append("ç³»ç»ŸåŸºæœ¬å¯ç”¨ï¼Œä½†å»ºè®®ä¼˜åŒ–å…³é”®æŒ‡æ ‡")
        else:
            self.validation_results["recommendations"].append("ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ")
        
        # é’ˆå¯¹å…·ä½“é—®é¢˜çš„å»ºè®®
        for module_name, validation in self.validation_results["module_validations"].items():
            if validation["status"] == "failed":
                self.validation_results["recommendations"].append(f"ä¼˜å…ˆä¿®å¤ {module_name} æ¨¡å—çš„å…³é”®é—®é¢˜")
            elif validation["status"] == "warning":
                self.validation_results["recommendations"].append(f"æ”¹è¿› {module_name} æ¨¡å—çš„æ€§èƒ½æŒ‡æ ‡")
    
    def save_validation_report(self, output_path: str):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

def main():
    """ä¸»å‡½æ•° - ç”¨äºç‹¬ç«‹è¿è¡ŒéªŒè¯å™¨"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯•ç»“æœéªŒè¯å™¨")
    parser.add_argument("test_results_file", help="æµ‹è¯•ç»“æœJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", help="æµ‹è¯•é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="éªŒè¯æŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    
    args = parser.parse_args()
    
    # åŠ è½½æµ‹è¯•ç»“æœ
    with open(args.test_results_file, 'r', encoding='utf-8') as f:
        test_results = json.load(f)
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = TestResultValidator(args.config)
    
    # æ‰§è¡ŒéªŒè¯
    validation_results = validator.validate_test_results(test_results)
    
    # ä¿å­˜éªŒè¯æŠ¥å‘Š
    if args.output:
        validator.save_validation_report(args.output)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\néªŒè¯ç»“æœ: {validation_results['overall_status'].upper()}")
    print(f"åˆè§„æ€§è¯„åˆ†: {validation_results['compliance_score']:.1%}")
    
    if validation_results['critical_failures']:
        print("\nå…³é”®å¤±è´¥:")
        for failure in validation_results['critical_failures']:
            print(f"  âŒ {failure}")
    
    if validation_results['warnings']:
        print("\nè­¦å‘Š:")
        for warning in validation_results['warnings']:
            print(f"  âš ï¸ {warning}")
    
    return 0 if validation_results['overall_status'] in ['passed', 'warning'] else 1

if __name__ == "__main__":
    exit(main())
