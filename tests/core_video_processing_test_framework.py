#!/usr/bin/env python3
"""
VisionAI-ClipsMaster æ ¸å¿ƒè§†é¢‘å¤„ç†æ¨¡å—å®Œæ•´æµ‹è¯•æ¡†æ¶
å®ç°è§†é¢‘-å­—å¹•æ˜ å°„ç²¾åº¦éªŒè¯ã€çˆ†æ¬¾SRTç”ŸæˆåŠŸèƒ½æµ‹è¯•ã€ç«¯åˆ°ç«¯å·¥ä½œæµéªŒè¯ç­‰
"""

import os
import sys
import json
import time
import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import tempfile
import shutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class CoreVideoProcessingTestFramework:
    """æ ¸å¿ƒè§†é¢‘å¤„ç†æ¨¡å—æµ‹è¯•æ¡†æ¶"""
    
    def __init__(self):
        self.test_start_time = datetime.now()
        self.test_results = {
            "framework_info": {
                "name": "VisionAI-ClipsMaster Core Video Processing Test",
                "version": "1.0.0",
                "start_time": self.test_start_time.isoformat(),
                "test_environment": self._get_environment_info()
            },
            "test_modules": {},
            "performance_metrics": {},
            "quality_assessments": {},
            "errors": [],
            "summary": {}
        }
        
        # åˆ›å»ºæµ‹è¯•ç›®å½•ç»“æ„
        self.test_dir = project_root / "test_output" / "core_video_processing"
        self.test_data_dir = project_root / "test_data" / "core_processing"
        self.temp_dir = self.test_dir / "temp"
        
        self._setup_test_environment()
        self._setup_logging()
        
    def _get_environment_info(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•ç¯å¢ƒä¿¡æ¯"""
        import platform
        import psutil
        
        return {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
            "memory_available_gb": round(psutil.virtual_memory().available / (1024**3), 2)
        }
    
    def _setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        self.test_dir.mkdir(parents=True, exist_ok=True)
        self.test_data_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.test_data_dir / "videos").mkdir(exist_ok=True)
        (self.test_data_dir / "subtitles").mkdir(exist_ok=True)
        (self.test_data_dir / "expected_outputs").mkdir(exist_ok=True)
        (self.test_dir / "reports").mkdir(exist_ok=True)
        (self.test_dir / "logs").mkdir(exist_ok=True)
        
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = self.test_dir / "logs" / f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("æ ¸å¿ƒè§†é¢‘å¤„ç†æµ‹è¯•æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
        
    def prepare_test_data(self):
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        self.logger.info("å¼€å§‹å‡†å¤‡æµ‹è¯•æ•°æ®...")
        
        try:
            # åˆ›å»ºä¸­æ–‡æµ‹è¯•å­—å¹•
            self._create_chinese_test_subtitles()
            
            # åˆ›å»ºè‹±æ–‡æµ‹è¯•å­—å¹•
            self._create_english_test_subtitles()
            
            # åˆ›å»ºæ··åˆè¯­è¨€æµ‹è¯•å­—å¹•
            self._create_mixed_language_subtitles()
            
            # åˆ›å»ºæ¨¡æ‹Ÿè§†é¢‘æ–‡ä»¶ä¿¡æ¯
            self._create_mock_video_metadata()
            
            # åˆ›å»ºæµ‹è¯•é…ç½®
            self._create_test_configurations()
            
            self.logger.info("æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"æµ‹è¯•æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            self.test_results["errors"].append({
                "module": "data_preparation",
                "error": str(e),
                "traceback": traceback.format_exc()
            })
            return False
    
    def _create_chinese_test_subtitles(self):
        """åˆ›å»ºä¸­æ–‡æµ‹è¯•å­—å¹•"""
        chinese_original = """1
00:00:01,000 --> 00:00:05,000
è¿™æ˜¯ä¸€ä¸ªå…³äºçˆ±æƒ…çš„æ•…äº‹ï¼Œç”·ä¸»è§’æ˜¯ä¸€ä¸ªæ™®é€šçš„ä¸Šç­æ—ã€‚

2
00:00:05,500 --> 00:00:10,000
å¥³ä¸»è§’æ˜¯ä¸€ä¸ªç‹¬ç«‹è‡ªå¼ºçš„è®¾è®¡å¸ˆï¼Œä¸¤äººåœ¨å’–å•¡å…ç›¸é‡ã€‚

3
00:00:10,500 --> 00:00:15,000
ä»–ä»¬å¼€å§‹äº†ä¸€æ®µç¾å¥½çš„æ‹æƒ…ï¼Œä½†æ˜¯é¢ä¸´ç€å„ç§æŒ‘æˆ˜ã€‚

4
00:00:15,500 --> 00:00:20,000
ç”·ä¸»è§’çš„å‰å¥³å‹çªç„¶å›å›½ï¼Œå¸¦æ¥äº†æ„æƒ³ä¸åˆ°çš„éº»çƒ¦ã€‚

5
00:00:20,500 --> 00:00:25,000
å¥³ä¸»è§’å¼€å§‹æ€€ç–‘ç”·ä¸»è§’çš„çœŸå¿ƒï¼Œä¸¤äººå…³ç³»å‡ºç°è£‚ç—•ã€‚

6
00:00:25,500 --> 00:00:30,000
ç»è¿‡ä¸€ç³»åˆ—çš„è¯¯ä¼šå’Œè§£é‡Šï¼Œä»–ä»¬æœ€ç»ˆé‡å½’äºå¥½ã€‚

7
00:00:30,500 --> 00:00:35,000
æ•…äº‹çš„ç»“å±€æ˜¯ä¸¤äººå†³å®šç»“å©šï¼Œè¿‡ä¸Šå¹¸ç¦çš„ç”Ÿæ´»ã€‚"""

        chinese_viral = """1
00:00:01,000 --> 00:00:03,000
çˆ±æƒ…æ¥å¾—å¤ªçªç„¶ï¼ä¸Šç­æ—é‡è§ç‹¬ç«‹å¥³è®¾è®¡å¸ˆ

2
00:00:05,500 --> 00:00:08,000
å’–å•¡å…å¶é‡ï¼Œä¸€è§é’Ÿæƒ…çš„æµªæ¼«å¼€å§‹äº†

3
00:00:15,500 --> 00:00:18,000
å‰å¥³å‹å›å›½ï¼ä¸‰è§’æ‹æƒ…å³å°†çˆ†å‘

4
00:00:20,500 --> 00:00:23,000
ä¿¡ä»»å±æœºï¼å¥³ä¸»å¼€å§‹æ€€ç–‘ç”·ä¸»çš„çœŸå¿ƒ

5
00:00:30,500 --> 00:00:33,000
çœŸç›¸å¤§ç™½ï¼è¯¯ä¼šè§£é™¤ï¼Œçˆ±æƒ…é‡ç‡ƒ

6
00:00:33,500 --> 00:00:35,000
å®Œç¾ç»“å±€ï¼šæ±‚å©šæˆåŠŸï¼Œå¹¸ç¦ç¾æ»¡ï¼"""

        # ä¿å­˜æ–‡ä»¶
        with open(self.test_data_dir / "subtitles" / "chinese_original.srt", 'w', encoding='utf-8') as f:
            f.write(chinese_original)
            
        with open(self.test_data_dir / "subtitles" / "chinese_viral.srt", 'w', encoding='utf-8') as f:
            f.write(chinese_viral)
    
    def _create_english_test_subtitles(self):
        """åˆ›å»ºè‹±æ–‡æµ‹è¯•å­—å¹•"""
        english_original = """1
00:00:01,000 --> 00:00:05,000
This is a story about love. The male protagonist is an ordinary office worker.

2
00:00:05,500 --> 00:00:10,000
The female protagonist is an independent designer. They meet at a coffee shop.

3
00:00:10,500 --> 00:00:15,000
They begin a beautiful romance, but face various challenges.

4
00:00:15,500 --> 00:00:20,000
The male protagonist's ex-girlfriend suddenly returns, bringing unexpected trouble.

5
00:00:20,500 --> 00:00:25,000
The female protagonist begins to doubt the male protagonist's sincerity.

6
00:00:25,500 --> 00:00:30,000
After a series of misunderstandings and explanations, they reconcile.

7
00:00:30,500 --> 00:00:35,000
The story ends with them deciding to marry and live happily."""

        english_viral = """1
00:00:01,000 --> 00:00:03,000
Love strikes suddenly! Office worker meets independent designer

2
00:00:05,500 --> 00:00:08,000
Coffee shop encounter - love at first sight begins!

3
00:00:15,500 --> 00:00:18,000
Ex-girlfriend returns! Love triangle about to explode

4
00:00:20,500 --> 00:00:23,000
Trust crisis! She doubts his true feelings

5
00:00:30,500 --> 00:00:33,000
Truth revealed! Misunderstanding cleared, love rekindled

6
00:00:33,500 --> 00:00:35,000
Perfect ending: Proposal accepted, happily ever after!"""

        # ä¿å­˜æ–‡ä»¶
        with open(self.test_data_dir / "subtitles" / "english_original.srt", 'w', encoding='utf-8') as f:
            f.write(english_original)
            
        with open(self.test_data_dir / "subtitles" / "english_viral.srt", 'w', encoding='utf-8') as f:
            f.write(english_viral)
    
    def _create_mixed_language_subtitles(self):
        """åˆ›å»ºæ··åˆè¯­è¨€æµ‹è¯•å­—å¹•"""
        mixed_content = """1
00:00:01,000 --> 00:00:05,000
è¿™æ˜¯ä¸€ä¸ªlove storyï¼Œç”·ä¸»è§’æ˜¯ä¸€ä¸ªordinary office workerã€‚

2
00:00:05,500 --> 00:00:10,000
å¥³ä¸»è§’æ˜¯independent designerï¼Œä»–ä»¬åœ¨coffee shopç›¸é‡ã€‚

3
00:00:10,500 --> 00:00:15,000
Beautiful romanceå¼€å§‹äº†ï¼Œä½†æ˜¯face various challengesã€‚"""

        with open(self.test_data_dir / "subtitles" / "mixed_language.srt", 'w', encoding='utf-8') as f:
            f.write(mixed_content)
    
    def _create_mock_video_metadata(self):
        """åˆ›å»ºæ¨¡æ‹Ÿè§†é¢‘å…ƒæ•°æ®"""
        video_metadata = {
            "test_video_1.mp4": {
                "duration": 35.0,
                "fps": 25.0,
                "resolution": "1920x1080",
                "format": "mp4",
                "size_mb": 50.2
            },
            "test_video_2.avi": {
                "duration": 35.0,
                "fps": 30.0,
                "resolution": "1280x720",
                "format": "avi",
                "size_mb": 45.8
            },
            "test_video_3.flv": {
                "duration": 35.0,
                "fps": 24.0,
                "resolution": "854x480",
                "format": "flv",
                "size_mb": 25.3
            }
        }
        
        with open(self.test_data_dir / "videos" / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(video_metadata, f, indent=2, ensure_ascii=False)
    
    def _create_test_configurations(self):
        """åˆ›å»ºæµ‹è¯•é…ç½®"""
        test_config = {
            "alignment_precision_threshold": 0.5,  # æ—¶é—´è½´å¯¹é½ç²¾åº¦é˜ˆå€¼ï¼ˆç§’ï¼‰
            "memory_limit_gb": 4.0,  # å†…å­˜é™åˆ¶
            "model_switch_timeout": 1.5,  # æ¨¡å‹åˆ‡æ¢è¶…æ—¶ï¼ˆç§’ï¼‰
            "supported_video_formats": ["mp4", "avi", "flv"],
            "supported_subtitle_formats": ["srt"],
            "test_scenarios": {
                "basic_alignment": True,
                "multi_format_compatibility": True,
                "language_detection": True,
                "screenplay_reconstruction": True,
                "end_to_end_workflow": True,
                "performance_benchmarks": True
            }
        }
        
        with open(self.test_data_dir / "test_config.json", 'w', encoding='utf-8') as f:
            json.dump(test_config, f, indent=2, ensure_ascii=False)
        
        self.test_config = test_config

    def run_comprehensive_tests(self):
        """è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶"""
        self.logger.info("å¼€å§‹è¿è¡ŒVisionAI-ClipsMasterå®Œæ•´æµ‹è¯•å¥—ä»¶...")

        try:
            # å¯¼å…¥æµ‹è¯•æ¨¡å—
            from test_alignment_precision import AlignmentPrecisionTester
            from test_viral_srt_generation import ViralSRTGenerationTester
            from test_system_integration import SystemIntegrationTester

            # 1. è§†é¢‘-å­—å¹•æ˜ å°„ç²¾åº¦éªŒè¯
            self.logger.info("=" * 60)
            self.logger.info("1. å¼€å§‹è§†é¢‘-å­—å¹•æ˜ å°„ç²¾åº¦éªŒè¯æµ‹è¯•")
            alignment_tester = AlignmentPrecisionTester(self)
            alignment_results = alignment_tester.run_all_tests()
            self.test_results["test_modules"]["alignment_precision"] = alignment_results

            # 2. AIå‰§æœ¬é‡æ„åŠŸèƒ½æµ‹è¯•
            self.logger.info("=" * 60)
            self.logger.info("2. å¼€å§‹AIå‰§æœ¬é‡æ„åŠŸèƒ½æµ‹è¯•")
            viral_tester = ViralSRTGenerationTester(self)
            viral_results = viral_tester.run_all_tests()
            self.test_results["test_modules"]["viral_srt_generation"] = viral_results

            # 3. ç«¯åˆ°ç«¯å·¥ä½œæµéªŒè¯
            self.logger.info("=" * 60)
            self.logger.info("3. å¼€å§‹ç«¯åˆ°ç«¯å·¥ä½œæµéªŒè¯æµ‹è¯•")
            integration_tester = SystemIntegrationTester(self)
            integration_results = integration_tester.run_all_tests()
            self.test_results["test_modules"]["system_integration"] = integration_results

            # 4. ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š
            self._generate_comprehensive_report()

            # 5. æ¸…ç†æµ‹è¯•ç¯å¢ƒ
            self._cleanup_test_environment()

            self.logger.info("=" * 60)
            self.logger.info("âœ… VisionAI-ClipsMasterå®Œæ•´æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ")

            return self.test_results

        except Exception as e:
            self.logger.error(f"æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¤±è´¥: {e}")
            self.test_results["errors"].append({
                "module": "comprehensive_test_suite",
                "error": str(e),
                "traceback": traceback.format_exc()
            })
            return self.test_results

    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š...")

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_modules = len(self.test_results["test_modules"])
        successful_modules = 0
        total_test_cases = 0
        passed_test_cases = 0

        for module_name, module_results in self.test_results["test_modules"].items():
            if module_results.get("test_cases"):
                total_test_cases += len(module_results["test_cases"])
                passed_test_cases += sum(1 for tc in module_results["test_cases"]
                                       if tc.get("status") in ["completed", "passed"])

            # æ£€æŸ¥æ¨¡å—æ˜¯å¦æˆåŠŸ
            if not module_results.get("errors"):
                successful_modules += 1

        # ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡
        self.test_results["performance_metrics"] = {
            "total_execution_time": (datetime.now() - self.test_start_time).total_seconds(),
            "modules_tested": total_modules,
            "successful_modules": successful_modules,
            "module_success_rate": successful_modules / total_modules if total_modules > 0 else 0,
            "total_test_cases": total_test_cases,
            "passed_test_cases": passed_test_cases,
            "test_case_success_rate": passed_test_cases / total_test_cases if total_test_cases > 0 else 0
        }

        # ç”Ÿæˆè´¨é‡è¯„ä¼°
        self.test_results["quality_assessments"] = {
            "alignment_precision_score": self._extract_metric("alignment_precision", "precision_metrics", "success_rate", 0),
            "viral_generation_score": self._extract_metric("viral_srt_generation", "generation_metrics", "success_rate", 0),
            "integration_workflow_score": self._extract_metric("system_integration", "workflow_metrics", "success_rate", 0),
            "overall_system_quality": 0
        }

        # è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†
        quality_scores = [
            self.test_results["quality_assessments"]["alignment_precision_score"],
            self.test_results["quality_assessments"]["viral_generation_score"],
            self.test_results["quality_assessments"]["integration_workflow_score"]
        ]
        valid_scores = [score for score in quality_scores if score > 0]
        self.test_results["quality_assessments"]["overall_system_quality"] = (
            sum(valid_scores) / len(valid_scores) if valid_scores else 0
        )

        # ç”Ÿæˆæ€»ç»“
        self.test_results["summary"] = {
            "test_execution_status": "completed" if successful_modules == total_modules else "partial_failure",
            "critical_issues": len(self.test_results["errors"]),
            "performance_rating": self._calculate_performance_rating(),
            "recommendations": self._generate_recommendations(),
            "next_steps": self._generate_next_steps()
        }

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self._save_detailed_report()

    def _extract_metric(self, module_name: str, metric_category: str, metric_name: str, default_value: Any) -> Any:
        """ä»æµ‹è¯•ç»“æœä¸­æå–æŒ‡æ ‡"""
        try:
            return self.test_results["test_modules"][module_name][metric_category][metric_name]
        except (KeyError, TypeError):
            return default_value

    def _calculate_performance_rating(self) -> str:
        """è®¡ç®—æ€§èƒ½è¯„çº§"""
        overall_quality = self.test_results["quality_assessments"]["overall_system_quality"]

        if overall_quality >= 0.9:
            return "ä¼˜ç§€ (A)"
        elif overall_quality >= 0.8:
            return "è‰¯å¥½ (B)"
        elif overall_quality >= 0.7:
            return "åˆæ ¼ (C)"
        elif overall_quality >= 0.6:
            return "éœ€æ”¹è¿› (D)"
        else:
            return "ä¸åˆæ ¼ (F)"

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        alignment_score = self.test_results["quality_assessments"]["alignment_precision_score"]
        if alignment_score < 0.8:
            recommendations.append("å»ºè®®ä¼˜åŒ–è§†é¢‘-å­—å¹•å¯¹é½ç®—æ³•ï¼Œæé«˜æ—¶é—´è½´ç²¾åº¦")

        viral_score = self.test_results["quality_assessments"]["viral_generation_score"]
        if viral_score < 0.8:
            recommendations.append("å»ºè®®æ”¹è¿›AIå‰§æœ¬é‡æ„æ¨¡å‹ï¼Œæå‡çˆ†æ¬¾ç‰¹å¾è¯†åˆ«èƒ½åŠ›")

        integration_score = self.test_results["quality_assessments"]["integration_workflow_score"]
        if integration_score < 0.8:
            recommendations.append("å»ºè®®ä¼˜åŒ–ç³»ç»Ÿé›†æˆæµç¨‹ï¼Œæé«˜ç«¯åˆ°ç«¯å·¥ä½œæµç¨³å®šæ€§")

        if len(self.test_results["errors"]) > 0:
            recommendations.append("å»ºè®®ä¿®å¤æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç°çš„é”™è¯¯å’Œå¼‚å¸¸")

        if not recommendations:
            recommendations.append("ç³»ç»Ÿæ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒå½“å‰è´¨é‡æ°´å¹³")

        return recommendations

    def _generate_next_steps(self) -> List[str]:
        """ç”Ÿæˆåç»­æ­¥éª¤"""
        next_steps = [
            "1. å®¡æŸ¥æµ‹è¯•æŠ¥å‘Šä¸­çš„è¯¦ç»†ç»“æœå’ŒæŒ‡æ ‡",
            "2. æ ¹æ®å»ºè®®ä¼˜åŒ–ç›¸åº”çš„ç³»ç»Ÿæ¨¡å—",
            "3. åœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œç”¨æˆ·éªŒæ”¶æµ‹è¯•",
            "4. å‡†å¤‡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²é…ç½®",
            "5. å»ºç«‹æŒç»­é›†æˆå’Œç›‘æ§æœºåˆ¶"
        ]

        return next_steps

    def _save_detailed_report(self):
        """ä¿å­˜è¯¦ç»†æµ‹è¯•æŠ¥å‘Š"""
        report_file = self.test_dir / "reports" / f"comprehensive_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False, default=str)

        self.logger.info(f"è¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        # ç”Ÿæˆç®€åŒ–çš„HTMLæŠ¥å‘Š
        self._generate_html_report(report_file.with_suffix('.html'))

    def _generate_html_report(self, html_file: Path):
        """ç”ŸæˆHTMLæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VisionAI-ClipsMaster æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; color: #333; border-bottom: 2px solid #007acc; padding-bottom: 20px; margin-bottom: 30px; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric-card {{ background: #f8f9fa; padding: 15px; border-radius: 6px; border-left: 4px solid #007acc; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #007acc; }}
        .metric-label {{ color: #666; font-size: 14px; }}
        .status-passed {{ color: #28a745; }}
        .status-failed {{ color: #dc3545; }}
        .status-warning {{ color: #ffc107; }}
        .section {{ margin: 30px 0; }}
        .section h2 {{ color: #333; border-bottom: 1px solid #ddd; padding-bottom: 10px; }}
        .recommendations {{ background: #e7f3ff; padding: 15px; border-radius: 6px; border-left: 4px solid #007acc; }}
        .recommendations ul {{ margin: 10px 0; padding-left: 20px; }}
        .test-module {{ background: #f8f9fa; margin: 10px 0; padding: 15px; border-radius: 6px; }}
        .test-module h3 {{ margin-top: 0; color: #007acc; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>VisionAI-ClipsMaster ç»¼åˆæµ‹è¯•æŠ¥å‘Š</h1>
            <p>æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p>æµ‹è¯•ç‰ˆæœ¬: {self.test_results['framework_info']['version']}</p>
        </div>

        <div class="section">
            <h2>ğŸ“Š æ€»ä½“æŒ‡æ ‡</h2>
            <div class="metric-grid">
                <div class="metric-card">
                    <div class="metric-value">{self.test_results['performance_metrics']['module_success_rate']:.1%}</div>
                    <div class="metric-label">æ¨¡å—æˆåŠŸç‡</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{self.test_results['performance_metrics']['test_case_success_rate']:.1%}</div>
                    <div class="metric-label">æµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{self.test_results['quality_assessments']['overall_system_quality']:.1%}</div>
                    <div class="metric-label">ç³»ç»Ÿæ•´ä½“è´¨é‡</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{self.test_results['summary']['performance_rating']}</div>
                    <div class="metric-label">æ€§èƒ½è¯„çº§</div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>ğŸ” æµ‹è¯•æ¨¡å—è¯¦æƒ…</h2>
            <div class="test-module">
                <h3>1. è§†é¢‘-å­—å¹•æ˜ å°„ç²¾åº¦éªŒè¯</h3>
                <p>ç²¾åº¦è¯„åˆ†: <span class="metric-value">{self.test_results['quality_assessments']['alignment_precision_score']:.1%}</span></p>
                <p>çŠ¶æ€: <span class="{'status-passed' if self.test_results['quality_assessments']['alignment_precision_score'] >= 0.8 else 'status-warning'}">
                    {'âœ… é€šè¿‡' if self.test_results['quality_assessments']['alignment_precision_score'] >= 0.8 else 'âš ï¸ éœ€ä¼˜åŒ–'}
                </span></p>
            </div>

            <div class="test-module">
                <h3>2. AIå‰§æœ¬é‡æ„åŠŸèƒ½</h3>
                <p>ç”Ÿæˆè´¨é‡è¯„åˆ†: <span class="metric-value">{self.test_results['quality_assessments']['viral_generation_score']:.1%}</span></p>
                <p>çŠ¶æ€: <span class="{'status-passed' if self.test_results['quality_assessments']['viral_generation_score'] >= 0.8 else 'status-warning'}">
                    {'âœ… é€šè¿‡' if self.test_results['quality_assessments']['viral_generation_score'] >= 0.8 else 'âš ï¸ éœ€ä¼˜åŒ–'}
                </span></p>
            </div>

            <div class="test-module">
                <h3>3. ç«¯åˆ°ç«¯å·¥ä½œæµé›†æˆ</h3>
                <p>é›†æˆè¯„åˆ†: <span class="metric-value">{self.test_results['quality_assessments']['integration_workflow_score']:.1%}</span></p>
                <p>çŠ¶æ€: <span class="{'status-passed' if self.test_results['quality_assessments']['integration_workflow_score'] >= 0.8 else 'status-warning'}">
                    {'âœ… é€šè¿‡' if self.test_results['quality_assessments']['integration_workflow_score'] >= 0.8 else 'âš ï¸ éœ€ä¼˜åŒ–'}
                </span></p>
            </div>
        </div>

        <div class="section">
            <h2>ğŸ’¡ æ”¹è¿›å»ºè®®</h2>
            <div class="recommendations">
                <ul>
                    {''.join(f'<li>{rec}</li>' for rec in self.test_results['summary']['recommendations'])}
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>ğŸ“‹ åç»­æ­¥éª¤</h2>
            <div class="recommendations">
                <ol>
                    {''.join(f'<li>{step}</li>' for step in self.test_results['summary']['next_steps'])}
                </ol>
            </div>
        </div>

        <div class="section">
            <h2>ğŸ”§ ç¯å¢ƒä¿¡æ¯</h2>
            <p><strong>å¹³å°:</strong> {self.test_results['framework_info']['test_environment']['platform']}</p>
            <p><strong>Pythonç‰ˆæœ¬:</strong> {self.test_results['framework_info']['test_environment']['python_version']}</p>
            <p><strong>CPUæ ¸å¿ƒæ•°:</strong> {self.test_results['framework_info']['test_environment']['cpu_count']}</p>
            <p><strong>æ€»å†…å­˜:</strong> {self.test_results['framework_info']['test_environment']['memory_total_gb']} GB</p>
            <p><strong>å¯ç”¨å†…å­˜:</strong> {self.test_results['framework_info']['test_environment']['memory_available_gb']} GB</p>
        </div>
    </div>
</body>
</html>
        """

        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        self.logger.info(f"HTMLæµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

    def _cleanup_test_environment(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        self.logger.info("å¼€å§‹æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")

        try:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)
                self.logger.info("âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

            # æ¸…ç†ç”Ÿæˆçš„æµ‹è¯•è§†é¢‘æ–‡ä»¶
            output_dir = self.test_dir / "output"
            if output_dir.exists():
                for file in output_dir.glob("test_*.mp4"):
                    file.unlink()
                self.logger.info("âœ… æµ‹è¯•è§†é¢‘æ–‡ä»¶æ¸…ç†å®Œæˆ")

            # æ¸…ç†æ¨¡å‹ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            cache_dir = project_root / "models" / "cache"
            if cache_dir.exists():
                for file in cache_dir.glob("test_*.cache"):
                    file.unlink()
                self.logger.info("âœ… æ¨¡å‹ç¼“å­˜æ¸…ç†å®Œæˆ")

            # é‡ç½®æ—¥å¿—æ–‡ä»¶å¤§å°ï¼ˆä¿ç•™æœ€æ–°çš„æ—¥å¿—ï¼‰
            log_dir = self.test_dir / "logs"
            if log_dir.exists():
                log_files = sorted(log_dir.glob("test_*.log"), key=lambda x: x.stat().st_mtime)
                # ä¿ç•™æœ€æ–°çš„3ä¸ªæ—¥å¿—æ–‡ä»¶
                for old_log in log_files[:-3]:
                    old_log.unlink()
                self.logger.info("âœ… å†å²æ—¥å¿—æ¸…ç†å®Œæˆ")

            self.logger.info("ğŸ§¹ æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")

        except Exception as e:
            self.logger.warning(f"æµ‹è¯•ç¯å¢ƒæ¸…ç†æ—¶å‡ºç°è­¦å‘Š: {e}")

if __name__ == "__main__":
    # åˆå§‹åŒ–æµ‹è¯•æ¡†æ¶
    framework = CoreVideoProcessingTestFramework()

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    if framework.prepare_test_data():
        print("âœ… æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ æµ‹è¯•ç›®å½•: {framework.test_dir}")
        print(f"ğŸ“ æµ‹è¯•æ•°æ®ç›®å½•: {framework.test_data_dir}")

        # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
        print("\nğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶...")
        test_results = framework.run_comprehensive_tests()

        # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"æ¨¡å—æˆåŠŸç‡: {test_results['performance_metrics']['module_success_rate']:.1%}")
        print(f"æµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡: {test_results['performance_metrics']['test_case_success_rate']:.1%}")
        print(f"ç³»ç»Ÿæ•´ä½“è´¨é‡: {test_results['quality_assessments']['overall_system_quality']:.1%}")
        print(f"æ€§èƒ½è¯„çº§: {test_results['summary']['performance_rating']}")
        print(f"æ‰§è¡Œæ—¶é—´: {test_results['performance_metrics']['total_execution_time']:.1f} ç§’")

        if test_results['summary']['test_execution_status'] == 'completed':
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•æ¨¡å—æ‰§è¡ŒæˆåŠŸï¼")
        else:
            print(f"\nâš ï¸ æµ‹è¯•æ‰§è¡Œå®Œæˆï¼Œä½†æœ‰ {test_results['summary']['critical_issues']} ä¸ªå…³é”®é—®é¢˜éœ€è¦å…³æ³¨")

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {framework.test_dir / 'reports'}")

    else:
        print("âŒ æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥")
        sys.exit(1)
