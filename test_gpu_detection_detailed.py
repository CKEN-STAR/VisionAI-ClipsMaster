#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUæ£€æµ‹è¯¦ç»†æµ‹è¯•è„šæœ¬

ä¸“é—¨æµ‹è¯•GPUæ£€æµ‹åŠŸèƒ½çš„å‡†ç¡®æ€§å’Œä¸åŒç¡¬ä»¶é…ç½®ä¸‹çš„æ¨èæ•ˆæœ
"""

import logging
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class GPUDetectionTester:
    """GPUæ£€æµ‹æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.test_results = {}
    
    def run_comprehensive_gpu_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„GPUæ£€æµ‹æµ‹è¯•"""
        logger.info("ğŸ” å¼€å§‹å…¨é¢GPUæ£€æµ‹æµ‹è¯•...")
        
        try:
            # 1. æµ‹è¯•æ‰€æœ‰GPUæ£€æµ‹æ–¹æ³•
            self._test_all_detection_methods()
            
            # 2. æµ‹è¯•ç¡¬ä»¶æ£€æµ‹å™¨
            self._test_hardware_detector()
            
            # 3. æµ‹è¯•æ™ºèƒ½æ¨èå™¨
            self._test_intelligent_recommender()
            
            # 4. æ¨¡æ‹Ÿä¸åŒGPUé…ç½®
            self._test_simulated_gpu_configs()
            
            # 5. éªŒè¯æ¨èé€»è¾‘
            self._validate_recommendation_logic()
            
            logger.info("âœ… æ‰€æœ‰GPUæ£€æµ‹æµ‹è¯•å®Œæˆ")
            return {
                "success": True,
                "test_results": self.test_results
            }
            
        except Exception as e:
            logger.error(f"âŒ GPUæ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "test_results": self.test_results
            }
    
    def _test_all_detection_methods(self):
        """æµ‹è¯•æ‰€æœ‰GPUæ£€æµ‹æ–¹æ³•"""
        logger.info("ğŸ” æµ‹è¯•æ‰€æœ‰GPUæ£€æµ‹æ–¹æ³•...")
        
        detection_results = {}
        
        # æ–¹æ³•1: PyTorch CUDA
        try:
            import torch
            if hasattr(torch, 'cuda'):
                cuda_available = torch.cuda.is_available()
                device_count = torch.cuda.device_count() if cuda_available else 0
                
                devices = []
                if cuda_available:
                    for i in range(device_count):
                        try:
                            name = torch.cuda.get_device_name(i)
                            props = torch.cuda.get_device_properties(i)
                            devices.append({
                                "id": i,
                                "name": name,
                                "memory_gb": props.total_memory / (1024**3),
                                "compute_capability": f"{props.major}.{props.minor}"
                            })
                        except Exception as e:
                            devices.append({"id": i, "error": str(e)})
                
                detection_results["pytorch_cuda"] = {
                    "available": cuda_available,
                    "device_count": device_count,
                    "devices": devices
                }
            else:
                detection_results["pytorch_cuda"] = {"available": False, "error": "CUDA not available"}
        except Exception as e:
            detection_results["pytorch_cuda"] = {"available": False, "error": str(e)}
        
        # æ–¹æ³•2: GPUtil
        try:
            import GPUtil
            gpus = GPUtil.getGPUs()
            
            devices = []
            for i, gpu in enumerate(gpus):
                devices.append({
                    "id": i,
                    "name": gpu.name,
                    "memory_total_gb": gpu.memoryTotal / 1024,
                    "memory_free_gb": gpu.memoryFree / 1024,
                    "memory_used_gb": gpu.memoryUsed / 1024,
                    "temperature": gpu.temperature,
                    "load": gpu.load
                })
            
            detection_results["gputil"] = {
                "available": len(gpus) > 0,
                "device_count": len(gpus),
                "devices": devices
            }
        except Exception as e:
            detection_results["gputil"] = {"available": False, "error": str(e)}
        
        # æ–¹æ³•3: pynvml
        try:
            import pynvml
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            
            devices = []
            for i in range(device_count):
                try:
                    handle = pynvml.nvmlDeviceGetHandleByIndex(i)
                    name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                    
                    devices.append({
                        "id": i,
                        "name": name,
                        "memory_total_gb": memory_info.total / (1024**3),
                        "memory_free_gb": memory_info.free / (1024**3),
                        "memory_used_gb": memory_info.used / (1024**3)
                    })
                except Exception as e:
                    devices.append({"id": i, "error": str(e)})
            
            detection_results["pynvml"] = {
                "available": device_count > 0,
                "device_count": device_count,
                "devices": devices
            }
        except Exception as e:
            detection_results["pynvml"] = {"available": False, "error": str(e)}
        
        self.test_results["detection_methods"] = detection_results
        
        # åˆ†ææ£€æµ‹ç»“æœ
        available_methods = [method for method, result in detection_results.items() if result.get("available")]
        logger.info(f"å¯ç”¨çš„GPUæ£€æµ‹æ–¹æ³•: {available_methods}")
        
        if available_methods:
            # æ¯”è¾ƒä¸åŒæ–¹æ³•çš„ç»“æœ
            self._compare_detection_methods(detection_results)
        else:
            logger.info("âŒ æœªæ£€æµ‹åˆ°ä»»ä½•GPUè®¾å¤‡")
    
    def _compare_detection_methods(self, detection_results: Dict[str, Any]):
        """æ¯”è¾ƒä¸åŒæ£€æµ‹æ–¹æ³•çš„ç»“æœ"""
        logger.info("ğŸ” æ¯”è¾ƒä¸åŒæ£€æµ‹æ–¹æ³•çš„ç»“æœ...")
        
        comparison = {
            "device_counts": {},
            "memory_info": {},
            "consistency_check": {}
        }
        
        # æ”¶é›†è®¾å¤‡æ•°é‡
        for method, result in detection_results.items():
            if result.get("available"):
                comparison["device_counts"][method] = result.get("device_count", 0)
        
        # æ”¶é›†å†…å­˜ä¿¡æ¯
        for method, result in detection_results.items():
            if result.get("available") and result.get("devices"):
                total_memory = 0
                for device in result["devices"]:
                    if "memory_gb" in device:
                        total_memory += device["memory_gb"]
                    elif "memory_total_gb" in device:
                        total_memory += device["memory_total_gb"]
                comparison["memory_info"][method] = total_memory
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        device_counts = list(comparison["device_counts"].values())
        memory_values = list(comparison["memory_info"].values())
        
        comparison["consistency_check"] = {
            "device_count_consistent": len(set(device_counts)) <= 1,
            "memory_info_consistent": len(memory_values) <= 1 or (
                max(memory_values) - min(memory_values) <= max(memory_values) * 0.1
            ) if memory_values else True
        }
        
        self.test_results["method_comparison"] = comparison
        logger.info(f"æ£€æµ‹æ–¹æ³•æ¯”è¾ƒç»“æœ: {comparison}")
    
    def _test_hardware_detector(self):
        """æµ‹è¯•ç¡¬ä»¶æ£€æµ‹å™¨"""
        logger.info("ğŸ” æµ‹è¯•ç¡¬ä»¶æ£€æµ‹å™¨...")
        
        try:
            from src.core.hardware_detector import HardwareDetector
            
            detector = HardwareDetector()
            hardware_info = detector.detect_hardware()
            
            hardware_result = {
                "gpu_type": str(hardware_info.gpu_type) if hasattr(hardware_info, 'gpu_type') else "unknown",
                "gpu_count": getattr(hardware_info, 'gpu_count', 0),
                "gpu_memory_gb": getattr(hardware_info, 'gpu_memory_gb', 0),
                "gpu_names": getattr(hardware_info, 'gpu_names', []),
                "system_ram_gb": getattr(hardware_info, 'total_memory_gb', 0),
                "performance_level": str(hardware_info.performance_level) if hasattr(hardware_info, 'performance_level') else "unknown",
                "recommended_quantization": getattr(hardware_info, 'recommended_quantization', 'unknown'),
                "gpu_acceleration": getattr(hardware_info, 'gpu_acceleration', False)
            }
            
            self.test_results["hardware_detector"] = hardware_result
            logger.info(f"ç¡¬ä»¶æ£€æµ‹å™¨ç»“æœ: {hardware_result}")
            
        except Exception as e:
            logger.error(f"ç¡¬ä»¶æ£€æµ‹å™¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["hardware_detector"] = {"error": str(e)}
    
    def _test_intelligent_recommender(self):
        """æµ‹è¯•æ™ºèƒ½æ¨èå™¨"""
        logger.info("ğŸ” æµ‹è¯•æ™ºèƒ½æ¨èå™¨...")
        
        try:
            from src.core.intelligent_model_selector import IntelligentModelSelector
            
            selector = IntelligentModelSelector()
            selector.force_refresh_hardware()
            
            # æµ‹è¯•ä¸­æ–‡æ¨¡å‹æ¨è
            zh_recommendation = selector.recommend_model_version("qwen2.5-7b")
            
            # æµ‹è¯•è‹±æ–‡æ¨¡å‹æ¨è
            en_recommendation = selector.recommend_model_version("mistral-7b")
            
            recommender_result = {
                "zh_model": {
                    "model_name": zh_recommendation.model_name if zh_recommendation else None,
                    "variant_name": zh_recommendation.variant.name if zh_recommendation and zh_recommendation.variant else None,
                    "quantization": zh_recommendation.variant.quantization.value if zh_recommendation and zh_recommendation.variant else None,
                    "size_gb": zh_recommendation.variant.size_gb if zh_recommendation and zh_recommendation.variant else None,
                    "quality_retention": zh_recommendation.variant.quality_retention if zh_recommendation and zh_recommendation.variant else None,
                    "reasoning": zh_recommendation.reasoning if zh_recommendation else None
                },
                "en_model": {
                    "model_name": en_recommendation.model_name if en_recommendation else None,
                    "variant_name": en_recommendation.variant.name if en_recommendation and en_recommendation.variant else None,
                    "quantization": en_recommendation.variant.quantization.value if en_recommendation and en_recommendation.variant else None,
                    "size_gb": en_recommendation.variant.size_gb if en_recommendation and en_recommendation.variant else None,
                    "quality_retention": en_recommendation.variant.quality_retention if en_recommendation and en_recommendation.variant else None,
                    "reasoning": en_recommendation.reasoning if en_recommendation else None
                }
            }
            
            self.test_results["intelligent_recommender"] = recommender_result
            logger.info(f"æ™ºèƒ½æ¨èå™¨ç»“æœ: ä¸­æ–‡æ¨¡å‹={recommender_result['zh_model']['quantization']}, è‹±æ–‡æ¨¡å‹={recommender_result['en_model']['quantization']}")
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½æ¨èå™¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["intelligent_recommender"] = {"error": str(e)}
    
    def _test_simulated_gpu_configs(self):
        """æµ‹è¯•æ¨¡æ‹ŸGPUé…ç½®"""
        logger.info("ğŸ” æµ‹è¯•æ¨¡æ‹ŸGPUé…ç½®...")
        
        gpu_configs = [
            {"name": "RTX 4090", "memory_gb": 24, "expected_quant": "Q8_0"},
            {"name": "RTX 4080", "memory_gb": 16, "expected_quant": "Q8_0"},
            {"name": "RTX 4070", "memory_gb": 12, "expected_quant": "Q5_K"},
            {"name": "RTX 4060", "memory_gb": 8, "expected_quant": "Q5_K"},
            {"name": "GTX 1660", "memory_gb": 6, "expected_quant": "Q4_K_M"},
            {"name": "é›†æˆæ˜¾å¡", "memory_gb": 2, "expected_quant": "Q4_K_M"},
            {"name": "æ— GPU", "memory_gb": 0, "expected_quant": "Q2_K"}
        ]
        
        simulation_results = {}
        
        for config in gpu_configs:
            # æ¨¡æ‹Ÿæ¨èé€»è¾‘
            memory_gb = config["memory_gb"]
            
            if memory_gb >= 16:
                recommended_quant = "Q8_0"
                performance_level = "ultra"
            elif memory_gb >= 8:
                recommended_quant = "Q5_K"
                performance_level = "high"
            elif memory_gb >= 4:
                recommended_quant = "Q4_K_M"
                performance_level = "medium"
            else:
                recommended_quant = "Q2_K"
                performance_level = "low"
            
            simulation_results[config["name"]] = {
                "memory_gb": memory_gb,
                "recommended_quantization": recommended_quant,
                "expected_quantization": config["expected_quant"],
                "performance_level": performance_level,
                "match": recommended_quant == config["expected_quant"]
            }
        
        self.test_results["simulated_configs"] = simulation_results
        
        # ç»Ÿè®¡åŒ¹é…ç‡
        total_configs = len(simulation_results)
        matched_configs = sum(1 for result in simulation_results.values() if result["match"])
        match_rate = (matched_configs / total_configs * 100) if total_configs > 0 else 0
        
        logger.info(f"æ¨¡æ‹Ÿé…ç½®æµ‹è¯•: {matched_configs}/{total_configs} åŒ¹é… ({match_rate:.1f}%)")
    
    def _validate_recommendation_logic(self):
        """éªŒè¯æ¨èé€»è¾‘"""
        logger.info("ğŸ” éªŒè¯æ¨èé€»è¾‘...")
        
        validation_results = {
            "gpu_detection_working": False,
            "performance_scaling": False,
            "quantization_appropriate": False,
            "gpu_acceleration_considered": False,
            "issues": []
        }
        
        # æ£€æŸ¥GPUæ£€æµ‹æ˜¯å¦å·¥ä½œ
        detection_methods = self.test_results.get("detection_methods", {})
        available_methods = [method for method, result in detection_methods.items() if result.get("available")]
        
        if available_methods:
            validation_results["gpu_detection_working"] = True
        else:
            validation_results["issues"].append("GPUæ£€æµ‹æœªå·¥ä½œ")
        
        # æ£€æŸ¥æ€§èƒ½ç­‰çº§æ˜¯å¦åˆç†
        hardware_detector = self.test_results.get("hardware_detector", {})
        gpu_memory = hardware_detector.get("gpu_memory_gb", 0)
        performance_level = hardware_detector.get("performance_level", "").lower()
        
        if gpu_memory > 0:
            if gpu_memory >= 8 and performance_level in ["high", "ultra"]:
                validation_results["performance_scaling"] = True
            elif gpu_memory < 8 and performance_level in ["medium", "low"]:
                validation_results["performance_scaling"] = True
            else:
                validation_results["issues"].append(f"æ€§èƒ½ç­‰çº§ä¸åŒ¹é…GPUé…ç½®: {gpu_memory}GB -> {performance_level}")
        else:
            if performance_level in ["low", "medium"]:
                validation_results["performance_scaling"] = True
        
        # æ£€æŸ¥é‡åŒ–ç­‰çº§æ˜¯å¦åˆé€‚
        recommender = self.test_results.get("intelligent_recommender", {})
        zh_quant = recommender.get("zh_model", {}).get("quantization", "").upper()
        
        if gpu_memory >= 16 and zh_quant in ["Q8_0", "Q5_K"]:
            validation_results["quantization_appropriate"] = True
        elif gpu_memory >= 8 and zh_quant in ["Q5_K", "Q4_K_M"]:
            validation_results["quantization_appropriate"] = True
        elif gpu_memory < 8 and zh_quant in ["Q4_K_M", "Q2_K"]:
            validation_results["quantization_appropriate"] = True
        else:
            validation_results["issues"].append(f"é‡åŒ–ç­‰çº§ä¸é€‚åˆGPUé…ç½®: {gpu_memory}GB -> {zh_quant}")
        
        # æ£€æŸ¥æ˜¯å¦è€ƒè™‘äº†GPUåŠ é€Ÿ
        gpu_acceleration = hardware_detector.get("gpu_acceleration", False)
        if gpu_memory > 0:
            validation_results["gpu_acceleration_considered"] = gpu_acceleration
            if not gpu_acceleration:
                validation_results["issues"].append("æœ‰GPUä½†æœªå¯ç”¨GPUåŠ é€Ÿ")
        else:
            validation_results["gpu_acceleration_considered"] = not gpu_acceleration
        
        self.test_results["validation"] = validation_results
        
        # è¾“å‡ºéªŒè¯ç»“æœ
        passed_checks = sum(1 for key, value in validation_results.items() 
                          if key != "issues" and value)
        total_checks = 4
        
        logger.info(f"æ¨èé€»è¾‘éªŒè¯: {passed_checks}/{total_checks} é¡¹é€šè¿‡")
        
        if validation_results["issues"]:
            logger.warning(f"å‘ç°é—®é¢˜: {validation_results['issues']}")
    
    def print_detailed_report(self):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ” GPUæ£€æµ‹è¯¦ç»†æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # GPUæ£€æµ‹æ–¹æ³•ç»“æœ
        if "detection_methods" in self.test_results:
            print(f"\nğŸ® GPUæ£€æµ‹æ–¹æ³•ç»“æœ:")
            detection_methods = self.test_results["detection_methods"]
            
            for method, result in detection_methods.items():
                if result.get("available"):
                    device_count = result.get("device_count", 0)
                    print(f"  âœ… {method}: {device_count} ä¸ªGPU")
                    
                    if result.get("devices"):
                        for device in result["devices"]:
                            if "error" not in device:
                                name = device.get("name", "Unknown")
                                memory = device.get("memory_gb") or device.get("memory_total_gb", 0)
                                print(f"    â€¢ {name}: {memory:.1f}GB")
                else:
                    error = result.get("error", "æœªçŸ¥é”™è¯¯")
                    print(f"  âŒ {method}: {error}")
        
        # ç¡¬ä»¶æ£€æµ‹å™¨ç»“æœ
        if "hardware_detector" in self.test_results:
            print(f"\nğŸ–¥ï¸ ç¡¬ä»¶æ£€æµ‹å™¨ç»“æœ:")
            hw = self.test_results["hardware_detector"]
            
            if "error" not in hw:
                print(f"  GPUç±»å‹: {hw.get('gpu_type', 'unknown')}")
                print(f"  GPUæ•°é‡: {hw.get('gpu_count', 0)}")
                print(f"  GPUæ˜¾å­˜: {hw.get('gpu_memory_gb', 0):.1f}GB")
                print(f"  ç³»ç»Ÿå†…å­˜: {hw.get('system_ram_gb', 0):.1f}GB")
                print(f"  æ€§èƒ½ç­‰çº§: {hw.get('performance_level', 'unknown')}")
                print(f"  æ¨èé‡åŒ–: {hw.get('recommended_quantization', 'unknown')}")
                print(f"  GPUåŠ é€Ÿ: {hw.get('gpu_acceleration', False)}")
            else:
                print(f"  âŒ é”™è¯¯: {hw['error']}")
        
        # æ™ºèƒ½æ¨èç»“æœ
        if "intelligent_recommender" in self.test_results:
            print(f"\nğŸ¤– æ™ºèƒ½æ¨èç»“æœ:")
            rec = self.test_results["intelligent_recommender"]
            
            if "error" not in rec:
                zh_model = rec.get("zh_model", {})
                en_model = rec.get("en_model", {})
                
                print(f"  ä¸­æ–‡æ¨¡å‹:")
                print(f"    é‡åŒ–ç­‰çº§: {zh_model.get('quantization', 'unknown')}")
                print(f"    æ¨¡å‹å¤§å°: {zh_model.get('size_gb', 0):.1f}GB")
                print(f"    è´¨é‡ä¿æŒ: {zh_model.get('quality_retention', 0):.1%}")
                
                print(f"  è‹±æ–‡æ¨¡å‹:")
                print(f"    é‡åŒ–ç­‰çº§: {en_model.get('quantization', 'unknown')}")
                print(f"    æ¨¡å‹å¤§å°: {en_model.get('size_gb', 0):.1f}GB")
                print(f"    è´¨é‡ä¿æŒ: {en_model.get('quality_retention', 0):.1%}")
            else:
                print(f"  âŒ é”™è¯¯: {rec['error']}")
        
        # éªŒè¯ç»“æœ
        if "validation" in self.test_results:
            print(f"\nâœ… éªŒè¯ç»“æœ:")
            validation = self.test_results["validation"]
            
            checks = [
                ("GPUæ£€æµ‹å·¥ä½œ", validation.get("gpu_detection_working", False)),
                ("æ€§èƒ½ç­‰çº§åˆç†", validation.get("performance_scaling", False)),
                ("é‡åŒ–ç­‰çº§é€‚å½“", validation.get("quantization_appropriate", False)),
                ("GPUåŠ é€Ÿè€ƒè™‘", validation.get("gpu_acceleration_considered", False))
            ]
            
            for check_name, passed in checks:
                status = "âœ…" if passed else "âŒ"
                print(f"  {status} {check_name}")
            
            if validation.get("issues"):
                print(f"\nâš ï¸ å‘ç°çš„é—®é¢˜:")
                for issue in validation["issues"]:
                    print(f"  â€¢ {issue}")
        
        print("\n" + "="*80)
    
    def save_report(self, output_path: Optional[Path] = None) -> Path:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        if output_path is None:
            output_path = Path("logs") / f"gpu_detection_test_{int(time.time())}.json"
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        return output_path


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” GPUæ£€æµ‹è¯¦ç»†æµ‹è¯•å·¥å…·")
    print("="*50)
    
    tester = GPUDetectionTester()
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_comprehensive_gpu_tests()
    
    # æ˜¾ç¤ºè¯¦ç»†æŠ¥å‘Š
    tester.print_detailed_report()
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = tester.save_report()
    
    if results["success"]:
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        return 0
    else:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
