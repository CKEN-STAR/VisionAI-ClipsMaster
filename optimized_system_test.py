#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster ä¼˜åŒ–ç³»ç»Ÿé›†æˆæµ‹è¯•
éªŒè¯æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ï¼šæ¨¡åž‹è®­ç»ƒã€GPU/CPUå…¼å®¹æ€§ã€è·¯å¾„ç®¡ç†
"""

import os
import sys
import json
import time
import tempfile
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class OptimizedSystemTest:
    """ä¼˜åŒ–ç³»ç»Ÿé›†æˆæµ‹è¯•"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•"""
        self.test_start_time = time.time()
        self.test_results = []
        self.logger = self._setup_logger()
        self.temp_dir = Path(tempfile.mkdtemp(prefix="optimized_test_"))
        
        self.logger.info("ðŸš€ ä¼˜åŒ–ç³»ç»Ÿé›†æˆæµ‹è¯•å¼€å§‹")
        self.logger.info(f"ðŸ“ æµ‹è¯•ç›®å½•: {self.temp_dir}")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('optimized_system_test.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def test_enhanced_trainer(self) -> Dict[str, Any]:
        """æµ‹è¯•å¢žå¼ºè®­ç»ƒå™¨"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•1: å¢žå¼ºè®­ç»ƒå™¨åŠŸèƒ½éªŒè¯")
        self.logger.info("=" * 60)
        
        test_result = {
            "test_name": "enhanced_trainer",
            "start_time": time.time(),
            "status": "running",
            "details": {}
        }
        
        try:
            # å¯¼å…¥å¢žå¼ºè®­ç»ƒå™¨
            from src.training.enhanced_trainer import EnhancedTrainer
            
            # åˆ›å»ºè®­ç»ƒå™¨å®žä¾‹
            trainer = EnhancedTrainer(use_gpu=None)  # è‡ªåŠ¨æ£€æµ‹
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = [
                {"original": "è¿™æ˜¯ä¸€ä¸ªæ™®é€šçš„å‰§æƒ…æè¿°", "viral": "éœ‡æ’¼ï¼è¿™ä¸ªå‰§æƒ…è®©äººæ¬²ç½¢ä¸èƒ½ï¼"},
                {"original": "è§’è‰²ä¹‹é—´çš„å¯¹è¯å¾ˆå¹³æ·¡", "viral": "ç»äº†ï¼è¿™æ®µå¯¹è¯å¤ªæœ‰æ·±åº¦äº†ï¼"},
                {"original": "æ•…äº‹å‘å±•æ¯”è¾ƒç¼“æ…¢", "viral": "èŠ‚å¥å®Œç¾Žï¼æ¯ä¸€ç§’éƒ½æ˜¯é«˜æ½®ï¼"},
                {"original": "ç»“å±€è¿˜ç®—ä¸é”™", "viral": "ç¥žç»“å±€ï¼å®Œå…¨æ²¡æƒ³åˆ°ä¼šè¿™æ ·ï¼"},
                {"original": "æ¼”å‘˜è¡¨æ¼”ä¸€èˆ¬", "viral": "æ¼”æŠ€ç‚¸è£‚ï¼æ¯ä¸ªè¡¨æƒ…éƒ½æ˜¯æˆï¼"}
            ] * 10  # æ‰©å±•åˆ°50ä¸ªæ ·æœ¬
            
            # æ‰§è¡Œè®­ç»ƒ
            def progress_callback(progress, message):
                self.logger.info(f"è®­ç»ƒè¿›åº¦: {progress:.1%} - {message}")
                return True
            
            training_result = trainer.train(test_data, progress_callback)
            
            # éªŒè¯ç»“æžœ
            if training_result["success"]:
                accuracy = training_result["final_accuracy"]
                test_result["status"] = "passed" if accuracy >= 0.8 else "partial"
                test_result["details"] = {
                    "accuracy": accuracy,
                    "device": training_result["device"],
                    "training_time": training_result["training_time"],
                    "epochs": training_result["epochs_completed"]
                }
                
                if accuracy >= 0.8:
                    self.logger.info(f"âœ… è®­ç»ƒå™¨æµ‹è¯•é€šè¿‡: å‡†ç¡®çŽ‡ {accuracy:.2%}")
                else:
                    self.logger.warning(f"âš ï¸ è®­ç»ƒå™¨éƒ¨åˆ†é€šè¿‡: å‡†ç¡®çŽ‡ {accuracy:.2%}")
            else:
                test_result["status"] = "failed"
                test_result["error"] = training_result.get("error", "æœªçŸ¥é”™è¯¯")
                self.logger.error(f"âŒ è®­ç»ƒå™¨æµ‹è¯•å¤±è´¥: {test_result['error']}")
                
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            self.logger.error(f"âŒ è®­ç»ƒå™¨æµ‹è¯•å¼‚å¸¸: {e}")
        
        test_result["duration"] = time.time() - test_result["start_time"]
        return test_result
    
    def test_gpu_cpu_manager(self) -> Dict[str, Any]:
        """æµ‹è¯•GPU/CPUç®¡ç†å™¨"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•2: GPU/CPUå…¼å®¹æ€§ç®¡ç†å™¨")
        self.logger.info("=" * 60)
        
        test_result = {
            "test_name": "gpu_cpu_manager",
            "start_time": time.time(),
            "status": "running",
            "details": {}
        }
        
        try:
            # å¯¼å…¥GPU/CPUç®¡ç†å™¨
            from src.core.gpu_cpu_manager import GPUCPUManager
            
            # åˆ›å»ºç®¡ç†å™¨å®žä¾‹
            manager = GPUCPUManager()
            
            # èŽ·å–ç³»ç»ŸæŠ¥å‘Š
            system_report = manager.get_system_report()
            
            # èŽ·å–æœ€ä¼˜é…ç½®
            optimal_config = manager.get_optimal_config("training")
            
            # è‡ªåŠ¨é…ç½®PyTorch
            torch_config = manager.auto_configure_torch()
            
            # æ£€æŸ¥å…¼å®¹æ€§
            requirements = {"memory_gb": 4, "cuda_version": "11.0"}
            compatibility = manager.check_compatibility(requirements)
            
            test_result["status"] = "passed"
            test_result["details"] = {
                "recommended_device": system_report["recommended_device"],
                "gpu_available": system_report["gpu_info"]["cuda_available"],
                "gpu_count": system_report["gpu_info"]["gpu_count"],
                "cpu_cores": system_report["cpu_info"]["cores"],
                "memory_gb": system_report["system_info"]["memory_total_gb"],
                "torch_config_success": torch_config["success"],
                "optimal_batch_size": optimal_config["batch_size"],
                "compatibility": compatibility["compatible"]
            }
            
            self.logger.info(f"âœ… GPU/CPUç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
            self.logger.info(f"ðŸ“± æŽ¨èè®¾å¤‡: {system_report['recommended_device']}")
            self.logger.info(f"âš™ï¸ æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {optimal_config['batch_size']}")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            self.logger.error(f"âŒ GPU/CPUç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        
        test_result["duration"] = time.time() - test_result["start_time"]
        return test_result
    
    def test_path_manager(self) -> Dict[str, Any]:
        """æµ‹è¯•è·¯å¾„ç®¡ç†å™¨"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•3: è·¯å¾„ç®¡ç†å™¨åŠŸèƒ½éªŒè¯")
        self.logger.info("=" * 60)
        
        test_result = {
            "test_name": "path_manager",
            "start_time": time.time(),
            "status": "running",
            "details": {}
        }
        
        try:
            # å¯¼å…¥è·¯å¾„ç®¡ç†å™¨
            from src.core.path_manager import PathManager
            
            # åˆ›å»ºç®¡ç†å™¨å®žä¾‹
            manager = PathManager()
            
            # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
            test_file = self.temp_dir / "test_video.mp4"
            test_file.write_text("test video content")
            
            # æµ‹è¯•è·¯å¾„è§£æž
            resolved_path = manager.resolve_file_path(test_file)
            
            # æµ‹è¯•å¯ç§»æ¤è·¯å¾„
            portable_path = manager.create_portable_path(test_file)
            restored_path = manager.resolve_portable_path(portable_path)
            
            # æµ‹è¯•é¡¹ç›®ç»“æž„éªŒè¯
            validation = manager.validate_project_structure()
            
            # æµ‹è¯•è·¯å¾„æ˜ å°„
            file_list = [str(test_file), "nonexistent.mp4"]
            path_mapping = manager.create_path_mapping(file_list)
            
            # èŽ·å–è·¯å¾„æŠ¥å‘Š
            path_report = manager.get_path_report()
            
            test_result["status"] = "passed"
            test_result["details"] = {
                "path_resolution_success": resolved_path is not None,
                "portable_path_created": portable_path is not None,
                "project_structure_valid": validation["valid"],
                "missing_files_count": len(validation["missing_files"]),
                "path_mapping_success": len(path_mapping) > 0,
                "cache_size": path_report["cache_size"]
            }
            
            self.logger.info(f"âœ… è·¯å¾„ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
            self.logger.info(f"ðŸ“ é¡¹ç›®ç»“æž„: {'æœ‰æ•ˆ' if validation['valid'] else 'éœ€è¦ä¿®å¤'}")
            self.logger.info(f"ðŸ” è·¯å¾„ç¼“å­˜: {path_report['cache_size']} é¡¹")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            self.logger.error(f"âŒ è·¯å¾„ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        
        test_result["duration"] = time.time() - test_result["start_time"]
        return test_result
    
    def test_integrated_workflow(self) -> Dict[str, Any]:
        """æµ‹è¯•é›†æˆå·¥ä½œæµç¨‹"""
        self.logger.info("=" * 60)
        self.logger.info("æµ‹è¯•4: é›†æˆå·¥ä½œæµç¨‹éªŒè¯")
        self.logger.info("=" * 60)
        
        test_result = {
            "test_name": "integrated_workflow",
            "start_time": time.time(),
            "status": "running",
            "details": {}
        }
        
        try:
            # è¿è¡ŒåŽŸæœ‰çš„ç«¯åˆ°ç«¯æµ‹è¯•
            from complete_e2e_integration_test import CompleteE2EIntegrationTester
            
            e2e_tester = CompleteE2EIntegrationTester()
            
            # æ‰§è¡Œæ ¸å¿ƒæ­¥éª¤
            step1 = e2e_tester.create_realistic_test_data()
            step2 = e2e_tester.test_subtitle_understanding_and_script_reconstruction()
            step3 = e2e_tester.test_viral_subtitle_generation()
            step4 = e2e_tester.test_video_editing_processing()
            step5 = e2e_tester.test_jianying_project_file_generation()
            
            # è¯„ä¼°ç»“æžœ
            all_steps = [step1, step2, step3, step4, step5]
            success_count = sum(1 for step in all_steps if step.get("status") == "success")
            success_rate = success_count / len(all_steps)
            
            test_result["status"] = "passed" if success_rate >= 0.8 else "partial"
            test_result["details"] = {
                "total_steps": len(all_steps),
                "successful_steps": success_count,
                "success_rate": success_rate,
                "step_results": [step.get("status", "unknown") for step in all_steps]
            }
            
            if success_rate >= 0.8:
                self.logger.info(f"âœ… é›†æˆå·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡: {success_rate:.1%}")
            else:
                self.logger.warning(f"âš ï¸ é›†æˆå·¥ä½œæµç¨‹éƒ¨åˆ†é€šè¿‡: {success_rate:.1%}")
            
            # æ¸…ç†æµ‹è¯•æ•°æ®
            e2e_tester.cleanup_test_files()
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            self.logger.error(f"âŒ é›†æˆå·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        
        test_result["duration"] = time.time() - test_result["start_time"]
        return test_result
    
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.logger.info("ðŸŽ¯ å¼€å§‹ä¼˜åŒ–ç³»ç»Ÿé›†æˆæµ‹è¯•")
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        test_methods = [
            self.test_enhanced_trainer,
            self.test_gpu_cpu_manager,
            self.test_path_manager,
            self.test_integrated_workflow
        ]
        
        for test_method in test_methods:
            try:
                result = test_method()
                self.test_results.append(result)
            except Exception as e:
                self.logger.error(f"æµ‹è¯•æ–¹æ³• {test_method.__name__} æ‰§è¡Œå¤±è´¥: {e}")
                self.test_results.append({
                    "test_name": test_method.__name__,
                    "status": "failed",
                    "error": str(e),
                    "duration": 0
                })
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        return self.generate_final_report()
    
    def generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r["status"] == "passed")
        partial_tests = sum(1 for r in self.test_results if r["status"] == "partial")
        failed_tests = sum(1 for r in self.test_results if r["status"] == "failed")
        
        success_rate = (passed_tests + partial_tests * 0.5) / total_tests if total_tests > 0 else 0
        
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "partial_tests": partial_tests,
                "failed_tests": failed_tests,
                "success_rate": success_rate,
                "total_duration": time.time() - self.test_start_time
            },
            "test_results": self.test_results,
            "timestamp": datetime.now().isoformat(),
            "status": "ä¼˜ç§€" if success_rate >= 0.95 else "è‰¯å¥½" if success_rate >= 0.8 else "éœ€è¦æ”¹è¿›"
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"optimized_system_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‘˜è¦
        self.logger.info("=" * 80)
        self.logger.info("ðŸŽ‰ ä¼˜åŒ–ç³»ç»Ÿé›†æˆæµ‹è¯•å®Œæˆ")
        self.logger.info("=" * 80)
        self.logger.info(f"ðŸ“Š æ€»æµ‹è¯•æ•°: {total_tests}")
        self.logger.info(f"âœ… é€šè¿‡: {passed_tests}")
        self.logger.info(f"âš ï¸ éƒ¨åˆ†é€šè¿‡: {partial_tests}")
        self.logger.info(f"âŒ å¤±è´¥: {failed_tests}")
        self.logger.info(f"ðŸŽ¯ æˆåŠŸçŽ‡: {success_rate:.1%}")
        self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {report['test_summary']['total_duration']:.2f}ç§’")
        self.logger.info(f"ðŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
        return report
    
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•çŽ¯å¢ƒ"""
        try:
            import shutil
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)
                self.logger.info(f"ðŸ§¹ æµ‹è¯•ç›®å½•å·²æ¸…ç†: {self.temp_dir}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ¸…ç†æµ‹è¯•ç›®å½•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    tester = OptimizedSystemTest()
    
    try:
        report = tester.run_all_tests()
        return 0 if report["test_summary"]["success_rate"] >= 0.95 else 1
    finally:
        tester.cleanup()

if __name__ == "__main__":
    sys.exit(main())
