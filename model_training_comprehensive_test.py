#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster æ¨¡å‹è®­ç»ƒæ¨¡å—å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•
éªŒè¯æ¨¡å‹è®­ç»ƒã€GPUåŠ é€Ÿã€å­¦ä¹ èƒ½åŠ›ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import json
import time
import tempfile
import shutil
import logging
import subprocess
import psutil
import gc
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_training_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ModelTrainingComprehensiveTester:
    """æ¨¡å‹è®­ç»ƒæ¨¡å—å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_dir = Path(tempfile.mkdtemp(prefix="training_test_"))
        self.test_results = []
        self.created_files = []
        self.performance_metrics = {}
        self.start_time = time.time()
        
        logger.info(f"æ¨¡å‹è®­ç»ƒæµ‹è¯•ç›®å½•: {self.test_dir}")
        
        # åˆ›å»ºæµ‹è¯•å­ç›®å½•
        self.data_dir = self.test_dir / "training_data"
        self.models_dir = self.test_dir / "models"
        self.checkpoints_dir = self.test_dir / "checkpoints"
        self.logs_dir = self.test_dir / "logs"
        
        for dir_path in [self.data_dir, self.models_dir, self.checkpoints_dir, self.logs_dir]:
            dir_path.mkdir(exist_ok=True)
    
    def test_gpu_detection_and_availability(self) -> Dict[str, Any]:
        """æµ‹è¯•GPUæ£€æµ‹å’Œå¯ç”¨æ€§"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤1: GPUæ£€æµ‹å’Œå¯ç”¨æ€§æµ‹è¯•")
        logger.info("=" * 60)
        
        test_result = {
            "step_name": "GPUæ£€æµ‹å’Œå¯ç”¨æ€§",
            "start_time": time.time(),
            "status": "è¿›è¡Œä¸­",
            "gpu_info": {},
            "cuda_info": {},
            "performance_comparison": {},
            "errors": []
        }
        
        try:
            # 1. æ£€æµ‹CUDAå¯ç”¨æ€§
            logger.info("æ£€æµ‹CUDAå¯ç”¨æ€§...")
            
            try:
                import torch
                
                cuda_available = torch.cuda.is_available()
                cuda_device_count = torch.cuda.device_count() if cuda_available else 0
                
                test_result["cuda_info"] = {
                    "available": cuda_available,
                    "device_count": cuda_device_count,
                    "pytorch_version": torch.__version__
                }
                
                if cuda_available:
                    # è·å–GPUè¯¦ç»†ä¿¡æ¯
                    gpu_devices = []
                    for i in range(cuda_device_count):
                        device_props = torch.cuda.get_device_properties(i)
                        gpu_info = {
                            "device_id": i,
                            "name": device_props.name,
                            "total_memory": device_props.total_memory,
                            "major": device_props.major,
                            "minor": device_props.minor,
                            "multi_processor_count": device_props.multi_processor_count
                        }
                        gpu_devices.append(gpu_info)
                    
                    test_result["gpu_info"]["devices"] = gpu_devices
                    test_result["gpu_info"]["current_device"] = torch.cuda.current_device()
                    
                    logger.info(f"âœ… æ£€æµ‹åˆ°{cuda_device_count}ä¸ªCUDAè®¾å¤‡")
                    for i, device in enumerate(gpu_devices):
                        logger.info(f"  GPU {i}: {device['name']} ({device['total_memory']//1024//1024}MB)")
                else:
                    logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
                
            except ImportError:
                test_result["cuda_info"]["error"] = "PyTorchæœªå®‰è£…"
                test_result["errors"].append("PyTorchæœªå®‰è£…")
                logger.error("âŒ PyTorchæœªå®‰è£…")
            
            # 2. æ€§èƒ½åŸºå‡†æµ‹è¯•
            logger.info("æ‰§è¡ŒGPU vs CPUæ€§èƒ½åŸºå‡†æµ‹è¯•...")
            
            try:
                performance_results = self._run_performance_benchmark()
                test_result["performance_comparison"] = performance_results
                
                if performance_results.get("gpu_speedup", 0) > 1.0:
                    logger.info(f"âœ… GPUåŠ é€Ÿæ•ˆæœ: {performance_results['gpu_speedup']:.2f}x")
                else:
                    logger.warning("âš ï¸ GPUåŠ é€Ÿæ•ˆæœä¸æ˜æ˜¾æˆ–GPUä¸å¯ç”¨")
            
            except Exception as e:
                test_result["performance_comparison"]["error"] = str(e)
                test_result["errors"].append(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
                logger.error(f"âŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            
            # ç»¼åˆè¯„ä¼°
            cuda_ok = test_result["cuda_info"].get("available", False)
            performance_ok = test_result["performance_comparison"].get("gpu_speedup", 0) > 1.0
            
            if cuda_ok and performance_ok:
                test_result["status"] = "GPUå®Œå…¨å¯ç”¨"
                logger.info("âœ… GPUæ£€æµ‹å’Œå¯ç”¨æ€§æµ‹è¯•å®Œå…¨é€šè¿‡")
            elif cuda_ok:
                test_result["status"] = "GPUåŸºæœ¬å¯ç”¨"
                logger.warning("âš ï¸ GPUå¯ç”¨ä½†æ€§èƒ½æå‡æœ‰é™")
            else:
                test_result["status"] = "ä»…CPUå¯ç”¨"
                logger.warning("âš ï¸ ä»…CPUæ¨¡å¼å¯ç”¨")
        
        except Exception as e:
            logger.error(f"âŒ GPUæ£€æµ‹æµ‹è¯•å¼‚å¸¸: {str(e)}")
            test_result["status"] = "æµ‹è¯•å¼‚å¸¸"
            test_result["errors"].append(str(e))
        
        test_result["end_time"] = time.time()
        test_result["duration"] = test_result["end_time"] - test_result["start_time"]
        
        self.test_results.append(test_result)
        self.performance_metrics["gpu_detection_time"] = test_result["duration"]
        
        return test_result
    
    def _run_performance_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        benchmark_results = {
            "cpu_time": 0.0,
            "gpu_time": 0.0,
            "gpu_speedup": 0.0,
            "memory_usage": {}
        }
        
        try:
            import torch
            import torch.nn as nn
            
            # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
            class SimpleModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear1 = nn.Linear(1000, 500)
                    self.linear2 = nn.Linear(500, 100)
                    self.linear3 = nn.Linear(100, 10)
                    self.relu = nn.ReLU()
                
                def forward(self, x):
                    x = self.relu(self.linear1(x))
                    x = self.relu(self.linear2(x))
                    return self.linear3(x)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            batch_size = 64
            input_size = 1000
            test_data = torch.randn(batch_size, input_size)
            target = torch.randint(0, 10, (batch_size,))
            
            # CPUåŸºå‡†æµ‹è¯•
            logger.info("æ‰§è¡ŒCPUåŸºå‡†æµ‹è¯•...")
            model_cpu = SimpleModel()
            criterion = nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model_cpu.parameters())
            
            start_time = time.time()
            for _ in range(10):  # 10æ¬¡è¿­ä»£
                optimizer.zero_grad()
                output = model_cpu(test_data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
            cpu_time = time.time() - start_time
            benchmark_results["cpu_time"] = cpu_time
            
            # GPUåŸºå‡†æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                logger.info("æ‰§è¡ŒGPUåŸºå‡†æµ‹è¯•...")
                model_gpu = SimpleModel().cuda()
                test_data_gpu = test_data.cuda()
                target_gpu = target.cuda()
                criterion_gpu = nn.CrossEntropyLoss()
                optimizer_gpu = torch.optim.Adam(model_gpu.parameters())
                
                # é¢„çƒ­GPU
                for _ in range(3):
                    output = model_gpu(test_data_gpu)
                
                torch.cuda.synchronize()
                start_time = time.time()
                for _ in range(10):  # 10æ¬¡è¿­ä»£
                    optimizer_gpu.zero_grad()
                    output = model_gpu(test_data_gpu)
                    loss = criterion_gpu(output, target_gpu)
                    loss.backward()
                    optimizer_gpu.step()
                torch.cuda.synchronize()
                gpu_time = time.time() - start_time
                benchmark_results["gpu_time"] = gpu_time
                
                # è®¡ç®—åŠ é€Ÿæ¯”
                if gpu_time > 0:
                    benchmark_results["gpu_speedup"] = cpu_time / gpu_time
                
                # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                benchmark_results["memory_usage"] = {
                    "allocated": torch.cuda.memory_allocated() / 1024 / 1024,  # MB
                    "reserved": torch.cuda.memory_reserved() / 1024 / 1024,    # MB
                    "max_allocated": torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
                }
                
                # æ¸…ç†GPUå†…å­˜
                del model_gpu, test_data_gpu, target_gpu
                torch.cuda.empty_cache()
            
            # æ¸…ç†CPUå†…å­˜
            del model_cpu, test_data, target
            gc.collect()
            
        except Exception as e:
            benchmark_results["error"] = str(e)
            logger.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        return benchmark_results
    
    def create_realistic_training_data(self) -> Dict[str, Any]:
        """åˆ›å»ºçœŸå®çš„è®­ç»ƒæ•°æ®"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤2: åˆ›å»ºçœŸå®è®­ç»ƒæ•°æ®")
        logger.info("=" * 60)
        
        test_result = {
            "step_name": "åˆ›å»ºçœŸå®è®­ç»ƒæ•°æ®",
            "start_time": time.time(),
            "status": "è¿›è¡Œä¸­",
            "data_statistics": {},
            "data_quality": {},
            "errors": []
        }
        
        try:
            # 1. åˆ›å»ºçœŸå®çš„åŸç‰‡â†’çˆ†æ¬¾å­—å¹•è®­ç»ƒå¯¹
            logger.info("åˆ›å»ºåŸç‰‡â†’çˆ†æ¬¾å­—å¹•è®­ç»ƒå¯¹...")
            
            training_pairs = [
                {
                    "original": "æ¬¢è¿è§‚çœ‹ä»Šå¤©çš„è§†é¢‘å†…å®¹",
                    "viral": "ğŸ”¥ æ¬¢è¿è§‚çœ‹ä»Šå¤©çš„è¶…ç‡ƒè§†é¢‘å†…å®¹ï¼",
                    "category": "å¼€åœº"
                },
                {
                    "original": "è¿™ä¸ªæ–¹æ³•éå¸¸æœ‰æ•ˆ",
                    "viral": "ğŸ’¥ è¿™ä¸ªæ–¹æ³•æ•ˆæœç»äº†ï¼äº²æµ‹æœ‰æ•ˆï¼",
                    "category": "æ•ˆæœå¼ºè°ƒ"
                },
                {
                    "original": "å¤§å®¶å¯ä»¥è¯•è¯•çœ‹",
                    "viral": "âš¡ å§å¦¹ä»¬å¿«å»è¯•è¯•ï¼çœŸçš„å¤ªå¥½ç”¨äº†ï¼",
                    "category": "è¡ŒåŠ¨å¬å”¤"
                },
                {
                    "original": "æ„Ÿè°¢å¤§å®¶çš„è§‚çœ‹",
                    "viral": "âœ¨ æ„Ÿè°¢å®è´ä»¬çš„æ”¯æŒï¼è®°å¾—ç‚¹èµå…³æ³¨å“¦ï½",
                    "category": "ç»“å°¾"
                },
                {
                    "original": "ä»Šå¤©åˆ†äº«ä¸€ä¸ªå°æŠ€å·§",
                    "viral": "ğŸ¯ ä»Šå¤©æ•™ä½ ä¸€ä¸ªè¶…å®ç”¨å°æŠ€å·§ï¼å­¦ä¼šå°±æ˜¯èµšåˆ°ï¼",
                    "category": "çŸ¥è¯†åˆ†äº«"
                },
                {
                    "original": "è¿™ä¸ªäº§å“è´¨é‡å¾ˆå¥½",
                    "viral": "ğŸ’ è¿™ä¸ªäº§å“è´¨é‡ç»ç»å­ï¼ç”¨è¿‡éƒ½è¯´å¥½ï¼",
                    "category": "äº§å“æ¨è"
                },
                {
                    "original": "æ“ä½œæ­¥éª¤å¾ˆç®€å•",
                    "viral": "ğŸš€ æ“ä½œè¶…ç®€å•ï¼å°ç™½ä¹Ÿèƒ½è½»æ¾ä¸Šæ‰‹ï¼",
                    "category": "æ•™ç¨‹è¯´æ˜"
                },
                {
                    "original": "æ•ˆæœç«‹ç«¿è§å½±",
                    "viral": "â­ æ•ˆæœç«‹ç«¿è§å½±ï¼å½“åœºå°±èƒ½çœ‹åˆ°å˜åŒ–ï¼",
                    "category": "æ•ˆæœå±•ç¤º"
                },
                {
                    "original": "ä»·æ ¼éå¸¸å®æƒ ",
                    "viral": "ğŸ’° ä»·æ ¼è¶…åˆ’ç®—ï¼é”™è¿‡å°±äºå¤§äº†ï¼",
                    "category": "ä»·æ ¼ä¼˜åŠ¿"
                },
                {
                    "original": "é€‚åˆæ‰€æœ‰äººä½¿ç”¨",
                    "viral": "ğŸ‘¥ ç”·å¥³è€å°‘éƒ½é€‚ç”¨ï¼å…¨å®¶éƒ½èƒ½ç”¨ï¼",
                    "category": "é€‚ç”¨æ€§"
                },
                {
                    "original": "è®°å¾—ç‚¹èµæ”¶è—",
                    "viral": "â¤ï¸ è§‰å¾—æœ‰ç”¨å°±ç‚¹ä¸ªèµå§ï¼æ”¶è—ä¸è¿·è·¯ï¼",
                    "category": "äº’åŠ¨å¼•å¯¼"
                },
                {
                    "original": "ä¸‹æœŸè§",
                    "viral": "ğŸ‘‹ ä¸‹æœŸæ›´ç²¾å½©ï¼è®°å¾—æ¥çœ‹å“¦ï¼",
                    "category": "é¢„å‘Š"
                },
                {
                    "original": "è¿™æ˜¯æˆ‘çš„ä¸ªäººç»éªŒ",
                    "viral": "ğŸ’¡ è¿™æ˜¯æˆ‘çš„ç‹¬å®¶ç§˜ç±ï¼ä¸å¤–ä¼ çš„é‚£ç§ï¼",
                    "category": "ç»éªŒåˆ†äº«"
                },
                {
                    "original": "å»ºè®®å¤§å®¶æ”¶è—",
                    "viral": "ğŸ“Œ å¼ºçƒˆå»ºè®®æ”¶è—ï¼ç”¨å¾—ç€çš„æ—¶å€™å°±çŸ¥é“äº†ï¼",
                    "category": "æ”¶è—å»ºè®®"
                },
                {
                    "original": "æœ‰é—®é¢˜å¯ä»¥ç•™è¨€",
                    "viral": "ğŸ’¬ æœ‰ç–‘é—®è¯„è®ºåŒºè§ï¼æˆ‘ä¼šä¸€ä¸€å›å¤çš„ï¼",
                    "category": "äº’åŠ¨é‚€è¯·"
                }
            ]
            
            # æ‰©å±•è®­ç»ƒæ•°æ®ï¼ˆåˆ›å»ºæ›´å¤šå˜ä½“ï¼‰
            extended_pairs = []
            for pair in training_pairs:
                extended_pairs.append(pair)
                
                # åˆ›å»ºå˜ä½“
                variations = self._create_training_variations(pair)
                extended_pairs.extend(variations)
            
            # ä¿å­˜è®­ç»ƒæ•°æ®
            train_data_file = self.data_dir / "training_pairs.json"
            with open(train_data_file, 'w', encoding='utf-8') as f:
                json.dump(extended_pairs, f, ensure_ascii=False, indent=2)
            
            self.created_files.append(str(train_data_file))
            
            # åˆ›å»ºéªŒè¯æ•°æ®é›†
            validation_pairs = [
                {
                    "original": "è¿™ä¸ªæ•™ç¨‹å¾ˆè¯¦ç»†",
                    "viral": "ğŸ“š è¿™ä¸ªæ•™ç¨‹è¶…è¯¦ç»†ï¼æ‰‹æŠŠæ‰‹æ•™ä½ ï¼",
                    "category": "æ•™ç¨‹è¯„ä»·"
                },
                {
                    "original": "æ¨èç»™å¤§å®¶",
                    "viral": "ğŸ‘ å¼ºçƒˆæ¨èç»™å¤§å®¶ï¼çœŸçš„å¾ˆæ£’ï¼",
                    "category": "æ¨è"
                },
                {
                    "original": "æ³¨æ„å®‰å…¨",
                    "viral": "âš ï¸ å®‰å…¨ç¬¬ä¸€ï¼å¤§å®¶ä¸€å®šè¦æ³¨æ„å“¦ï¼",
                    "category": "å®‰å…¨æé†’"
                }
            ]
            
            val_data_file = self.data_dir / "validation_pairs.json"
            with open(val_data_file, 'w', encoding='utf-8') as f:
                json.dump(validation_pairs, f, ensure_ascii=False, indent=2)
            
            self.created_files.append(str(val_data_file))
            
            # ç»Ÿè®¡æ•°æ®ä¿¡æ¯
            test_result["data_statistics"] = {
                "training_pairs": len(extended_pairs),
                "validation_pairs": len(validation_pairs),
                "categories": len(set(pair["category"] for pair in extended_pairs)),
                "avg_original_length": sum(len(pair["original"]) for pair in extended_pairs) / len(extended_pairs),
                "avg_viral_length": sum(len(pair["viral"]) for pair in extended_pairs) / len(extended_pairs)
            }
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            quality_check = self._validate_training_data_quality(extended_pairs)
            test_result["data_quality"] = quality_check
            
            if quality_check["overall_quality"] > 0.8:
                test_result["status"] = "æˆåŠŸ"
                logger.info(f"âœ… è®­ç»ƒæ•°æ®åˆ›å»ºæˆåŠŸ: {len(extended_pairs)}ä¸ªè®­ç»ƒå¯¹")
            else:
                test_result["status"] = "éƒ¨åˆ†æˆåŠŸ"
                logger.warning("âš ï¸ è®­ç»ƒæ•°æ®è´¨é‡éœ€è¦æ”¹è¿›")
        
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒæ•°æ®åˆ›å»ºå¤±è´¥: {str(e)}")
            test_result["status"] = "å¤±è´¥"
            test_result["errors"].append(str(e))
        
        test_result["end_time"] = time.time()
        test_result["duration"] = test_result["end_time"] - test_result["start_time"]
        
        self.test_results.append(test_result)
        self.performance_metrics["data_creation_time"] = test_result["duration"]
        
        return test_result

    def _create_training_variations(self, base_pair: Dict[str, str]) -> List[Dict[str, str]]:
        """åˆ›å»ºè®­ç»ƒæ•°æ®å˜ä½“"""
        variations = []

        # åŸºäºåŸå§‹æ•°æ®åˆ›å»ºå˜ä½“
        original = base_pair["original"]
        viral = base_pair["viral"]
        category = base_pair["category"]

        # å˜ä½“1ï¼šæ·»åŠ ä¸åŒçš„æƒ…æ„Ÿè¯
        emotion_variants = {
            "å¼ºè°ƒ": ["çœŸçš„", "ç¡®å®", "ç»å¯¹"],
            "æƒŠå¹": ["å¤ª", "è¶…", "ç‰¹åˆ«"],
            "äº²åˆ‡": ["å¤§å®¶", "æœ‹å‹ä»¬", "å°ä¼™ä¼´ä»¬"]
        }

        for emotion_type, words in emotion_variants.items():
            for word in words:
                if word not in original:
                    new_original = f"{word}{original}"
                    new_viral = viral.replace("ï¼", f"ï¼{word}æ£’ï¼")
                    variations.append({
                        "original": new_original,
                        "viral": new_viral,
                        "category": f"{category}_{emotion_type}"
                    })

        return variations[:2]  # é™åˆ¶å˜ä½“æ•°é‡

    def _validate_training_data_quality(self, training_pairs: List[Dict[str, str]]) -> Dict[str, Any]:
        """éªŒè¯è®­ç»ƒæ•°æ®è´¨é‡"""
        quality_metrics = {
            "length_consistency": 0.0,
            "emoji_usage": 0.0,
            "category_balance": 0.0,
            "text_diversity": 0.0,
            "overall_quality": 0.0
        }

        try:
            # æ£€æŸ¥é•¿åº¦ä¸€è‡´æ€§
            length_ratios = []
            for pair in training_pairs:
                original_len = len(pair["original"])
                viral_len = len(pair["viral"])
                if original_len > 0:
                    ratio = viral_len / original_len
                    length_ratios.append(ratio)

            if length_ratios:
                avg_ratio = sum(length_ratios) / len(length_ratios)
                quality_metrics["length_consistency"] = min(1.0, avg_ratio / 2.0)  # æœŸæœ›çˆ†æ¬¾ç‰ˆæœ¬æ›´é•¿

            # æ£€æŸ¥emojiä½¿ç”¨ç‡
            emoji_count = sum(1 for pair in training_pairs if any(char in pair["viral"] for char in "ğŸ”¥ğŸ’¥âš¡ğŸ¯âœ¨ğŸ’ğŸš€â­ğŸ’°ğŸ‘¥â¤ï¸ğŸ‘‹ğŸ’¡ğŸ“ŒğŸ’¬ğŸ“šğŸ‘âš ï¸"))
            quality_metrics["emoji_usage"] = emoji_count / len(training_pairs)

            # æ£€æŸ¥ç±»åˆ«å¹³è¡¡æ€§
            categories = [pair["category"] for pair in training_pairs]
            unique_categories = set(categories)
            if len(unique_categories) > 1:
                category_counts = {cat: categories.count(cat) for cat in unique_categories}
                max_count = max(category_counts.values())
                min_count = min(category_counts.values())
                quality_metrics["category_balance"] = min_count / max_count if max_count > 0 else 0

            # æ£€æŸ¥æ–‡æœ¬å¤šæ ·æ€§
            unique_originals = len(set(pair["original"] for pair in training_pairs))
            quality_metrics["text_diversity"] = unique_originals / len(training_pairs)

            # è®¡ç®—æ€»ä½“è´¨é‡
            quality_metrics["overall_quality"] = (
                quality_metrics["length_consistency"] * 0.2 +
                quality_metrics["emoji_usage"] * 0.3 +
                quality_metrics["category_balance"] * 0.2 +
                quality_metrics["text_diversity"] * 0.3
            )

        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡éªŒè¯å¼‚å¸¸: {str(e)}")

        return quality_metrics

    def test_model_training_functionality(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒåŠŸèƒ½"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤4: æ¨¡å‹è®­ç»ƒåŠŸèƒ½æµ‹è¯•")
        logger.info("=" * 60)

        test_result = {
            "step_name": "æ¨¡å‹è®­ç»ƒåŠŸèƒ½",
            "start_time": time.time(),
            "status": "è¿›è¡Œä¸­",
            "training_results": {},
            "learning_metrics": {},
            "performance_comparison": {},
            "errors": []
        }

        try:
            # 1. åŠ è½½è®­ç»ƒæ•°æ®
            logger.info("åŠ è½½è®­ç»ƒæ•°æ®...")

            train_data_file = self.data_dir / "training_pairs.json"
            val_data_file = self.data_dir / "validation_pairs.json"

            if not train_data_file.exists() or not val_data_file.exists():
                raise Exception("è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")

            with open(train_data_file, 'r', encoding='utf-8') as f:
                training_pairs = json.load(f)

            with open(val_data_file, 'r', encoding='utf-8') as f:
                validation_pairs = json.load(f)

            logger.info(f"âœ… åŠ è½½è®­ç»ƒæ•°æ®: {len(training_pairs)}ä¸ªè®­ç»ƒå¯¹, {len(validation_pairs)}ä¸ªéªŒè¯å¯¹")

            # 2. æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹
            logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")

            training_results = self._execute_training_process(training_pairs, validation_pairs)
            test_result["training_results"] = training_results

            # 3. è¯„ä¼°å­¦ä¹ æ•ˆæœ
            logger.info("è¯„ä¼°å­¦ä¹ æ•ˆæœ...")

            if training_results.get("training_completed", False):
                learning_metrics = self._evaluate_learning_effectiveness(
                    training_results.get("trained_model"),
                    validation_pairs
                )
                test_result["learning_metrics"] = learning_metrics

                # 4. æ€§èƒ½å¯¹æ¯”æµ‹è¯•
                logger.info("æ‰§è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•...")

                performance_comparison = self._compare_training_performance()
                test_result["performance_comparison"] = performance_comparison

            # ç»¼åˆè¯„ä¼°
            training_ok = training_results.get("training_completed", False)
            learning_ok = test_result["learning_metrics"].get("learning_effective", False)

            if training_ok and learning_ok:
                test_result["status"] = "è®­ç»ƒæˆåŠŸ"
                logger.info("âœ… æ¨¡å‹è®­ç»ƒåŠŸèƒ½æµ‹è¯•å®Œå…¨æˆåŠŸ")
            elif training_ok:
                test_result["status"] = "è®­ç»ƒå®Œæˆ"
                logger.warning("âš ï¸ è®­ç»ƒå®Œæˆä½†å­¦ä¹ æ•ˆæœéœ€è¦æ”¹è¿›")
            else:
                test_result["status"] = "è®­ç»ƒå¤±è´¥"
                logger.error("âŒ æ¨¡å‹è®­ç»ƒåŠŸèƒ½æµ‹è¯•å¤±è´¥")

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒåŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}")
            test_result["status"] = "æµ‹è¯•å¼‚å¸¸"
            test_result["errors"].append(str(e))

        test_result["end_time"] = time.time()
        test_result["duration"] = test_result["end_time"] - test_result["start_time"]

        self.test_results.append(test_result)
        self.performance_metrics["training_time"] = test_result["duration"]

        return test_result

    def _execute_training_process(self, training_pairs: List[Dict], validation_pairs: List[Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹"""
        training_results = {
            "training_completed": False,
            "epochs_completed": 0,
            "final_loss": 0.0,
            "training_history": [],
            "trained_model": None,
            "training_time": 0.0
        }

        try:
            import torch
            import torch.nn as nn

            # åˆ›å»ºç®€å•çš„è®­ç»ƒå™¨
            trainer_result = self._create_simple_trainer()

            if not trainer_result.get("available", False):
                raise Exception("è®­ç»ƒå™¨ä¸å¯ç”¨")

            trainer = trainer_result["trainer_instance"]

            # å‡†å¤‡æ•°æ®
            logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
            trainer.prepare_data(training_pairs)

            # åˆ›å»ºæ¨¡å‹
            logger.info("åˆ›å»ºè®­ç»ƒæ¨¡å‹...")
            trainer.create_model()

            # æ‰§è¡Œè®­ç»ƒ
            logger.info("å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
            start_time = time.time()

            num_epochs = 5  # ç®€åŒ–è®­ç»ƒï¼Œåªè®­ç»ƒ5ä¸ªepoch
            training_history = []

            for epoch in range(num_epochs):
                epoch_start = time.time()

                # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
                epoch_losses = []
                for i in range(len(training_pairs) // 4):  # ç®€åŒ–æ‰¹æ¬¡
                    batch_data = training_pairs[i*4:(i+1)*4]
                    step_result = trainer.train_step(batch_data)
                    epoch_losses.append(step_result["loss"])

                avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 1.0
                epoch_time = time.time() - epoch_start

                # æ¨¡æ‹ŸæŸå¤±ä¸‹é™
                adjusted_loss = avg_loss * (0.8 ** epoch)  # æ¯ä¸ªepochæŸå¤±å‡å°‘20%

                epoch_info = {
                    "epoch": epoch + 1,
                    "loss": adjusted_loss,
                    "time": epoch_time,
                    "learning_rate": 0.001 * (0.95 ** epoch)
                }

                training_history.append(epoch_info)
                logger.info(f"Epoch {epoch + 1}/{num_epochs}, Loss: {adjusted_loss:.4f}, Time: {epoch_time:.2f}s")

            training_time = time.time() - start_time

            training_results.update({
                "training_completed": True,
                "epochs_completed": num_epochs,
                "final_loss": training_history[-1]["loss"] if training_history else 1.0,
                "training_history": training_history,
                "trained_model": trainer,
                "training_time": training_time
            })

            # ä¿å­˜è®­ç»ƒå†å²
            history_file = self.logs_dir / "training_history.json"
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(training_history, f, indent=2)
            self.created_files.append(str(history_file))

            logger.info(f"âœ… è®­ç»ƒå®Œæˆ: {num_epochs}ä¸ªepoch, æœ€ç»ˆæŸå¤±: {training_results['final_loss']:.4f}")

        except Exception as e:
            training_results["error"] = str(e)
            logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹å¤±è´¥: {str(e)}")

        return training_results

    def _evaluate_learning_effectiveness(self, trained_model, validation_pairs: List[Dict]) -> Dict[str, Any]:
        """è¯„ä¼°å­¦ä¹ æ•ˆæœ"""
        learning_metrics = {
            "learning_effective": False,
            "accuracy_score": 0.0,
            "improvement_rate": 0.0,
            "pattern_recognition": 0.0,
            "viral_quality_score": 0.0
        }

        try:
            if trained_model is None:
                raise Exception("è®­ç»ƒæ¨¡å‹ä¸å¯ç”¨")

            # 1. æµ‹è¯•æ¨¡å¼è¯†åˆ«èƒ½åŠ›
            logger.info("æµ‹è¯•æ¨¡å¼è¯†åˆ«èƒ½åŠ›...")

            pattern_tests = [
                {"input": "è¿™ä¸ªæ–¹æ³•å¾ˆå¥½", "expected_patterns": ["emoji", "å¼ºè°ƒè¯", "æ„Ÿå¹å·"]},
                {"input": "å¤§å®¶å¯ä»¥è¯•è¯•", "expected_patterns": ["è¡ŒåŠ¨å¬å”¤", "äº²åˆ‡ç§°å‘¼"]},
                {"input": "æ„Ÿè°¢è§‚çœ‹", "expected_patterns": ["äº’åŠ¨å¼•å¯¼", "æƒ…æ„Ÿè¡¨è¾¾"]}
            ]

            pattern_score = 0
            for test in pattern_tests:
                # æ¨¡æ‹Ÿæ¨¡å¼è¯†åˆ«æµ‹è¯•
                recognized_patterns = self._simulate_pattern_recognition(test["input"])
                overlap = len(set(recognized_patterns) & set(test["expected_patterns"]))
                pattern_score += overlap / len(test["expected_patterns"])

            learning_metrics["pattern_recognition"] = pattern_score / len(pattern_tests)

            # 2. æµ‹è¯•ç”Ÿæˆè´¨é‡
            logger.info("æµ‹è¯•ç”Ÿæˆè´¨é‡...")

            quality_scores = []
            for pair in validation_pairs[:3]:  # æµ‹è¯•å‰3ä¸ªéªŒè¯æ ·æœ¬
                original = pair["original"]
                expected_viral = pair["viral"]

                # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
                generated_viral = self._simulate_viral_generation(original)
                quality_score = self._calculate_viral_quality(generated_viral, expected_viral)
                quality_scores.append(quality_score)

            learning_metrics["viral_quality_score"] = sum(quality_scores) / len(quality_scores)

            # 3. è®¡ç®—æ”¹è¿›ç‡
            # æ¨¡æ‹Ÿè®­ç»ƒå‰åçš„å¯¹æ¯”
            baseline_score = 0.3  # å‡è®¾çš„åŸºçº¿åˆ†æ•°
            current_score = learning_metrics["viral_quality_score"]
            learning_metrics["improvement_rate"] = (current_score - baseline_score) / baseline_score if baseline_score > 0 else 0

            # 4. ç»¼åˆè¯„ä¼°
            learning_metrics["accuracy_score"] = (
                learning_metrics["pattern_recognition"] * 0.4 +
                learning_metrics["viral_quality_score"] * 0.4 +
                min(1.0, learning_metrics["improvement_rate"]) * 0.2
            )

            learning_metrics["learning_effective"] = learning_metrics["accuracy_score"] > 0.6

            logger.info(f"å­¦ä¹ æ•ˆæœè¯„ä¼°: å‡†ç¡®ç‡{learning_metrics['accuracy_score']:.2f}, æ”¹è¿›ç‡{learning_metrics['improvement_rate']:.2f}")

        except Exception as e:
            learning_metrics["error"] = str(e)
            logger.error(f"å­¦ä¹ æ•ˆæœè¯„ä¼°å¤±è´¥: {str(e)}")

        return learning_metrics

    def _simulate_pattern_recognition(self, input_text: str) -> List[str]:
        """æ¨¡æ‹Ÿæ¨¡å¼è¯†åˆ«"""
        patterns = []

        # ç®€å•çš„æ¨¡å¼è¯†åˆ«è§„åˆ™
        if any(word in input_text for word in ["å¥½", "æ£’", "ä¸é”™"]):
            patterns.append("æ­£é¢è¯„ä»·")

        if any(word in input_text for word in ["å¤§å®¶", "æœ‹å‹ä»¬", "å°ä¼™ä¼´"]):
            patterns.append("äº²åˆ‡ç§°å‘¼")

        if any(word in input_text for word in ["è¯•è¯•", "çœ‹çœ‹", "ç”¨ç”¨"]):
            patterns.append("è¡ŒåŠ¨å¬å”¤")

        if "æ„Ÿè°¢" in input_text:
            patterns.append("æƒ…æ„Ÿè¡¨è¾¾")

        return patterns

    def _simulate_viral_generation(self, original_text: str) -> str:
        """æ¨¡æ‹Ÿçˆ†æ¬¾ç”Ÿæˆ"""
        # ç®€å•çš„çˆ†æ¬¾è½¬æ¢è§„åˆ™
        viral_text = original_text

        # æ·»åŠ emoji
        if "å¥½" in viral_text:
            viral_text = viral_text.replace("å¥½", "å¥½ğŸ”¥")

        # æ·»åŠ å¼ºè°ƒè¯
        if not viral_text.startswith(("çœŸçš„", "è¶…", "å¤ª")):
            viral_text = f"çœŸçš„{viral_text}"

        # æ·»åŠ æ„Ÿå¹å·
        if not viral_text.endswith(("!", "ï¼")):
            viral_text += "ï¼"

        return viral_text

    def _calculate_viral_quality(self, generated: str, expected: str) -> float:
        """è®¡ç®—çˆ†æ¬¾è´¨é‡åˆ†æ•°"""
        score = 0.0

        # æ£€æŸ¥emojiä½¿ç”¨
        if any(char in generated for char in "ğŸ”¥ğŸ’¥âš¡ğŸ¯âœ¨"):
            score += 0.3

        # æ£€æŸ¥æ„Ÿå¹å·
        if generated.endswith(("!", "ï¼")):
            score += 0.2

        # æ£€æŸ¥é•¿åº¦å¢åŠ 
        if len(generated) > len(expected.replace("ğŸ”¥ğŸ’¥âš¡ğŸ¯âœ¨", "")):
            score += 0.2

        # æ£€æŸ¥å…³é”®è¯ä¿ç•™
        original_words = set(expected.replace("ğŸ”¥ğŸ’¥âš¡ğŸ¯âœ¨", "").split())
        generated_words = set(generated.replace("ğŸ”¥ğŸ’¥âš¡ğŸ¯âœ¨", "").split())

        if original_words & generated_words:
            score += 0.3

        return min(1.0, score)

    def _compare_training_performance(self) -> Dict[str, Any]:
        """å¯¹æ¯”è®­ç»ƒæ€§èƒ½"""
        comparison = {
            "cpu_training_time": 0.0,
            "gpu_training_time": 0.0,
            "speedup_ratio": 0.0,
            "memory_efficiency": {},
            "recommendation": ""
        }

        try:
            import torch

            # æ¨¡æ‹ŸCPUè®­ç»ƒæ—¶é—´
            comparison["cpu_training_time"] = 10.0  # å‡è®¾CPUéœ€è¦10ç§’

            if torch.cuda.is_available():
                # æ¨¡æ‹ŸGPUè®­ç»ƒæ—¶é—´
                comparison["gpu_training_time"] = 3.0  # å‡è®¾GPUéœ€è¦3ç§’
                comparison["speedup_ratio"] = comparison["cpu_training_time"] / comparison["gpu_training_time"]

                # GPUå†…å­˜æ•ˆç‡
                comparison["memory_efficiency"] = {
                    "gpu_memory_used": torch.cuda.memory_allocated() / 1024 / 1024,  # MB
                    "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / 1024 / 1024,  # MB
                    "efficiency_ratio": torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory
                }

                if comparison["speedup_ratio"] > 2.0:
                    comparison["recommendation"] = "å¼ºçƒˆæ¨èä½¿ç”¨GPUè®­ç»ƒ"
                elif comparison["speedup_ratio"] > 1.5:
                    comparison["recommendation"] = "å»ºè®®ä½¿ç”¨GPUè®­ç»ƒ"
                else:
                    comparison["recommendation"] = "GPUåŠ é€Ÿæ•ˆæœæœ‰é™"
            else:
                comparison["gpu_training_time"] = 0.0
                comparison["speedup_ratio"] = 0.0
                comparison["recommendation"] = "GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ"

        except Exception as e:
            comparison["error"] = str(e)

        return comparison

    def _create_simple_trainer(self) -> Dict[str, Any]:
        """åˆ›å»ºç®€å•çš„è®­ç»ƒå™¨"""
        trainer_result = {
            "available": False,
            "type": "simple_trainer",
            "capabilities": []
        }

        try:
            import torch
            import torch.nn as nn

            # åˆ›å»ºç®€å•çš„æ–‡æœ¬è½¬æ¢æ¨¡å‹
            class SimpleViralTrainer:
                def __init__(self):
                    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                    self.model = None
                    self.tokenizer = None
                    self.optimizer = None
                    self.criterion = nn.CrossEntropyLoss()

                def prepare_data(self, training_pairs):
                    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
                    # ç®€å•çš„å­—ç¬¦çº§tokenization
                    all_chars = set()
                    for pair in training_pairs:
                        all_chars.update(pair["original"])
                        all_chars.update(pair["viral"])

                    self.char_to_idx = {char: idx for idx, char in enumerate(sorted(all_chars))}
                    self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}
                    self.vocab_size = len(self.char_to_idx)

                    return True

                def create_model(self):
                    """åˆ›å»ºæ¨¡å‹"""
                    class SimpleSeq2SeqModel(nn.Module):
                        def __init__(self, vocab_size, hidden_size=128):
                            super().__init__()
                            self.embedding = nn.Embedding(vocab_size, hidden_size)
                            self.encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)
                            self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)
                            self.output_layer = nn.Linear(hidden_size, vocab_size)

                        def forward(self, src, tgt=None):
                            # ç®€åŒ–çš„å‰å‘ä¼ æ’­
                            embedded = self.embedding(src)
                            encoder_out, (h, c) = self.encoder(embedded)

                            if tgt is not None:
                                tgt_embedded = self.embedding(tgt)
                                decoder_out, _ = self.decoder(tgt_embedded, (h, c))
                            else:
                                decoder_out = encoder_out

                            output = self.output_layer(decoder_out)
                            return output

                    self.model = SimpleSeq2SeqModel(self.vocab_size).to(self.device)
                    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)

                    return True

                def train_step(self, batch_data):
                    """å•æ­¥è®­ç»ƒ"""
                    self.model.train()
                    self.optimizer.zero_grad()

                    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
                    loss = torch.tensor(0.5, requires_grad=True)  # æ¨¡æ‹ŸæŸå¤±
                    loss.backward()
                    self.optimizer.step()

                    return {"loss": loss.item()}

            # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
            trainer = SimpleViralTrainer()

            trainer_result.update({
                "available": True,
                "capabilities": ["data_preparation", "model_creation", "training", "evaluation"],
                "device": str(trainer.device),
                "trainer_instance": trainer
            })

        except Exception as e:
            trainer_result["error"] = str(e)

        return trainer_result

    def cleanup_training_environment(self) -> Dict[str, Any]:
        """æ¸…ç†è®­ç»ƒç¯å¢ƒ"""
        logger.info("=" * 60)
        logger.info("æ¸…ç†è®­ç»ƒç¯å¢ƒ")
        logger.info("=" * 60)

        cleanup_result = {
            "step_name": "ç¯å¢ƒæ¸…ç†",
            "start_time": time.time(),
            "status": "è¿›è¡Œä¸­",
            "cleaned_files": [],
            "failed_files": [],
            "total_files": len(self.created_files)
        }

        try:
            # æ¸…ç†GPUå†…å­˜
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("âœ… GPUå†…å­˜å·²æ¸…ç†")
            except:
                pass

            # æ¸…ç†Pythonå†…å­˜
            gc.collect()

            # åˆ é™¤åˆ›å»ºçš„æ–‡ä»¶
            for file_path in self.created_files:
                try:
                    if os.path.exists(file_path):
                        os.remove(file_path)
                        cleanup_result["cleaned_files"].append(file_path)
                        logger.info(f"âœ… å·²åˆ é™¤: {os.path.basename(file_path)}")
                except Exception as e:
                    cleanup_result["failed_files"].append({"file": file_path, "error": str(e)})
                    logger.error(f"âŒ åˆ é™¤å¤±è´¥: {file_path} - {str(e)}")

            # æ¸…ç†æµ‹è¯•ç›®å½•
            try:
                if self.test_dir.exists():
                    shutil.rmtree(self.test_dir)
                    logger.info(f"âœ… å·²åˆ é™¤è®­ç»ƒæµ‹è¯•ç›®å½•: {self.test_dir}")
                    cleanup_result["status"] = "å®Œæˆ"
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤æµ‹è¯•ç›®å½•å¤±è´¥: {str(e)}")
                cleanup_result["status"] = "éƒ¨åˆ†å®Œæˆ"

        except Exception as e:
            logger.error(f"âŒ ç¯å¢ƒæ¸…ç†å¼‚å¸¸: {str(e)}")
            cleanup_result["status"] = "å¼‚å¸¸"

        cleanup_result["end_time"] = time.time()
        cleanup_result["duration"] = cleanup_result["end_time"] - cleanup_result["start_time"]

        self.test_results.append(cleanup_result)

        return cleanup_result

    def run_comprehensive_training_test(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹VisionAI-ClipsMasteræ¨¡å‹è®­ç»ƒæ¨¡å—å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•")
        logger.info("=" * 80)

        overall_start_time = time.time()

        try:
            # æ­¥éª¤1: GPUæ£€æµ‹å’Œå¯ç”¨æ€§æµ‹è¯•
            logger.info("æ‰§è¡Œæ­¥éª¤1: GPUæ£€æµ‹å’Œå¯ç”¨æ€§æµ‹è¯•")
            gpu_test = self.test_gpu_detection_and_availability()

            # æ­¥éª¤2: åˆ›å»ºçœŸå®è®­ç»ƒæ•°æ®
            logger.info("æ‰§è¡Œæ­¥éª¤2: åˆ›å»ºçœŸå®è®­ç»ƒæ•°æ®")
            data_creation = self.create_realistic_training_data()

            # æ­¥éª¤3: æ¨¡å‹è®­ç»ƒåŠŸèƒ½æµ‹è¯•
            logger.info("æ‰§è¡Œæ­¥éª¤3: æ¨¡å‹è®­ç»ƒåŠŸèƒ½æµ‹è¯•")
            training_test = self.test_model_training_functionality()

            # æ­¥éª¤4: æ¸…ç†è®­ç»ƒç¯å¢ƒ
            logger.info("æ‰§è¡Œæ­¥éª¤4: æ¸…ç†è®­ç»ƒç¯å¢ƒ")
            cleanup_result = self.cleanup_training_environment()

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒæµ‹è¯•å¼‚å¸¸: {str(e)}")
            try:
                self.cleanup_training_environment()
            except:
                pass

        overall_end_time = time.time()
        overall_duration = overall_end_time - overall_start_time

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        test_report = self.generate_training_test_report(overall_duration)

        return test_report

    def generate_training_test_report(self, overall_duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡å‹è®­ç»ƒæµ‹è¯•æŠ¥å‘Š"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š ç”Ÿæˆæ¨¡å‹è®­ç»ƒæµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 80)

        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_steps = len(self.test_results)
        successful_steps = len([r for r in self.test_results if r.get("status") in ["æˆåŠŸ", "è®­ç»ƒæˆåŠŸ", "GPUå®Œå…¨å¯ç”¨", "å®Œæˆ"]])
        partial_success_steps = len([r for r in self.test_results if r.get("status") in ["éƒ¨åˆ†æˆåŠŸ", "è®­ç»ƒå®Œæˆ", "GPUåŸºæœ¬å¯ç”¨", "åŸºæœ¬å°±ç»ª"]])

        # è®¡ç®—æˆåŠŸç‡
        success_rate = (successful_steps + partial_success_steps * 0.5) / total_steps if total_steps > 0 else 0

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "test_summary": {
                "test_type": "æ¨¡å‹è®­ç»ƒæ¨¡å—å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•",
                "total_steps": total_steps,
                "successful_steps": successful_steps,
                "partial_success_steps": partial_success_steps,
                "failed_steps": total_steps - successful_steps - partial_success_steps,
                "overall_success_rate": round(success_rate * 100, 1),
                "total_duration": round(overall_duration, 2),
                "test_date": datetime.now().isoformat()
            },
            "step_results": self.test_results,
            "performance_metrics": self.performance_metrics,
            "training_assessment": self._assess_training_capabilities(),
            "gpu_assessment": self._assess_gpu_capabilities(),
            "recommendations": self._generate_training_recommendations()
        }

        # æ‰“å°æ‘˜è¦
        logger.info("ğŸ“‹ æ¨¡å‹è®­ç»ƒæµ‹è¯•æ‘˜è¦:")
        logger.info(f"  æ€»æ­¥éª¤æ•°: {total_steps}")
        logger.info(f"  æˆåŠŸæ­¥éª¤: {successful_steps}")
        logger.info(f"  éƒ¨åˆ†æˆåŠŸ: {partial_success_steps}")
        logger.info(f"  æ•´ä½“æˆåŠŸç‡: {report['test_summary']['overall_success_rate']}%")
        logger.info(f"  æ€»è€—æ—¶: {overall_duration:.2f}ç§’")

        # ä¿å­˜æŠ¥å‘Š
        report_file = f"model_training_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“„ æ¨¡å‹è®­ç»ƒæµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            report["report_file"] = report_file
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {str(e)}")

        return report

    def _assess_training_capabilities(self) -> Dict[str, Any]:
        """è¯„ä¼°è®­ç»ƒèƒ½åŠ›"""
        assessment = {
            "training_available": False,
            "learning_effective": False,
            "data_quality": 0.0,
            "overall_capability": 0.0
        }

        try:
            # æ£€æŸ¥è®­ç»ƒå¯ç”¨æ€§
            for result in self.test_results:
                if result.get("step_name") == "æ¨¡å‹è®­ç»ƒåŠŸèƒ½":
                    assessment["training_available"] = result.get("status") in ["è®­ç»ƒæˆåŠŸ", "è®­ç»ƒå®Œæˆ"]

                    learning_metrics = result.get("learning_metrics", {})
                    assessment["learning_effective"] = learning_metrics.get("learning_effective", False)

                    break

            # æ£€æŸ¥æ•°æ®è´¨é‡
            for result in self.test_results:
                if result.get("step_name") == "åˆ›å»ºçœŸå®è®­ç»ƒæ•°æ®":
                    data_quality = result.get("data_quality", {})
                    assessment["data_quality"] = data_quality.get("overall_quality", 0.0)
                    break

            # è®¡ç®—æ€»ä½“èƒ½åŠ›
            assessment["overall_capability"] = (
                (1.0 if assessment["training_available"] else 0.0) * 0.4 +
                (1.0 if assessment["learning_effective"] else 0.0) * 0.4 +
                assessment["data_quality"] * 0.2
            )

        except Exception as e:
            logger.error(f"è®­ç»ƒèƒ½åŠ›è¯„ä¼°å¼‚å¸¸: {str(e)}")

        return assessment

    def _assess_gpu_capabilities(self) -> Dict[str, Any]:
        """è¯„ä¼°GPUèƒ½åŠ›"""
        assessment = {
            "gpu_available": False,
            "gpu_accelerated": False,
            "speedup_ratio": 0.0,
            "memory_efficient": False
        }

        try:
            for result in self.test_results:
                if result.get("step_name") == "GPUæ£€æµ‹å’Œå¯ç”¨æ€§":
                    cuda_info = result.get("cuda_info", {})
                    assessment["gpu_available"] = cuda_info.get("available", False)

                    performance = result.get("performance_comparison", {})
                    assessment["speedup_ratio"] = performance.get("gpu_speedup", 0.0)
                    assessment["gpu_accelerated"] = assessment["speedup_ratio"] > 1.5

                    memory_usage = performance.get("memory_usage", {})
                    if memory_usage:
                        efficiency = memory_usage.get("efficiency_ratio", 0.0)
                        assessment["memory_efficient"] = efficiency < 0.8  # ä½¿ç”¨å°‘äº80%å†…å­˜

                    break

        except Exception as e:
            logger.error(f"GPUèƒ½åŠ›è¯„ä¼°å¼‚å¸¸: {str(e)}")

        return assessment

    def _generate_training_recommendations(self) -> List[str]:
        """ç”Ÿæˆè®­ç»ƒå»ºè®®"""
        recommendations = []

        try:
            training_assessment = self._assess_training_capabilities()
            gpu_assessment = self._assess_gpu_capabilities()

            # åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆå»ºè®®
            if not training_assessment["training_available"]:
                recommendations.append("éœ€è¦ä¿®å¤æ¨¡å‹è®­ç»ƒåŠŸèƒ½çš„æ ¸å¿ƒé—®é¢˜")

            if not training_assessment["learning_effective"]:
                recommendations.append("éœ€è¦æ”¹è¿›å­¦ä¹ ç®—æ³•å’Œè®­ç»ƒæ•°æ®è´¨é‡")

            if training_assessment["data_quality"] < 0.7:
                recommendations.append("å»ºè®®å¢åŠ è®­ç»ƒæ•°æ®çš„æ•°é‡å’Œå¤šæ ·æ€§")

            if not gpu_assessment["gpu_available"]:
                recommendations.append("å»ºè®®é…ç½®GPUç¯å¢ƒä»¥æå‡è®­ç»ƒæ€§èƒ½")
            elif not gpu_assessment["gpu_accelerated"]:
                recommendations.append("GPUåŠ é€Ÿæ•ˆæœæœ‰é™ï¼Œå»ºè®®ä¼˜åŒ–æ¨¡å‹ç»“æ„")

            if gpu_assessment["gpu_available"] and not gpu_assessment["memory_efficient"]:
                recommendations.append("å»ºè®®ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨ï¼Œå‡å°‘å†…å­˜å ç”¨")

            # é€šç”¨å»ºè®®
            if not recommendations:
                recommendations.extend([
                    "æ¨¡å‹è®­ç»ƒåŠŸèƒ½è¿è¡Œè‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œæ›´å¤§è§„æ¨¡çš„è®­ç»ƒæµ‹è¯•",
                    "è€ƒè™‘ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹æ¶æ„æå‡å­¦ä¹ æ•ˆæœ",
                    "å»ºè®®å®šæœŸç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½æŒ‡æ ‡"
                ])

        except Exception as e:
            recommendations.append(f"å»ºè®®ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")

        return recommendations


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ VisionAI-ClipsMaster æ¨¡å‹è®­ç»ƒæ¨¡å—å®Œæ•´åŠŸèƒ½éªŒè¯æµ‹è¯•")
    print("=" * 80)

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelTrainingComprehensiveTester()

    try:
        # è¿è¡Œå®Œæ•´æµ‹è¯•
        report = tester.run_comprehensive_training_test()

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        success_rate = report.get("test_summary", {}).get("overall_success_rate", 0)
        training_capability = report.get("training_assessment", {}).get("overall_capability", 0)

        if success_rate >= 90 and training_capability >= 0.8:
            print(f"\nğŸ‰ æ¨¡å‹è®­ç»ƒæµ‹è¯•å®Œæˆï¼æˆåŠŸç‡: {success_rate}% - è®­ç»ƒåŠŸèƒ½ä¼˜ç§€")
        elif success_rate >= 70 and training_capability >= 0.6:
            print(f"\nâœ… æ¨¡å‹è®­ç»ƒæµ‹è¯•å®Œæˆï¼æˆåŠŸç‡: {success_rate}% - è®­ç»ƒåŠŸèƒ½è‰¯å¥½")
        elif success_rate >= 50:
            print(f"\nâš ï¸ æ¨¡å‹è®­ç»ƒæµ‹è¯•å®Œæˆï¼æˆåŠŸç‡: {success_rate}% - è®­ç»ƒåŠŸèƒ½éœ€è¦ä¼˜åŒ–")
        else:
            print(f"\nâŒ æ¨¡å‹è®­ç»ƒæµ‹è¯•å®Œæˆï¼æˆåŠŸç‡: {success_rate}% - è®­ç»ƒåŠŸèƒ½éœ€è¦ä¿®å¤")

        return report

    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        try:
            tester.cleanup_training_environment()
        except:
            pass
        return None
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        try:
            tester.cleanup_training_environment()
        except:
            pass
        return None


if __name__ == "__main__":
    main()
