#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster è®­ç»ƒæ¨¡å—å…¨é¢æµ‹è¯•éªŒè¯æœ€ç»ˆæŠ¥å‘Š
Final Comprehensive Training Module Test Report
"""

import os
import sys
import json
import time
import torch
import psutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

class FinalTrainingModuleTestReport:
    """æœ€ç»ˆè®­ç»ƒæ¨¡å—æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.report_data = {
            "test_timestamp": datetime.now().isoformat(),
            "system_environment": self.get_system_environment(),
            "test_results": {},
            "performance_metrics": {},
            "recommendations": {},
            "final_assessment": {}
        }
        
    def get_system_environment(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç¯å¢ƒä¿¡æ¯"""
        return {
            "python_version": sys.version,
            "pytorch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "memory_available_gb": psutil.virtual_memory().available / (1024**3),
            "platform": sys.platform
        }
        
    def run_comprehensive_tests(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹VisionAI-ClipsMasterè®­ç»ƒæ¨¡å—å…¨é¢æµ‹è¯•éªŒè¯")
        print("=" * 80)
        
        # 1. è®­ç»ƒå·¥ä½œæµå®Œæ•´æ€§æµ‹è¯•
        print("ğŸ“‹ 1. è®­ç»ƒå·¥ä½œæµå®Œæ•´æ€§æµ‹è¯•...")
        workflow_results = self.test_workflow_integrity()
        self.report_data["test_results"]["workflow_integrity"] = workflow_results
        
        # 2. è®­ç»ƒæ•°æ®å¤„ç†éªŒè¯
        print("ğŸ“Š 2. è®­ç»ƒæ•°æ®å¤„ç†éªŒè¯...")
        data_results = self.test_data_processing()
        self.report_data["test_results"]["data_processing"] = data_results
        
        # 3. GPUåŠ é€ŸéªŒè¯
        print("ğŸš€ 3. GPUåŠ é€ŸçœŸå®æ€§éªŒè¯...")
        gpu_results = self.test_gpu_acceleration()
        self.report_data["test_results"]["gpu_acceleration"] = gpu_results
        
        # 4. æ¨¡å‹åˆ‡æ¢æœºåˆ¶æµ‹è¯•
        print("ğŸ”„ 4. æ¨¡å‹åˆ‡æ¢æœºåˆ¶æµ‹è¯•...")
        model_switch_results = self.test_model_switching()
        self.report_data["test_results"]["model_switching"] = model_switch_results
        
        # 5. å†…å­˜ä¼˜åŒ–æµ‹è¯•
        print("ğŸ’¾ 5. å†…å­˜ä¼˜åŒ–æµ‹è¯•...")
        memory_results = self.test_memory_optimization()
        self.report_data["test_results"]["memory_optimization"] = memory_results
        
        # 6. è®­ç»ƒæ•ˆæœéªŒè¯
        print("ğŸ¯ 6. è®­ç»ƒæ•ˆæœéªŒè¯...")
        effectiveness_results = self.test_training_effectiveness()
        self.report_data["test_results"]["training_effectiveness"] = effectiveness_results
        
        return self.report_data
        
    def test_workflow_integrity(self) -> Dict[str, Any]:
        """æµ‹è¯•å·¥ä½œæµå®Œæ•´æ€§"""
        results = {"status": "PASS", "details": {}, "issues": []}
        
        # æ£€æŸ¥å…³é”®æ–‡ä»¶
        required_files = [
            "src/training/en_trainer.py",
            "src/training/zh_trainer.py",
            "src/training/curriculum.py",
            "src/training/data_splitter.py",
            "src/training/data_augment.py",
            "src/training/plot_augment.py"
        ]
        
        missing_files = []
        for file_path in required_files:
            if not (self.project_root / file_path).exists():
                missing_files.append(file_path)
                
        results["details"]["required_files"] = len(required_files)
        results["details"]["found_files"] = len(required_files) - len(missing_files)
        results["details"]["missing_files"] = missing_files
        
        if missing_files:
            results["status"] = "FAIL"
            results["issues"].append(f"ç¼ºå¤±å…³é”®æ–‡ä»¶: {missing_files}")
            
        # æµ‹è¯•æ¨¡å—å¯¼å…¥
        import_results = {}
        try:
            from src.training.en_trainer import EnTrainer
            import_results["EnTrainer"] = "SUCCESS"
        except Exception as e:
            import_results["EnTrainer"] = f"FAILED: {str(e)}"
            results["status"] = "FAIL"
            
        try:
            from src.training.zh_trainer import ZhTrainer
            import_results["ZhTrainer"] = "SUCCESS"
        except Exception as e:
            import_results["ZhTrainer"] = f"FAILED: {str(e)}"
            results["status"] = "FAIL"
            
        results["details"]["import_tests"] = import_results
        return results
        
    def test_data_processing(self) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®å¤„ç†"""
        results = {"status": "PASS", "details": {}, "issues": []}
        
        # æ£€æŸ¥è®­ç»ƒæ•°æ®ç›®å½•
        en_dir = self.project_root / "data/training/en"
        zh_dir = self.project_root / "data/training/zh"
        
        en_files = list(en_dir.glob("*.json")) + list(en_dir.glob("*.txt")) if en_dir.exists() else []
        zh_files = list(zh_dir.glob("*.json")) + list(zh_dir.glob("*.txt")) if zh_dir.exists() else []
        
        results["details"]["english_files"] = len(en_files)
        results["details"]["chinese_files"] = len(zh_files)
        results["details"]["total_files"] = len(en_files) + len(zh_files)
        
        # éªŒè¯æ•°æ®æ ¼å¼
        valid_files = 0
        for file_path in (en_files + zh_files)[:5]:  # æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if file_path.suffix == '.json':
                    json.loads(content)
                valid_files += 1
            except:
                pass
                
        results["details"]["format_validation"] = {
            "checked_files": min(5, len(en_files) + len(zh_files)),
            "valid_files": valid_files
        }
        
        if len(en_files) + len(zh_files) == 0:
            results["status"] = "WARNING"
            results["issues"].append("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
            
        return results
        
    def test_gpu_acceleration(self) -> Dict[str, Any]:
        """æµ‹è¯•GPUåŠ é€Ÿ"""
        results = {"status": "PASS", "details": {}, "issues": []}
        
        cuda_available = torch.cuda.is_available()
        results["details"]["cuda_available"] = cuda_available
        results["details"]["pytorch_version"] = torch.__version__
        
        if cuda_available:
            results["details"]["gpu_count"] = torch.cuda.device_count()
            gpu_info = []
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                gpu_info.append({
                    "name": props.name,
                    "memory_gb": props.total_memory / (1024**3)
                })
            results["details"]["gpu_devices"] = gpu_info
            
            # ç®€å•æ€§èƒ½æµ‹è¯•
            try:
                device = torch.device("cuda:0")
                test_tensor = torch.randn(1000, 1000, device=device)
                start_time = time.time()
                result = torch.mm(test_tensor, test_tensor)
                torch.cuda.synchronize()
                gpu_time = time.time() - start_time
                results["details"]["gpu_performance_test"] = {
                    "success": True,
                    "computation_time": gpu_time
                }
                torch.cuda.empty_cache()
            except Exception as e:
                results["details"]["gpu_performance_test"] = {
                    "success": False,
                    "error": str(e)
                }
        else:
            results["status"] = "WARNING"
            results["issues"].append("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            
        return results
        
    def test_model_switching(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹åˆ‡æ¢"""
        results = {"status": "PASS", "details": {}, "issues": []}
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_files = [
            "configs/models/active_model.yaml",
            "configs/models/available_models/mistral-7b-en.yaml",
            "configs/models/available_models/qwen2.5-7b-zh.yaml"
        ]
        
        config_status = {}
        for config_file in config_files:
            config_status[config_file] = (self.project_root / config_file).exists()
            
        results["details"]["config_files"] = config_status
        
        # æµ‹è¯•è¯­è¨€æ£€æµ‹
        try:
            from src.core.language_detector import LanguageDetector
            detector = LanguageDetector()
            
            test_results = []
            test_cases = [
                ("Hello world", "en"),
                ("ä½ å¥½ä¸–ç•Œ", "zh")
            ]
            
            for text, expected in test_cases:
                detected = detector.detect_language(text)
                test_results.append({
                    "text": text,
                    "expected": expected,
                    "detected": detected,
                    "correct": detected == expected
                })
                
            results["details"]["language_detection"] = test_results
            
        except Exception as e:
            results["status"] = "WARNING"
            results["issues"].append(f"è¯­è¨€æ£€æµ‹æµ‹è¯•å¤±è´¥: {str(e)}")
            
        return results
        
    def test_memory_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä¼˜åŒ–"""
        results = {"status": "PASS", "details": {}, "issues": []}
        
        # ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        memory_info = psutil.virtual_memory()
        results["details"]["system_memory"] = {
            "total_gb": memory_info.total / (1024**3),
            "available_gb": memory_info.available / (1024**3),
            "used_percent": memory_info.percent
        }
        
        # æ£€æŸ¥å†…å­˜ç›‘æ§æ¨¡å—
        try:
            from src.utils.memory_guard import MemoryGuard
            guard = MemoryGuard()
            current_usage = guard.get_memory_usage()
            results["details"]["memory_guard"] = {
                "available": True,
                "current_usage_mb": current_usage
            }
            
            # 4GBè®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥
            if memory_info.total / (1024**3) <= 4.5:
                if current_usage > 3800:
                    results["status"] = "WARNING"
                    results["issues"].append("å†…å­˜ä½¿ç”¨å¯èƒ½è¶…è¿‡4GBè®¾å¤‡é™åˆ¶")
                    
        except Exception as e:
            results["status"] = "WARNING"
            results["issues"].append(f"å†…å­˜ç›‘æ§æ¨¡å—ä¸å¯ç”¨: {str(e)}")
            
        return results
        
    def test_training_effectiveness(self) -> Dict[str, Any]:
        """æµ‹è¯•è®­ç»ƒæ•ˆæœ"""
        results = {"status": "PASS", "details": {}, "issues": []}
        
        # æ£€æŸ¥æ¨¡å‹ç›®å½•
        model_dirs = ["models/mistral", "models/qwen"]
        model_status = {}
        
        for model_dir in model_dirs:
            model_path = self.project_root / model_dir
            if model_path.exists():
                model_files = list(model_path.rglob("*.bin")) + list(model_path.rglob("*.safetensors"))
                model_status[model_dir] = {
                    "exists": True,
                    "model_files": len(model_files)
                }
            else:
                model_status[model_dir] = {"exists": False, "model_files": 0}
                
        results["details"]["model_availability"] = model_status
        
        # æ£€æŸ¥è®­ç»ƒå†å²
        history_file = self.project_root / "data/training/training_history.json"
        results["details"]["training_history_exists"] = history_file.exists()
        
        return results
        
    def generate_final_assessment(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆè¯„ä¼°"""
        test_results = self.report_data["test_results"]
        
        # è®¡ç®—é€šè¿‡ç‡
        total_tests = len(test_results)
        passed_tests = sum(1 for result in test_results.values() if result["status"] == "PASS")
        warning_tests = sum(1 for result in test_results.values() if result["status"] == "WARNING")
        failed_tests = sum(1 for result in test_results.values() if result["status"] == "FAIL")
        
        assessment = {
            "overall_status": "PASS" if failed_tests == 0 else "FAIL",
            "test_summary": {
                "total": total_tests,
                "passed": passed_tests,
                "warnings": warning_tests,
                "failed": failed_tests,
                "pass_rate": passed_tests / total_tests * 100 if total_tests > 0 else 0
            },
            "readiness_score": self.calculate_readiness_score(),
            "critical_issues": self.identify_critical_issues(),
            "recommendations": self.generate_final_recommendations()
        }
        
        return assessment
        
    def calculate_readiness_score(self) -> float:
        """è®¡ç®—å°±ç»ªåº¦åˆ†æ•°"""
        test_results = self.report_data["test_results"]
        
        # æƒé‡åˆ†é…
        weights = {
            "workflow_integrity": 0.25,
            "data_processing": 0.20,
            "gpu_acceleration": 0.15,
            "model_switching": 0.20,
            "memory_optimization": 0.10,
            "training_effectiveness": 0.10
        }
        
        total_score = 0
        for test_name, result in test_results.items():
            if result["status"] == "PASS":
                score = 100
            elif result["status"] == "WARNING":
                score = 70
            else:
                score = 0
                
            total_score += score * weights.get(test_name, 0)
            
        return round(total_score, 1)
        
    def identify_critical_issues(self) -> List[str]:
        """è¯†åˆ«å…³é”®é—®é¢˜"""
        critical_issues = []
        test_results = self.report_data["test_results"]
        
        for test_name, result in test_results.items():
            if result["status"] == "FAIL":
                critical_issues.extend(result.get("issues", []))
                
        return critical_issues
        
    def generate_final_recommendations(self) -> List[str]:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        recommendations = []
        test_results = self.report_data["test_results"]
        
        # GPUç›¸å…³å»ºè®®
        if not self.report_data["system_environment"]["cuda_available"]:
            recommendations.append("è€ƒè™‘åœ¨æœ‰GPUçš„ç¯å¢ƒä¸­éƒ¨ç½²ä»¥è·å¾—æ›´å¥½çš„è®­ç»ƒæ€§èƒ½")
            
        # å†…å­˜ç›¸å…³å»ºè®®
        memory_gb = self.report_data["system_environment"]["memory_total_gb"]
        if memory_gb < 8:
            recommendations.append("å»ºè®®åœ¨å†…å­˜â‰¥8GBçš„è®¾å¤‡ä¸Šè¿è¡Œä»¥è·å¾—æ›´å¥½çš„ç¨³å®šæ€§")
            
        # æ•°æ®ç›¸å…³å»ºè®®
        data_result = test_results.get("data_processing", {})
        total_files = data_result.get("details", {}).get("total_files", 0)
        if total_files < 10:
            recommendations.append("å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®ä»¥æé«˜æ¨¡å‹æ•ˆæœ")
            
        return recommendations
        
    def generate_html_report(self) -> str:
        """ç”ŸæˆHTMLæ ¼å¼æŠ¥å‘Š"""
        assessment = self.generate_final_assessment()
        self.report_data["final_assessment"] = assessment
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>VisionAI-ClipsMaster è®­ç»ƒæ¨¡å—æµ‹è¯•æŠ¥å‘Š</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}
        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .pass {{ color: #27ae60; }}
        .warning {{ color: #f39c12; }}
        .fail {{ color: #e74c3c; }}
        .metric {{ display: inline-block; margin: 10px; padding: 10px; background: #ecf0f1; border-radius: 3px; }}
        table {{ width: 100%; border-collapse: collapse; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ VisionAI-ClipsMaster è®­ç»ƒæ¨¡å—å…¨é¢æµ‹è¯•æŠ¥å‘Š</h1>
        <p>æµ‹è¯•æ—¶é—´: {self.report_data['test_timestamp']}</p>
    </div>
    
    <div class="section">
        <h2>ğŸ“Š æµ‹è¯•æ¦‚è§ˆ</h2>
        <div class="metric">æ€»æµ‹è¯•æ•°: {assessment['test_summary']['total']}</div>
        <div class="metric pass">é€šè¿‡: {assessment['test_summary']['passed']}</div>
        <div class="metric warning">è­¦å‘Š: {assessment['test_summary']['warnings']}</div>
        <div class="metric fail">å¤±è´¥: {assessment['test_summary']['failed']}</div>
        <div class="metric">é€šè¿‡ç‡: {assessment['test_summary']['pass_rate']:.1f}%</div>
        <div class="metric">å°±ç»ªåº¦åˆ†æ•°: {assessment['readiness_score']}/100</div>
    </div>
    
    <div class="section">
        <h2>ğŸ–¥ï¸ ç³»ç»Ÿç¯å¢ƒ</h2>
        <table>
            <tr><th>é¡¹ç›®</th><th>å€¼</th></tr>
            <tr><td>Pythonç‰ˆæœ¬</td><td>{self.report_data['system_environment']['python_version'].split()[0]}</td></tr>
            <tr><td>PyTorchç‰ˆæœ¬</td><td>{self.report_data['system_environment']['pytorch_version']}</td></tr>
            <tr><td>CUDAå¯ç”¨</td><td>{'æ˜¯' if self.report_data['system_environment']['cuda_available'] else 'å¦'}</td></tr>
            <tr><td>GPUæ•°é‡</td><td>{self.report_data['system_environment']['gpu_count']}</td></tr>
            <tr><td>CPUæ ¸å¿ƒæ•°</td><td>{self.report_data['system_environment']['cpu_count']}</td></tr>
            <tr><td>æ€»å†…å­˜</td><td>{self.report_data['system_environment']['memory_total_gb']:.1f} GB</td></tr>
        </table>
    </div>
    
    <div class="section">
        <h2>ğŸ¯ æœ€ç»ˆè¯„ä¼°</h2>
        <p><strong>æ•´ä½“çŠ¶æ€:</strong> <span class="{'pass' if assessment['overall_status'] == 'PASS' else 'fail'}">{assessment['overall_status']}</span></p>
        <p><strong>å°±ç»ªåº¦åˆ†æ•°:</strong> {assessment['readiness_score']}/100</p>
        
        {'<h3>âŒ å…³é”®é—®é¢˜:</h3><ul>' + ''.join(f'<li>{issue}</li>' for issue in assessment['critical_issues']) + '</ul>' if assessment['critical_issues'] else ''}
        
        {'<h3>ğŸ’¡ å»ºè®®:</h3><ul>' + ''.join(f'<li>{rec}</li>' for rec in assessment['recommendations']) + '</ul>' if assessment['recommendations'] else ''}
    </div>
    
    <div class="section">
        <h2>ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ</h2>
        <table>
            <tr><th>æµ‹è¯•é¡¹ç›®</th><th>çŠ¶æ€</th><th>è¯¦ç»†ä¿¡æ¯</th></tr>
"""
        
        for test_name, result in self.report_data["test_results"].items():
            status_class = result["status"].lower()
            status_icon = {"PASS": "âœ…", "WARNING": "âš ï¸", "FAIL": "âŒ"}.get(result["status"], "â“")
            
            html_content += f"""
            <tr>
                <td>{test_name}</td>
                <td class="{status_class}">{status_icon} {result["status"]}</td>
                <td>{json.dumps(result.get("details", {}), ensure_ascii=False)[:200]}...</td>
            </tr>
"""
        
        html_content += """
        </table>
    </div>
</body>
</html>
"""
        return html_content
        
    def save_reports(self):
        """ä¿å­˜æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæŠ¥å‘Š
        json_file = f"training_module_final_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜HTMLæŠ¥å‘Š
        html_content = self.generate_html_report()
        html_file = f"training_module_final_report_{timestamp}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
            
        return json_file, html_file


def main():
    """ä¸»å‡½æ•°"""
    reporter = FinalTrainingModuleTestReport()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = reporter.run_comprehensive_tests()
    
    # ç”Ÿæˆæœ€ç»ˆè¯„ä¼°
    assessment = reporter.generate_final_assessment()
    results["final_assessment"] = assessment
    
    # ä¿å­˜æŠ¥å‘Š
    json_file, html_file = reporter.save_reports()
    
    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"   æ•´ä½“çŠ¶æ€: {assessment['overall_status']}")
    print(f"   å°±ç»ªåº¦åˆ†æ•°: {assessment['readiness_score']}/100")
    print(f"   é€šè¿‡ç‡: {assessment['test_summary']['pass_rate']:.1f}%")
    print(f"   æŠ¥å‘Šå·²ä¿å­˜: {html_file}")
    
    if assessment['critical_issues']:
        print(f"\nâŒ å…³é”®é—®é¢˜:")
        for issue in assessment['critical_issues']:
            print(f"   - {issue}")
            
    if assessment['recommendations']:
        print(f"\nğŸ’¡ å»ºè®®:")
        for rec in assessment['recommendations']:
            print(f"   - {rec}")
            
    print("=" * 80)
    
    return 0 if assessment['overall_status'] == 'PASS' else 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
