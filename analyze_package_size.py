#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster æ•´åˆåŒ…ä½“ç§¯åˆ†æå·¥å…·
åˆ†ææ‰“åŒ…åçš„ä½“ç§¯æ„æˆå’Œä¼˜åŒ–å»ºè®®
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple
import json

class PackageSizeAnalyzer:
    """æ•´åˆåŒ…ä½“ç§¯åˆ†æå™¨"""
    
    def __init__(self, package_path: str):
        self.package_path = Path(package_path)
        self.internal_path = self.package_path / "_internal"
        
        self.analysis_results = {
            "total_size_mb": 0,
            "file_count": 0,
            "directory_analysis": {},
            "largest_files": [],
            "component_breakdown": {},
            "optimization_opportunities": []
        }
    
    def get_directory_size(self, directory: Path) -> Tuple[int, int]:
        """è·å–ç›®å½•å¤§å°å’Œæ–‡ä»¶æ•°é‡"""
        total_size = 0
        file_count = 0
        
        try:
            for item in directory.rglob('*'):
                if item.is_file():
                    total_size += item.stat().st_size
                    file_count += 1
        except (PermissionError, OSError):
            pass
        
        return total_size, file_count
    
    def analyze_internal_components(self):
        """åˆ†æ_internalç›®å½•ä¸­çš„ç»„ä»¶"""
        print("ğŸ” åˆ†æ_internalç›®å½•ç»„ä»¶...")
        
        if not self.internal_path.exists():
            print("âŒ _internalç›®å½•ä¸å­˜åœ¨")
            return
        
        components = {}
        
        # åˆ†æä¸»è¦ç»„ä»¶
        for item in self.internal_path.iterdir():
            if item.is_dir():
                size, count = self.get_directory_size(item)
                components[item.name] = {
                    "size_mb": size / 1024 / 1024,
                    "file_count": count,
                    "type": "directory"
                }
            elif item.is_file():
                size = item.stat().st_size
                components[item.name] = {
                    "size_mb": size / 1024 / 1024,
                    "file_count": 1,
                    "type": "file"
                }
        
        # æŒ‰å¤§å°æ’åº
        sorted_components = sorted(components.items(), 
                                 key=lambda x: x[1]["size_mb"], 
                                 reverse=True)
        
        self.analysis_results["component_breakdown"] = dict(sorted_components)
        
        print("ğŸ“Š ä¸»è¦ç»„ä»¶å¤§å°æ’åº:")
        for name, info in sorted_components[:15]:
            size_mb = info["size_mb"]
            file_count = info["file_count"]
            comp_type = info["type"]
            print(f"   {name:30} {size_mb:8.1f} MB ({file_count:4d} files) [{comp_type}]")
    
    def analyze_ai_frameworks(self):
        """åˆ†æAIæ¡†æ¶ç»„ä»¶"""
        print("\nğŸ¤– åˆ†æAIæ¡†æ¶ç»„ä»¶...")
        
        ai_components = {
            "torch": "PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶",
            "transformers": "HuggingFace Transformers",
            "numpy": "æ•°å€¼è®¡ç®—åº“",
            "scipy": "ç§‘å­¦è®¡ç®—åº“",
            "opencv": "è®¡ç®—æœºè§†è§‰åº“",
            "sklearn": "æœºå™¨å­¦ä¹ åº“",
            "pandas": "æ•°æ®å¤„ç†åº“",
            "matplotlib": "ç»˜å›¾åº“",
            "PIL": "å›¾åƒå¤„ç†åº“",
            "cv2": "OpenCV Pythonç»‘å®š"
        }
        
        ai_analysis = {}
        total_ai_size = 0
        
        for component, description in ai_components.items():
            component_path = self.internal_path / component
            if component_path.exists():
                size, count = self.get_directory_size(component_path)
                size_mb = size / 1024 / 1024
                ai_analysis[component] = {
                    "size_mb": size_mb,
                    "file_count": count,
                    "description": description
                }
                total_ai_size += size_mb
        
        print(f"ğŸ¯ AIæ¡†æ¶æ€»å¤§å°: {total_ai_size:.1f} MB")
        print("ğŸ“‹ AIæ¡†æ¶è¯¦ç»†åˆ†æ:")
        
        for component, info in sorted(ai_analysis.items(), 
                                    key=lambda x: x[1]["size_mb"], 
                                    reverse=True):
            size_mb = info["size_mb"]
            file_count = info["file_count"]
            desc = info["description"]
            print(f"   {component:15} {size_mb:8.1f} MB ({file_count:4d} files) - {desc}")
        
        return ai_analysis, total_ai_size
    
    def analyze_python_runtime(self):
        """åˆ†æPythonè¿è¡Œæ—¶ç»„ä»¶"""
        print("\nğŸ åˆ†æPythonè¿è¡Œæ—¶ç»„ä»¶...")
        
        python_components = [
            "python3.dll", "python311.dll", "base_library.zip",
            "vcruntime140.dll", "msvcp140.dll", "ucrtbase.dll"
        ]
        
        python_analysis = {}
        total_python_size = 0
        
        for component in python_components:
            component_path = self.internal_path / component
            if component_path.exists():
                size = component_path.stat().st_size
                size_mb = size / 1024 / 1024
                python_analysis[component] = size_mb
                total_python_size += size_mb
        
        print(f"ğŸ¯ Pythonè¿è¡Œæ—¶æ€»å¤§å°: {total_python_size:.1f} MB")
        print("ğŸ“‹ Pythonè¿è¡Œæ—¶è¯¦ç»†åˆ†æ:")
        
        for component, size_mb in sorted(python_analysis.items(), 
                                       key=lambda x: x[1], 
                                       reverse=True):
            print(f"   {component:20} {size_mb:8.1f} MB")
        
        return python_analysis, total_python_size
    
    def find_largest_files(self, top_n: int = 20):
        """æŸ¥æ‰¾æœ€å¤§çš„æ–‡ä»¶"""
        print(f"\nğŸ“ æŸ¥æ‰¾æœ€å¤§çš„{top_n}ä¸ªæ–‡ä»¶...")
        
        all_files = []
        
        for item in self.package_path.rglob('*'):
            if item.is_file():
                try:
                    size = item.stat().st_size
                    relative_path = item.relative_to(self.package_path)
                    all_files.append((str(relative_path), size))
                except (PermissionError, OSError):
                    continue
        
        # æŒ‰å¤§å°æ’åº
        largest_files = sorted(all_files, key=lambda x: x[1], reverse=True)[:top_n]
        
        print("ğŸ“Š æœ€å¤§æ–‡ä»¶åˆ—è¡¨:")
        for i, (file_path, size) in enumerate(largest_files, 1):
            size_mb = size / 1024 / 1024
            print(f"   {i:2d}. {file_path:60} {size_mb:8.1f} MB")
        
        self.analysis_results["largest_files"] = largest_files
        return largest_files
    
    def identify_optimization_opportunities(self):
        """è¯†åˆ«ä¼˜åŒ–æœºä¼š"""
        print("\nğŸ”§ è¯†åˆ«ä¼˜åŒ–æœºä¼š...")
        
        opportunities = []
        
        # æ£€æŸ¥å¯èƒ½çš„å†—ä½™ç»„ä»¶
        redundant_patterns = [
            ("tests", "æµ‹è¯•æ–‡ä»¶ç›®å½•"),
            ("docs", "æ–‡æ¡£ç›®å½•"),
            ("examples", "ç¤ºä¾‹æ–‡ä»¶"),
            ("__pycache__", "Pythonç¼“å­˜æ–‡ä»¶"),
            (".dist-info", "åŒ…ä¿¡æ¯æ–‡ä»¶"),
            ("locale", "æœ¬åœ°åŒ–æ–‡ä»¶"),
            ("tcl8", "Tcl/Tkç»„ä»¶"),
            ("tk86t.dll", "Tk GUIç»„ä»¶"),
            ("_tcl_data", "Tclæ•°æ®æ–‡ä»¶"),
            ("_tk_data", "Tkæ•°æ®æ–‡ä»¶")
        ]
        
        for pattern, description in redundant_patterns:
            matching_items = list(self.internal_path.glob(f"**/*{pattern}*"))
            if matching_items:
                total_size = 0
                for item in matching_items:
                    if item.is_file():
                        total_size += item.stat().st_size
                    elif item.is_dir():
                        size, _ = self.get_directory_size(item)
                        total_size += size
                
                if total_size > 1024 * 1024:  # å¤§äº1MBæ‰æŠ¥å‘Š
                    opportunities.append({
                        "pattern": pattern,
                        "description": description,
                        "size_mb": total_size / 1024 / 1024,
                        "items_count": len(matching_items),
                        "optimization_type": "remove_redundant"
                    })
        
        # æ£€æŸ¥å¤§å‹AIåº“çš„ä¼˜åŒ–æœºä¼š
        large_ai_libs = [
            ("torch", "è€ƒè™‘ä½¿ç”¨CPUç‰ˆæœ¬æˆ–é‡åŒ–ç‰ˆæœ¬"),
            ("transformers", "åªä¿ç•™å¿…éœ€çš„æ¨¡å‹ç±»å‹"),
            ("scipy", "è€ƒè™‘ä½¿ç”¨ç²¾ç®€ç‰ˆæœ¬"),
            ("matplotlib", "å¦‚ä¸éœ€è¦ç»˜å›¾åŠŸèƒ½å¯ç§»é™¤"),
            ("pandas", "è€ƒè™‘ä½¿ç”¨æ›´è½»é‡çš„æ•°æ®å¤„ç†åº“")
        ]
        
        for lib_name, suggestion in large_ai_libs:
            lib_path = self.internal_path / lib_name
            if lib_path.exists():
                size, _ = self.get_directory_size(lib_path)
                size_mb = size / 1024 / 1024
                if size_mb > 50:  # å¤§äº50MB
                    opportunities.append({
                        "pattern": lib_name,
                        "description": suggestion,
                        "size_mb": size_mb,
                        "optimization_type": "optimize_ai_lib"
                    })
        
        self.analysis_results["optimization_opportunities"] = opportunities
        
        print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for opp in opportunities:
            print(f"   â€¢ {opp['pattern']:20} {opp['size_mb']:8.1f} MB - {opp['description']}")
        
        return opportunities
    
    def generate_optimization_plan(self):
        """ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ"""
        print("\nğŸ“‹ ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ...")
        
        optimization_plan = {
            "immediate_savings": [],  # ç«‹å³å¯å®ç°çš„èŠ‚çœ
            "moderate_savings": [],   # ä¸­ç­‰é£é™©çš„èŠ‚çœ
            "aggressive_savings": []  # æ¿€è¿›çš„èŠ‚çœæ–¹æ¡ˆ
        }
        
        # ç«‹å³å¯å®ç°çš„ä¼˜åŒ–ï¼ˆä½é£é™©ï¼‰
        immediate_items = [
            ("__pycache__", "Pythonç¼“å­˜æ–‡ä»¶", 5, "åˆ é™¤æ‰€æœ‰.pycæ–‡ä»¶"),
            ("tests", "æµ‹è¯•æ–‡ä»¶", 20, "åˆ é™¤æµ‹è¯•ç›®å½•"),
            ("docs", "æ–‡æ¡£æ–‡ä»¶", 10, "åˆ é™¤æ–‡æ¡£ç›®å½•"),
            (".dist-info", "åŒ…ä¿¡æ¯", 5, "åˆ é™¤åŒ…å…ƒæ•°æ®"),
            ("locale", "æœ¬åœ°åŒ–", 15, "åªä¿ç•™ä¸­è‹±æ–‡")
        ]
        
        for pattern, desc, est_savings, action in immediate_items:
            optimization_plan["immediate_savings"].append({
                "item": pattern,
                "description": desc,
                "estimated_savings_mb": est_savings,
                "action": action,
                "risk": "ä½"
            })
        
        # ä¸­ç­‰é£é™©çš„ä¼˜åŒ–
        moderate_items = [
            ("matplotlib", "ç»˜å›¾åº“", 80, "å¦‚ä¸éœ€è¦å¯è§†åŒ–åŠŸèƒ½å¯åˆ é™¤"),
            ("pandas", "æ•°æ®å¤„ç†", 60, "ä½¿ç”¨æ›´è½»é‡çš„æ›¿ä»£æ–¹æ¡ˆ"),
            ("scipy", "ç§‘å­¦è®¡ç®—", 100, "åªä¿ç•™å¿…éœ€çš„å­æ¨¡å—"),
            ("tcl8/tk86t", "GUIç»„ä»¶", 30, "åˆ é™¤Tkinterç›¸å…³ç»„ä»¶")
        ]
        
        for pattern, desc, est_savings, action in moderate_items:
            optimization_plan["moderate_savings"].append({
                "item": pattern,
                "description": desc,
                "estimated_savings_mb": est_savings,
                "action": action,
                "risk": "ä¸­"
            })
        
        # æ¿€è¿›çš„ä¼˜åŒ–ï¼ˆé«˜é£é™©ï¼‰
        aggressive_items = [
            ("torch", "PyTorch", 300, "ä½¿ç”¨ONNX Runtimeæ›¿ä»£"),
            ("transformers", "Transformers", 200, "åªä¿ç•™å¿…éœ€çš„æ¨¡å‹æ¶æ„"),
            ("opencv", "OpenCV", 150, "ä½¿ç”¨ç²¾ç®€ç‰ˆæœ¬"),
            ("numpy", "NumPy", 50, "ä½¿ç”¨Intel MKLä¼˜åŒ–ç‰ˆæœ¬")
        ]
        
        for pattern, desc, est_savings, action in aggressive_items:
            optimization_plan["aggressive_savings"].append({
                "item": pattern,
                "description": desc,
                "estimated_savings_mb": est_savings,
                "action": action,
                "risk": "é«˜"
            })
        
        # è®¡ç®—æ€»çš„æ½œåœ¨èŠ‚çœ
        total_immediate = sum(item["estimated_savings_mb"] for item in optimization_plan["immediate_savings"])
        total_moderate = sum(item["estimated_savings_mb"] for item in optimization_plan["moderate_savings"])
        total_aggressive = sum(item["estimated_savings_mb"] for item in optimization_plan["aggressive_savings"])
        
        print(f"ğŸ’¾ æ½œåœ¨èŠ‚çœç©ºé—´:")
        print(f"   ç«‹å³å¯å®ç°: {total_immediate} MB (ä½é£é™©)")
        print(f"   ä¸­ç­‰ä¼˜åŒ–:   {total_moderate} MB (ä¸­é£é™©)")
        print(f"   æ¿€è¿›ä¼˜åŒ–:   {total_aggressive} MB (é«˜é£é™©)")
        print(f"   æ€»è®¡å¯èŠ‚çœ: {total_immediate + total_moderate + total_aggressive} MB")
        
        return optimization_plan
    
    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” VisionAI-ClipsMaster æ•´åˆåŒ…ä½“ç§¯åˆ†æ")
        print("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        total_size, file_count = self.get_directory_size(self.package_path)
        self.analysis_results["total_size_mb"] = total_size / 1024 / 1024
        self.analysis_results["file_count"] = file_count
        
        print(f"ğŸ“Š æ•´åˆåŒ…åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ€»å¤§å°: {self.analysis_results['total_size_mb']:.1f} MB")
        print(f"   æ–‡ä»¶æ•°: {file_count:,} ä¸ª")
        
        # è¯¦ç»†åˆ†æ
        self.analyze_internal_components()
        ai_analysis, ai_total = self.analyze_ai_frameworks()
        python_analysis, python_total = self.analyze_python_runtime()
        self.find_largest_files()
        self.identify_optimization_opportunities()
        optimization_plan = self.generate_optimization_plan()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report(ai_total, python_total, optimization_plan)
        
        return self.analysis_results
    
    def generate_summary_report(self, ai_total, python_total, optimization_plan):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ ä½“ç§¯åˆ†ææ€»ç»“æŠ¥å‘Š")
        print("=" * 60)
        
        total_mb = self.analysis_results["total_size_mb"]
        
        print(f"ğŸ¯ ä½“ç§¯æ„æˆåˆ†æ:")
        print(f"   AIæ¡†æ¶ç»„ä»¶:    {ai_total:8.1f} MB ({ai_total/total_mb*100:.1f}%)")
        print(f"   Pythonè¿è¡Œæ—¶:  {python_total:8.1f} MB ({python_total/total_mb*100:.1f}%)")
        print(f"   å…¶ä»–ç»„ä»¶:      {total_mb-ai_total-python_total:8.1f} MB ({(total_mb-ai_total-python_total)/total_mb*100:.1f}%)")
        print(f"   æ€»è®¡:          {total_mb:8.1f} MB (100.0%)")
        
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®æ€»ç»“:")
        immediate_savings = sum(item["estimated_savings_mb"] for item in optimization_plan["immediate_savings"])
        moderate_savings = sum(item["estimated_savings_mb"] for item in optimization_plan["moderate_savings"])
        aggressive_savings = sum(item["estimated_savings_mb"] for item in optimization_plan["aggressive_savings"])
        
        print(f"   ä¿å®ˆä¼˜åŒ–: å¯èŠ‚çœ {immediate_savings} MB (ç›®æ ‡å¤§å°: {total_mb-immediate_savings:.1f} MB)")
        print(f"   ä¸­ç­‰ä¼˜åŒ–: å¯èŠ‚çœ {immediate_savings+moderate_savings} MB (ç›®æ ‡å¤§å°: {total_mb-immediate_savings-moderate_savings:.1f} MB)")
        print(f"   æ¿€è¿›ä¼˜åŒ–: å¯èŠ‚çœ {immediate_savings+moderate_savings+aggressive_savings} MB (ç›®æ ‡å¤§å°: {total_mb-immediate_savings-moderate_savings-aggressive_savings:.1f} MB)")
    
    def save_analysis_report(self, output_file: str = "package_analysis_report.json"):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    package_path = "dist/VisionAI-ClipsMaster"
    
    if not Path(package_path).exists():
        print(f"âŒ æ•´åˆåŒ…ç›®å½•ä¸å­˜åœ¨: {package_path}")
        return
    
    analyzer = PackageSizeAnalyzer(package_path)
    results = analyzer.run_full_analysis()
    analyzer.save_analysis_report()

if __name__ == "__main__":
    main()
