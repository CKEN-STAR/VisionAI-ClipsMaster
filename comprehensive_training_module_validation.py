#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VisionAI-ClipsMaster è®­ç»ƒæ¨¡å—å…¨é¢æµ‹è¯•éªŒè¯ç³»ç»Ÿ
Comprehensive Training Module Validation System

æµ‹è¯•èŒƒå›´ï¼š
1. è®­ç»ƒå·¥ä½œæµå®Œæ•´æ€§æµ‹è¯•
2. è®­ç»ƒæ•°æ®å¤„ç†éªŒè¯
3. GPUåŠ é€ŸçœŸå®æ€§éªŒè¯
4. æ¨¡å‹åˆ‡æ¢æœºåˆ¶æµ‹è¯•
5. è®­ç»ƒæ•ˆæœéªŒè¯
"""

import os
import sys
import json
import time
import torch
import psutil
import logging
import traceback
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

class TrainingModuleValidator:
    """è®­ç»ƒæ¨¡å—éªŒè¯å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.test_results = {
            "timestamp": datetime.now().isoformat(),
            "test_summary": {
                "total_tests": 0,
                "passed": 0,
                "failed": 0,
                "warnings": 0
            },
            "detailed_results": {}
        }
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = self.project_root / "logs"
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"training_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def run_test(self, test_name: str, test_func, *args, **kwargs) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        self.test_results["test_summary"]["total_tests"] += 1
        
        try:
            self.logger.info(f"å¼€å§‹æµ‹è¯•: {test_name}")
            start_time = time.time()
            
            result = test_func(*args, **kwargs)
            
            end_time = time.time()
            duration = end_time - start_time
            
            if result.get("status") == "PASS":
                self.test_results["test_summary"]["passed"] += 1
                self.logger.info(f"âœ… {test_name} - é€šè¿‡ ({duration:.2f}s)")
            elif result.get("status") == "WARNING":
                self.test_results["test_summary"]["warnings"] += 1
                self.logger.warning(f"âš ï¸ {test_name} - è­¦å‘Š ({duration:.2f}s)")
            else:
                self.test_results["test_summary"]["failed"] += 1
                self.logger.error(f"âŒ {test_name} - å¤±è´¥ ({duration:.2f}s)")
                
            result["duration"] = duration
            self.test_results["detailed_results"][test_name] = result
            
            return result
            
        except Exception as e:
            self.test_results["test_summary"]["failed"] += 1
            error_result = {
                "status": "FAIL",
                "error": str(e),
                "traceback": traceback.format_exc(),
                "duration": time.time() - start_time if 'start_time' in locals() else 0
            }
            self.test_results["detailed_results"][test_name] = error_result
            self.logger.error(f"âŒ {test_name} - å¼‚å¸¸: {e}")
            return error_result

    def test_training_workflow_integrity(self) -> Dict[str, Any]:
        """æµ‹è¯•è®­ç»ƒå·¥ä½œæµå®Œæ•´æ€§"""
        results = {
            "status": "PASS",
            "details": {},
            "issues": []
        }
        
        # 1. æ£€æŸ¥è®­ç»ƒæ¨¡å—æ–‡ä»¶å­˜åœ¨æ€§
        required_files = [
            "src/training/en_trainer.py",
            "src/training/zh_trainer.py", 
            "src/training/curriculum.py",
            "src/training/data_splitter.py",
            "src/training/data_augment.py",
            "src/training/plot_augment.py"
        ]
        
        missing_files = []
        for file_path in required_files:
            full_path = self.project_root / file_path
            if not full_path.exists():
                missing_files.append(file_path)
                
        if missing_files:
            results["status"] = "FAIL"
            results["issues"].append(f"ç¼ºå¤±å…³é”®æ–‡ä»¶: {missing_files}")
            
        results["details"]["file_check"] = {
            "required_files": len(required_files),
            "found_files": len(required_files) - len(missing_files),
            "missing_files": missing_files
        }
        
        # 2. æ£€æŸ¥è®­ç»ƒæ•°æ®ç›®å½•ç»“æ„
        data_dirs = [
            "data/training/en",
            "data/training/zh",
            "data/training/en/hit_subtitles",
            "data/training/zh/hit_subtitles"
        ]
        
        missing_dirs = []
        for dir_path in data_dirs:
            full_path = self.project_root / dir_path
            if not full_path.exists():
                missing_dirs.append(dir_path)
                
        if missing_dirs:
            results["status"] = "WARNING" if results["status"] == "PASS" else "FAIL"
            results["issues"].append(f"ç¼ºå¤±æ•°æ®ç›®å½•: {missing_dirs}")
            
        results["details"]["directory_check"] = {
            "required_dirs": len(data_dirs),
            "found_dirs": len(data_dirs) - len(missing_dirs),
            "missing_dirs": missing_dirs
        }
        
        # 3. å°è¯•å¯¼å…¥è®­ç»ƒæ¨¡å—
        import_results = {}
        modules_to_test = [
            ("src.training.en_trainer", "EnTrainer"),
            ("src.training.zh_trainer", "ZhTrainer"),
            ("src.training.curriculum", "CurriculumLearning"),
            ("src.training.data_splitter", "DataSplitter")
        ]
        
        for module_name, class_name in modules_to_test:
            try:
                module = __import__(module_name, fromlist=[class_name])
                cls = getattr(module, class_name)
                import_results[module_name] = "SUCCESS"
            except Exception as e:
                import_results[module_name] = f"FAILED: {str(e)}"
                results["status"] = "FAIL"
                results["issues"].append(f"æ— æ³•å¯¼å…¥ {module_name}: {e}")
                
        results["details"]["import_check"] = import_results
        
        return results

    def test_training_data_processing(self) -> Dict[str, Any]:
        """æµ‹è¯•è®­ç»ƒæ•°æ®å¤„ç†éªŒè¯"""
        results = {
            "status": "PASS",
            "details": {},
            "issues": []
        }
        
        # 1. æ£€æŸ¥è®­ç»ƒæ•°æ®æ–‡ä»¶
        en_data_files = list((self.project_root / "data/training/en").glob("*.txt")) + \
                        list((self.project_root / "data/training/en").glob("*.json"))
        zh_data_files = list((self.project_root / "data/training/zh").glob("*.txt")) + \
                        list((self.project_root / "data/training/zh").glob("*.json"))
        
        results["details"]["data_files"] = {
            "english_files": len(en_data_files),
            "chinese_files": len(zh_data_files),
            "total_files": len(en_data_files) + len(zh_data_files)
        }
        
        if len(en_data_files) == 0 and len(zh_data_files) == 0:
            results["status"] = "WARNING"
            results["issues"].append("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
            
        # 2. éªŒè¯æ•°æ®æ ¼å¼
        sample_files = (en_data_files[:2] if en_data_files else []) + \
                      (zh_data_files[:2] if zh_data_files else [])
        
        valid_formats = 0
        format_errors = []
        
        for file_path in sample_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if file_path.suffix == '.json':
                        json.loads(content)
                    valid_formats += 1
            except Exception as e:
                format_errors.append(f"{file_path.name}: {str(e)}")
                
        results["details"]["format_validation"] = {
            "checked_files": len(sample_files),
            "valid_files": valid_formats,
            "format_errors": format_errors
        }
        
        if format_errors:
            results["status"] = "WARNING" if results["status"] == "PASS" else "FAIL"
            results["issues"].extend(format_errors)
            
        return results

    def test_gpu_acceleration_reality(self) -> Dict[str, Any]:
        """æµ‹è¯•GPUåŠ é€ŸçœŸå®æ€§éªŒè¯"""
        results = {
            "status": "PASS",
            "details": {},
            "issues": []
        }

        # 1. æ£€æŸ¥CUDAå¯ç”¨æ€§
        cuda_available = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if cuda_available else 0

        results["details"]["cuda_info"] = {
            "cuda_available": cuda_available,
            "gpu_count": gpu_count,
            "pytorch_version": torch.__version__
        }

        if cuda_available:
            # è·å–GPUä¿¡æ¯
            gpu_info = []
            for i in range(gpu_count):
                gpu_props = torch.cuda.get_device_properties(i)
                gpu_info.append({
                    "device_id": i,
                    "name": gpu_props.name,
                    "memory_total": gpu_props.total_memory / (1024**3),  # GB
                    "compute_capability": f"{gpu_props.major}.{gpu_props.minor}"
                })
            results["details"]["gpu_devices"] = gpu_info

            # 2. æµ‹è¯•GPUå†…å­˜åˆ†é…
            try:
                device = torch.device("cuda:0")
                test_tensor = torch.randn(1000, 1000, device=device)
                memory_allocated = torch.cuda.memory_allocated(0) / (1024**2)  # MB
                torch.cuda.empty_cache()

                results["details"]["gpu_memory_test"] = {
                    "allocation_successful": True,
                    "memory_allocated_mb": memory_allocated
                }
            except Exception as e:
                results["status"] = "WARNING"
                results["issues"].append(f"GPUå†…å­˜åˆ†é…æµ‹è¯•å¤±è´¥: {e}")
                results["details"]["gpu_memory_test"] = {
                    "allocation_successful": False,
                    "error": str(e)
                }

            # 3. æµ‹è¯•ç®€å•è®¡ç®—æ€§èƒ½
            try:
                # CPUè®¡ç®—
                start_time = time.time()
                cpu_tensor = torch.randn(2000, 2000)
                cpu_result = torch.mm(cpu_tensor, cpu_tensor)
                cpu_time = time.time() - start_time

                # GPUè®¡ç®—
                start_time = time.time()
                gpu_tensor = torch.randn(2000, 2000, device=device)
                gpu_result = torch.mm(gpu_tensor, gpu_tensor)
                torch.cuda.synchronize()
                gpu_time = time.time() - start_time

                speedup = cpu_time / gpu_time if gpu_time > 0 else 0

                results["details"]["performance_comparison"] = {
                    "cpu_time_seconds": cpu_time,
                    "gpu_time_seconds": gpu_time,
                    "speedup_ratio": speedup
                }

                if speedup < 1.5:
                    results["status"] = "WARNING"
                    results["issues"].append(f"GPUåŠ é€Ÿæ•ˆæœä¸æ˜æ˜¾ (åŠ é€Ÿæ¯”: {speedup:.2f})")

            except Exception as e:
                results["status"] = "WARNING"
                results["issues"].append(f"GPUæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

        else:
            results["status"] = "WARNING"
            results["issues"].append("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")

        return results

    def test_model_switching_mechanism(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹åˆ‡æ¢æœºåˆ¶"""
        results = {
            "status": "PASS",
            "details": {},
            "issues": []
        }

        # 1. æ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶
        model_config_files = [
            "configs/models/active_model.yaml",
            "configs/models/available_models/mistral-7b-en.yaml",
            "configs/models/available_models/qwen2.5-7b-zh.yaml"
        ]

        config_status = {}
        for config_file in model_config_files:
            config_path = self.project_root / config_file
            config_status[config_file] = config_path.exists()

        results["details"]["config_files"] = config_status

        missing_configs = [f for f, exists in config_status.items() if not exists]
        if missing_configs:
            results["status"] = "WARNING"
            results["issues"].append(f"ç¼ºå¤±æ¨¡å‹é…ç½®æ–‡ä»¶: {missing_configs}")

        # 2. æµ‹è¯•è¯­è¨€æ£€æµ‹åŠŸèƒ½
        try:
            # å°è¯•å¯¼å…¥è¯­è¨€æ£€æµ‹å™¨
            from src.core.language_detector import LanguageDetector
            detector = LanguageDetector()

            # æµ‹è¯•ä¸­è‹±æ–‡æ£€æµ‹
            test_texts = [
                ("Hello, this is an English text.", "en"),
                ("ä½ å¥½ï¼Œè¿™æ˜¯ä¸­æ–‡æ–‡æœ¬ã€‚", "zh"),
                ("Mixed text: ä½ å¥½ Hello ä¸–ç•Œ World", "mixed")
            ]

            detection_results = []
            for text, expected in test_texts:
                try:
                    detected = detector.detect_language(text)
                    detection_results.append({
                        "text": text[:30] + "...",
                        "expected": expected,
                        "detected": detected,
                        "correct": detected == expected or (expected == "mixed" and detected in ["zh", "en"])
                    })
                except Exception as e:
                    detection_results.append({
                        "text": text[:30] + "...",
                        "expected": expected,
                        "detected": "ERROR",
                        "error": str(e),
                        "correct": False
                    })

            results["details"]["language_detection"] = detection_results

            # æ£€æŸ¥æ£€æµ‹å‡†ç¡®æ€§
            correct_detections = sum(1 for r in detection_results if r.get("correct", False))
            if correct_detections < len(test_texts) * 0.8:
                results["status"] = "WARNING"
                results["issues"].append("è¯­è¨€æ£€æµ‹å‡†ç¡®æ€§ä¸è¶³")

        except ImportError as e:
            results["status"] = "FAIL"
            results["issues"].append(f"æ— æ³•å¯¼å…¥è¯­è¨€æ£€æµ‹å™¨: {e}")
            results["details"]["language_detection"] = "MODULE_NOT_FOUND"

        # 3. æµ‹è¯•æ¨¡å‹åˆ‡æ¢å™¨
        try:
            from src.core.model_switcher import ModelSwitcher
            switcher = ModelSwitcher()

            # æµ‹è¯•æ¨¡å‹åŠ è½½çŠ¶æ€
            model_status = {
                "english_model_loaded": hasattr(switcher, 'english_model') and switcher.english_model is not None,
                "chinese_model_loaded": hasattr(switcher, 'chinese_model') and switcher.chinese_model is not None,
                "active_model": getattr(switcher, 'active_model', None)
            }

            results["details"]["model_switcher"] = model_status

        except ImportError as e:
            results["status"] = "WARNING"
            results["issues"].append(f"æ— æ³•å¯¼å…¥æ¨¡å‹åˆ‡æ¢å™¨: {e}")
            results["details"]["model_switcher"] = "MODULE_NOT_FOUND"

        return results

    def test_memory_optimization(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜ä¼˜åŒ–"""
        results = {
            "status": "PASS",
            "details": {},
            "issues": []
        }

        # 1. è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        memory_info = psutil.virtual_memory()
        results["details"]["system_memory"] = {
            "total_gb": memory_info.total / (1024**3),
            "available_gb": memory_info.available / (1024**3),
            "used_percent": memory_info.percent
        }

        # 2. æ£€æŸ¥å†…å­˜ç›‘æ§æ¨¡å—
        try:
            from src.utils.memory_guard import MemoryGuard
            guard = MemoryGuard()

            # æµ‹è¯•å†…å­˜ç›‘æ§åŠŸèƒ½
            current_usage = guard.get_memory_usage()
            results["details"]["memory_guard"] = {
                "module_loaded": True,
                "current_usage_mb": current_usage,
                "memory_limit_mb": getattr(guard, 'memory_limit', None)
            }

            # æ£€æŸ¥æ˜¯å¦é€‚åˆ4GBè®¾å¤‡
            if memory_info.total / (1024**3) <= 4.5:  # 4GBè®¾å¤‡
                if current_usage > 3800:  # è¶…è¿‡3.8GB
                    results["status"] = "WARNING"
                    results["issues"].append("å†…å­˜ä½¿ç”¨è¶…è¿‡4GBè®¾å¤‡é™åˆ¶")

        except ImportError as e:
            results["status"] = "WARNING"
            results["issues"].append(f"æ— æ³•å¯¼å…¥å†…å­˜ç›‘æ§æ¨¡å—: {e}")
            results["details"]["memory_guard"] = "MODULE_NOT_FOUND"

        return results

    def test_training_effectiveness(self) -> Dict[str, Any]:
        """æµ‹è¯•è®­ç»ƒæ•ˆæœéªŒè¯"""
        results = {
            "status": "PASS",
            "details": {},
            "issues": []
        }

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®­ç»ƒæ¨¡å‹
        model_dirs = [
            "models/mistral/finetuned",
            "models/qwen/finetuned"
        ]

        model_status = {}
        for model_dir in model_dirs:
            model_path = self.project_root / model_dir
            if model_path.exists():
                model_files = list(model_path.glob("*.bin")) + list(model_path.glob("*.safetensors"))
                model_status[model_dir] = {
                    "exists": True,
                    "model_files": len(model_files)
                }
            else:
                model_status[model_dir] = {
                    "exists": False,
                    "model_files": 0
                }

        results["details"]["model_availability"] = model_status

        # 2. æ¨¡æ‹Ÿè®­ç»ƒæ•ˆæœæµ‹è¯•
        test_cases = [
            {
                "input": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚æˆ‘å»äº†å…¬å›­ã€‚çœ‹åˆ°äº†å¾ˆå¤šèŠ±ã€‚",
                "expected_features": ["æ—¶é—´é¡ºåº", "åœºæ™¯è½¬æ¢", "æƒ…æ„Ÿè¡¨è¾¾"],
                "language": "zh"
            },
            {
                "input": "The weather is nice today. I went to the park. I saw many flowers.",
                "expected_features": ["temporal_sequence", "scene_transition", "emotional_expression"],
                "language": "en"
            }
        ]

        effectiveness_results = []
        for test_case in test_cases:
            try:
                # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
                # ç”±äºæ¨¡å‹å¯èƒ½æœªåŠ è½½ï¼Œæˆ‘ä»¬è¿›è¡Œæ¨¡æ‹Ÿæµ‹è¯•
                simulated_output = {
                    "input_length": len(test_case["input"]),
                    "language_detected": test_case["language"],
                    "features_extracted": len(test_case["expected_features"]),
                    "quality_score": 0.85  # æ¨¡æ‹Ÿè´¨é‡åˆ†æ•°
                }
                effectiveness_results.append({
                    "test_case": test_case["input"][:30] + "...",
                    "result": simulated_output,
                    "status": "SIMULATED"
                })
            except Exception as e:
                effectiveness_results.append({
                    "test_case": test_case["input"][:30] + "...",
                    "error": str(e),
                    "status": "ERROR"
                })

        results["details"]["effectiveness_tests"] = effectiveness_results

        # 3. æ£€æŸ¥è®­ç»ƒå†å²è®°å½•
        training_history_file = self.project_root / "data/training/training_history.json"
        if training_history_file.exists():
            try:
                with open(training_history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                results["details"]["training_history"] = {
                    "file_exists": True,
                    "total_sessions": len(history.get("sessions", [])),
                    "last_training": history.get("last_training", "æœªçŸ¥")
                }
            except Exception as e:
                results["details"]["training_history"] = {
                    "file_exists": True,
                    "error": str(e)
                }
        else:
            results["details"]["training_history"] = {
                "file_exists": False
            }
            results["status"] = "WARNING"
            results["issues"].append("æœªæ‰¾åˆ°è®­ç»ƒå†å²è®°å½•")

        return results

    def generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        report_lines = [
            "=" * 80,
            "VisionAI-ClipsMaster è®­ç»ƒæ¨¡å—å…¨é¢æµ‹è¯•éªŒè¯æŠ¥å‘Š",
            "=" * 80,
            f"æµ‹è¯•æ—¶é—´: {self.test_results['timestamp']}",
            "",
            "ğŸ“Š æµ‹è¯•æ¦‚è§ˆ:",
            f"  æ€»æµ‹è¯•æ•°: {self.test_results['test_summary']['total_tests']}",
            f"  é€šè¿‡: {self.test_results['test_summary']['passed']} âœ…",
            f"  å¤±è´¥: {self.test_results['test_summary']['failed']} âŒ",
            f"  è­¦å‘Š: {self.test_results['test_summary']['warnings']} âš ï¸",
            "",
            "ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:",
            ""
        ]

        for test_name, result in self.test_results["detailed_results"].items():
            status_icon = {
                "PASS": "âœ…",
                "FAIL": "âŒ",
                "WARNING": "âš ï¸"
            }.get(result["status"], "â“")

            report_lines.extend([
                f"{status_icon} {test_name}",
                f"   çŠ¶æ€: {result['status']}",
                f"   è€—æ—¶: {result.get('duration', 0):.2f}ç§’"
            ])

            if result.get("issues"):
                report_lines.append("   é—®é¢˜:")
                for issue in result["issues"]:
                    report_lines.append(f"     - {issue}")

            if result.get("details"):
                report_lines.append("   è¯¦ç»†ä¿¡æ¯:")
                for key, value in result["details"].items():
                    if isinstance(value, dict):
                        report_lines.append(f"     {key}: {json.dumps(value, ensure_ascii=False, indent=6)}")
                    else:
                        report_lines.append(f"     {key}: {value}")

            report_lines.append("")

        # æ·»åŠ å»ºè®®å’Œä¿®å¤æ–¹æ¡ˆ
        report_lines.extend([
            "ğŸ”§ ä¿®å¤å»ºè®®:",
            ""
        ])

        failed_tests = [name for name, result in self.test_results["detailed_results"].items()
                       if result["status"] == "FAIL"]
        warning_tests = [name for name, result in self.test_results["detailed_results"].items()
                        if result["status"] == "WARNING"]

        if failed_tests:
            report_lines.extend([
                "âŒ å…³é”®é—®é¢˜ä¿®å¤:",
                ""
            ])
            for test_name in failed_tests:
                result = self.test_results["detailed_results"][test_name]
                report_lines.append(f"  {test_name}:")
                if result.get("issues"):
                    for issue in result["issues"]:
                        report_lines.append(f"    - {issue}")
                report_lines.append("")

        if warning_tests:
            report_lines.extend([
                "âš ï¸ ä¼˜åŒ–å»ºè®®:",
                ""
            ])
            for test_name in warning_tests:
                result = self.test_results["detailed_results"][test_name]
                report_lines.append(f"  {test_name}:")
                if result.get("issues"):
                    for issue in result["issues"]:
                        report_lines.append(f"    - {issue}")
                report_lines.append("")

        report_lines.extend([
            "=" * 80,
            "æŠ¥å‘Šç»“æŸ",
            "=" * 80
        ])

        return "\n".join(report_lines)

    def save_report(self, report_content: str) -> str:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜JSONæ ¼å¼
        json_file = self.project_root / f"training_module_validation_report_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2)

        # ä¿å­˜æ–‡æœ¬æ ¼å¼
        txt_file = self.project_root / f"training_module_validation_report_{timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        return str(txt_file)

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.logger.info("å¼€å§‹VisionAI-ClipsMasterè®­ç»ƒæ¨¡å—å…¨é¢éªŒè¯")

        # å®šä¹‰æµ‹è¯•å¥—ä»¶
        test_suite = [
            ("è®­ç»ƒå·¥ä½œæµå®Œæ•´æ€§æµ‹è¯•", self.test_training_workflow_integrity),
            ("è®­ç»ƒæ•°æ®å¤„ç†éªŒè¯", self.test_training_data_processing),
            ("GPUåŠ é€ŸçœŸå®æ€§éªŒè¯", self.test_gpu_acceleration_reality),
            ("æ¨¡å‹åˆ‡æ¢æœºåˆ¶æµ‹è¯•", self.test_model_switching_mechanism),
            ("å†…å­˜ä¼˜åŒ–æµ‹è¯•", self.test_memory_optimization),
            ("è®­ç»ƒæ•ˆæœéªŒè¯", self.test_training_effectiveness)
        ]

        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        for test_name, test_func in test_suite:
            self.run_test(test_name, test_func)

        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        report_content = self.generate_comprehensive_report()
        report_file = self.save_report(report_content)

        self.logger.info(f"æµ‹è¯•å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")

        # æ‰“å°æ‘˜è¦
        summary = self.test_results["test_summary"]
        self.logger.info(f"æµ‹è¯•æ‘˜è¦: {summary['passed']}é€šè¿‡, {summary['failed']}å¤±è´¥, {summary['warnings']}è­¦å‘Š")

        return self.test_results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨VisionAI-ClipsMasterè®­ç»ƒæ¨¡å—å…¨é¢æµ‹è¯•éªŒè¯")
    print("=" * 60)

    try:
        validator = TrainingModuleValidator()
        results = validator.run_all_tests()

        # è¾“å‡ºæœ€ç»ˆç»“æœ
        summary = results["test_summary"]
        total = summary["total_tests"]
        passed = summary["passed"]
        failed = summary["failed"]
        warnings = summary["warnings"]

        print("\n" + "=" * 60)
        print("ğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ:")
        print(f"   æ€»æµ‹è¯•æ•°: {total}")
        print(f"   âœ… é€šè¿‡: {passed} ({passed/total*100:.1f}%)")
        print(f"   âŒ å¤±è´¥: {failed} ({failed/total*100:.1f}%)")
        print(f"   âš ï¸ è­¦å‘Š: {warnings} ({warnings/total*100:.1f}%)")

        if failed == 0:
            if warnings == 0:
                print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è®­ç»ƒæ¨¡å—çŠ¶æ€è‰¯å¥½ã€‚")
                return 0
            else:
                print("\nâœ… æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸ï¼Œä½†æœ‰ä¸€äº›ä¼˜åŒ–å»ºè®®ã€‚")
                return 0
        else:
            print("\nâŒ å‘ç°å…³é”®é—®é¢˜ï¼Œéœ€è¦ä¿®å¤åå†æ¬¡æµ‹è¯•ã€‚")
            return 1

    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        traceback.print_exc()
        return 2


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
